<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/site/avatar.jpg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/site/avatar.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/site/avatar.jpg">
  <link rel="mask-icon" href="/images/site/avatar.jpg" color="#222">
  <meta name="google-site-verification" content="fMKqXfnCsLFKKj0NjoZZApB_BuqLVUiJxtRkj-rznU4">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.hrwhisper.me","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="在transformer提出前，比如做机器翻译，常用的是seq2seq的模型。这类模型以RNN作为基本的结构，每一个时刻的输入依赖于上一个时刻的输出，难以并行化计算（当然也可以用CNN来做，如textCNN，CNN可以比较好的做到并行）；此外，RNN容易忘记较早看到的信息，尽管有LSTM、GRU使用门的机制来缓解这个问题，但对于特别长的句子，仍旧是有问题的。Transformer的提出正是解决了上">
<meta property="og:type" content="article">
<meta property="og:title" content="【深度学习】Attention is All You Need : Transformer模型">
<meta property="og:url" content="https://www.hrwhisper.me/deep-learning-attention-is-all-you-need-transformer/index.html">
<meta property="og:site_name" content="细语呢喃">
<meta property="og:description" content="在transformer提出前，比如做机器翻译，常用的是seq2seq的模型。这类模型以RNN作为基本的结构，每一个时刻的输入依赖于上一个时刻的输出，难以并行化计算（当然也可以用CNN来做，如textCNN，CNN可以比较好的做到并行）；此外，RNN容易忘记较早看到的信息，尽管有LSTM、GRU使用门的机制来缓解这个问题，但对于特别长的句子，仍旧是有问题的。Transformer的提出正是解决了上">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.hrwhisper.me/images/deep-learning-attention-is-all-you-need-transformer/transformer.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/deep-learning-attention-is-all-you-need-transformer/seq2seq-model-to-attention.jpeg">
<meta property="og:image" content="https://www.hrwhisper.me/images/deep-learning-attention-is-all-you-need-transformer/seq2seq-attention1.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/deep-learning-attention-is-all-you-need-transformer/seq2seq-attention2.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/deep-learning-attention-is-all-you-need-transformer/seq2seq-attention3.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/deep-learning-attention-is-all-you-need-transformer/replace-rnn-by-self-attention.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/deep-learning-attention-is-all-you-need-transformer/self-attention-q-k-v.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/deep-learning-attention-is-all-you-need-transformer/self-attention-q-k-v2.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/deep-learning-attention-is-all-you-need-transformer/self-attention-q-k-v3.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/deep-learning-attention-is-all-you-need-transformer/multi-head-attention.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/deep-learning-attention-is-all-you-need-transformer/multi-head-attention-pytorch.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/deep-learning-attention-is-all-you-need-transformer/transformer-encoder.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/deep-learning-attention-is-all-you-need-transformer/positional-encoding.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/deep-learning-attention-is-all-you-need-transformer/batch-normalization-vs-layer-normalization.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/deep-learning-attention-is-all-you-need-transformer/transformer-decoder.png">
<meta property="article:published_time" content="2020-03-29T14:36:25.000Z">
<meta property="article:modified_time" content="2020-10-19T14:35:48.727Z">
<meta property="article:author" content="hrwhisper">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Machine Learning model">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.hrwhisper.me/images/deep-learning-attention-is-all-you-need-transformer/transformer.png">

<link rel="canonical" href="https://www.hrwhisper.me/deep-learning-attention-is-all-you-need-transformer/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>【深度学习】Attention is All You Need : Transformer模型 | 细语呢喃</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-69270533-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-69270533-1');
      }
    </script>


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d6a8cb42bd9ae728375b6726daa75e95";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">细语呢喃</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">技术改变生活</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">26</span></a>

  </li>
        <li class="menu-item menu-item-leetcode">

    <a href="/leetcode-algorithm-solution/" rel="section"><i class="fa fa-archive fa-fw"></i>leetcode</a>

  </li>
        <li class="menu-item menu-item-friends">

    <a href="/friend-link/" rel="section"><i class="fa fa-link fa-fw"></i>friends</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about-me/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.hrwhisper.me/deep-learning-attention-is-all-you-need-transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/site/avatar.jpg">
      <meta itemprop="name" content="hrwhisper">
      <meta itemprop="description" content="一个分享机器学习、算法与数据结构，个人学习心得、读书笔记、生活的博客。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="细语呢喃">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【深度学习】Attention is All You Need : Transformer模型
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-03-29 22:36:25" itemprop="dateCreated datePublished" datetime="2020-03-29T22:36:25+08:00">2020-03-29</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/study/" itemprop="url" rel="index"><span itemprop="name">study</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/study/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span id="/deep-learning-attention-is-all-you-need-transformer/" class="post-meta-item leancloud_visitors" data-flag-title="【深度学习】Attention is All You Need : Transformer模型" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">评论数：</span>
    
    <a title="valine" href="/deep-learning-attention-is-all-you-need-transformer/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/deep-learning-attention-is-all-you-need-transformer/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>在transformer提出前，比如做机器翻译，常用的是seq2seq的模型。这类模型以RNN作为基本的结构，每一个时刻的输入依赖于上一个时刻的输出，难以并行化计算（当然也可以用CNN来做，如textCNN，CNN可以比较好的做到并行）；此外，RNN容易忘记较早看到的信息，尽管有LSTM、GRU使用门的机制来缓解这个问题，但对于特别长的句子，仍旧是有问题的。Transformer的提出正是解决了上述两个问题。 <a id="more"></a></p>
<p>首先来看看Transformer的结构：</p>
<figure>
<img src="../images/deep-learning-attention-is-all-you-need-transformer/transformer.png" alt="transformer" /><figcaption>transformer</figcaption>
</figure>
<p>看上去很复杂，但大体上，也是分为Encoder和Decoder，以机器翻译为例，Encoder输入“机器学习&quot;,Decoder输出”Machine learning“。</p>
<p>下面对Encoder和Decoder分别进行讲解。不过在此之前，会先介绍Self-Attention机制，因为transformer中大量的采用了这个机制。</p>
<h2 id="self-attention">Self-Attention</h2>
<p>想必在此之前，你已经听过了Seq2Seq模型中的Attention机制，那么它和Self Attetion有什么区别呢？我们先对了Seq2Seq模型中的Attention机制做一个简短的回顾，然后在切入主题。</p>
<h3 id="attetion-in-seq2seq-model">Attetion in Seq2Seq model</h3>
<p>前面提到过，seq2seq model在面对长句子的时候表现不佳，那么怎么办呢？2015年救星Attention诞生了。这个过程下面的图很生动的说明了这个情况：</p>
<figure>
<img src="../images/deep-learning-attention-is-all-you-need-transformer/seq2seq-model-to-attention.jpeg" alt="seq2seq-model-to-attention" /><figcaption>seq2seq-model-to-attention</figcaption>
</figure>
<p>注意到，一开始的Encoder-Decoder将input的sequence都encode为一个向量，而有了attention之后，多了好几个向量。这几个向量是怎么来的呢？</p>
<p>这里借用李宏毅老师的PPT来向大家说明。Attention相当于权重，比如你要翻译某个词的时候，可能会更加的注意输入句子某几个词。</p>
<p>下面的<span class="math inline">\(h^1,h^2,h^3,h^4\)</span>是对应的输出句子4个时刻的隐向量（而不是最后LSTM输出的y，因为y可能维度比较大），而<span class="math inline">\(\alpha_0^1\)</span>则代表<span class="math inline">\(z^0\)</span>和<span class="math inline">\(h^1\)</span>的权重，即需要关注第一个词的程度有多少。</p>
<p><span class="math inline">\(\alpha_0^1\)</span>是怎么得到的呢？可以想象为一个黑盒，输入是<span class="math inline">\(h^1\)</span>和<span class="math inline">\(z^0\)</span>。中间的部分可以是简单的<span class="math inline">\(z\)</span>和<span class="math inline">\(h\)</span>的余弦距离，而可以是一个简单的网络。</p>
<figure>
<img src="../images/deep-learning-attention-is-all-you-need-transformer/seq2seq-attention1.png" alt="seq2seq-attention1" /><figcaption>seq2seq-attention1</figcaption>
</figure>
<p>对每个词都用<span class="math inline">\(z^0\)</span>和<span class="math inline">\(h^i\)</span>跑一遍，就得到了<span class="math inline">\(\alpha_0^1,\alpha_0^2,\alpha_0^3,\alpha_0^4\)</span>，然后进行softmax概率归一化，得到<span class="math inline">\(\hat{a}_0^1，\hat{a}_0^2，\hat{a}_0^3，\hat{a}_0^4\)</span>，接着和<span class="math inline">\(h^i\)</span>相乘，就得到了<span class="math inline">\(c^0=\sum{\hat{a}_0^ih^i}\)</span>，这个<span class="math inline">\(c^0\)</span>就是Decoder的输入</p>
<figure>
<img src="../images/deep-learning-attention-is-all-you-need-transformer/seq2seq-attention2.png" alt="seq2seq-attention2" /><figcaption>seq2seq-attention2</figcaption>
</figure>
<p>对输出的每个时刻都重复上述的过程，相当于每个时刻对每个输入的x的权重是不一样的，得到的c也是不一样的。</p>
<figure>
<img src="../images/deep-learning-attention-is-all-you-need-transformer/seq2seq-attention3.png" alt="seq2seq-attention3" /><figcaption>seq2seq-attention3</figcaption>
</figure>
<p>这就是Seq2seq中的attention，简单的说就是细化Decoder每个时刻需要注意的x，从而得到不同的Decoder输入<span class="math inline">\(c^i\)</span></p>
<h3 id="self-attention-1">self-attention</h3>
<p>那么self attention是怎么样做的呢？</p>
<p>来个直观的印象：左边的是RNN，有4个时刻，每个时刻输出一个b。而右边是Self-attention，它的输出也是4个b，但是这个是<strong>并行</strong>的计算的！</p>
<figure>
<img src="../images/deep-learning-attention-is-all-you-need-transformer/replace-rnn-by-self-attention.png" alt="replace-rnn-by-self-attention" /><figcaption>replace-rnn-by-self-attention</figcaption>
</figure>
<p>这是怎么做的呢？回想之前Encoder和Decoder的权重是由Decoder的状态<span class="math inline">\(z\)</span>乘以encoder的输出<span class="math inline">\(h\)</span>得到的，而self-attention比较巧妙的地方就在于将每个输入x都先做一个线性变化<span class="math inline">\(a^i = Wx^i\)</span>，然后每个<span class="math inline">\(a^i\)</span>都分成了3个子向量<span class="math inline">\(q^i, k^i, v^i\)</span>，分别代表query，key，和value。（在实际的实现中，可以认为q，k, v分别是由3个不同的W乘上x得来）</p>
<figure>
<img src="../images/deep-learning-attention-is-all-you-need-transformer/self-attention-q-k-v.png" alt="self-attention-q-k-v" /><figcaption>self-attention-q-k-v</figcaption>
</figure>
<p>然后拿每个query去对每个key做attention，得到<span class="math inline">\(a_{1,i} = q^1\cdot k^i/\sqrt{d}\)</span>。其中d是q和k的维度，这里除以<span class="math inline">\(\sqrt{d}\)</span>是因为为了防止后过q和k随着维度的增长点积结果过大。</p>
<figure>
<img src="../images/deep-learning-attention-is-all-you-need-transformer/self-attention-q-k-v2.png" alt="self-attention-q-k-v2" /><figcaption>self-attention-q-k-v2</figcaption>
</figure>
<p>接下来也对各个<span class="math inline">\(\alpha_{1,i}\)</span>做softmax，得到<span class="math inline">\(\hat{a}_{1,i}\)</span>，然后每个<span class="math inline">\(\hat{a}_{1,i}\)</span>和对应的<span class="math inline">\(v^i\)</span>相乘并加起来，得到输出<span class="math inline">\(b^1 = \sum_{i}\hat{a}_{1,i}v^i\)</span>。用RNN的话说，这就是在时刻1得到的输出，并且这个输出考虑了整个句子。</p>
<figure>
<img src="../images/deep-learning-attention-is-all-you-need-transformer/self-attention-q-k-v3.png" alt="self-attention-q-k-v3" /><figcaption>self-attention-q-k-v3</figcaption>
</figure>
<p>用<span class="math inline">\(q^2\)</span>和其他的<span class="math inline">\(k^i\)</span>相乘，也就得到了<span class="math inline">\(a_{2,i}\)</span>。</p>
<p>这里简单的做一个和Encoder-Decoder中attention的对比：</p>
<ul>
<li>Encoder-Decoder将Decoder中的隐向量<span class="math inline">\(z^i\)</span>和Encoder的隐向量<span class="math inline">\(h^i\)</span>做相似度计算，得到<span class="math inline">\(a^{j}_i\)</span>，表示<span class="math inline">\(i\)</span>时刻对第<span class="math inline">\(j\)</span>个词的关注程度。然后将同一时刻的<span class="math inline">\(a_i^j\)</span>归一化得到<span class="math inline">\(\hat{a}^j_i\)</span>，并和各个对应的隐向量做加权和，得到Decoder的输入<span class="math inline">\(c^i=\sum{\hat{a}_0^1h^i}\)</span>。</li>
<li>而Self-attention则是首先做一个简单的线性变换，得到<span class="math inline">\(a^i = Wx^i\)</span>，并将<span class="math inline">\(a^i\)</span>分为三个分量<span class="math inline">\(q^i,k^i,v^i\)</span>，分别代表query，key，和value。拿每个query去对每个key做attention，得到<span class="math inline">\(a_{1,i} = q^1\cdot k^i/\sqrt{d}\)</span>， 然后softmax归一化，并和对应的value做加权和，得到输出<span class="math inline">\(b^1 = \sum_{i}\hat{a}_{1,i}v^i\)</span></li>
</ul>
<p>可以看出，两个比较类似，Self-attention将Query和Key进行match，得到相应的权重，这大概是叫做”self&quot;的原因。</p>
<h4 id="self-attention-并行化">self-attention 并行化</h4>
<p>前面提到过，self-attention能够并行化的计算，这是怎么做到的呢？其实就是用到了矩阵的乘法而已。</p>
<p>前面得到的query、key、value，很容易就能并行，那么计算attention的时候呢？对于同一时刻<span class="math inline">\(a_{i,j}\)</span>来说容易用矩阵一下子得出： <span class="math display">\[
\begin{bmatrix} \alpha_{1,1}\\ \alpha_{1,2}\\ \alpha_{1,3}\\ \alpha_{1,4} \end{bmatrix}
 =
  \begin{bmatrix} k^1 \\k^2\\k^3\\k^4\end{bmatrix} q^1
\]</span> 而不同时刻的权重类似的能这样得到（这里为了简单，省去了除以<span class="math inline">\(\sqrt{d}\)</span>）： <span class="math display">\[
\begin{bmatrix} 
\alpha_{1,1} &amp; \alpha_{2,1} &amp; \alpha_{3,1}&amp; \alpha_{4,1}\\ 
\alpha_{1,2}&amp; \alpha_{2,2} &amp; \alpha_{3,2}&amp; \alpha_{4,2}\\ 
\alpha_{1,3}&amp; \alpha_{2,3} &amp; \alpha_{3,3}&amp; \alpha_{4,3}\\ 
\alpha_{1,4} &amp; \alpha_{2,4} &amp; \alpha_{3,4}&amp; \alpha_{4,4}\\ 
\end{bmatrix}
 =
  \begin{bmatrix} k^1 \\k^2\\k^3\\k^4\end{bmatrix} 
   \begin{bmatrix} q^1 &amp; q^2 &amp;q^3 &amp;q^4\end{bmatrix}
\]</span> 即<span class="math inline">\(A = K^TQ\)</span>，然后<span class="math inline">\(A\)</span>也容易进行softmax得到<span class="math inline">\(\hat{A}\)</span>，然后要得到输出，只需要： <span class="math display">\[
\begin{bmatrix} b^1 &amp; b^2 &amp; b^3 &amp; b^4\end{bmatrix}
 =
 \begin{bmatrix} v^1 &amp; v^2 &amp;v^3 &amp;v^4\end{bmatrix}
\begin{bmatrix} 
\hat{\alpha}_{1,1} &amp; \hat{\alpha}_{2,1} &amp; \hat{\alpha}_{3,1}&amp; \hat{\alpha}_{4,1}\\ 
\hat{\alpha}_{1,2}&amp; \hat{\alpha}_{2,2} &amp; \hat{\alpha}_{3,2}&amp; \hat{\alpha}_{4,2}\\ 
\hat{\alpha}_{1,3}&amp; \hat{\alpha}_{2,3} &amp; \hat{\alpha}_{3,3}&amp; \hat{\alpha}_{4,3}\\ 
\hat{\alpha}_{1,4} &amp; \hat{\alpha}_{2,4} &amp; \hat{\alpha}_{3,4}&amp; \hat{\alpha}_{4,4}\\ 
\end{bmatrix}
\]</span> 即<span class="math inline">\(B = V \hat{A}\)</span></p>
<p>这就得到了所有的输出。</p>
<h3 id="multi-head-self-attention">Multi-head self-attention</h3>
<p>Multi-head这个又是什么呢？</p>
<p>其实就是将原来的<span class="math inline">\(q^i, k^i, v^i\)</span>分为多个，比如两个head的话，则query分为<span class="math inline">\(q^{1,i}, q^{i,2}\)</span>，则key分为<span class="math inline">\(k^{i,1}, k^{i,2}\)</span>，则value分为<span class="math inline">\(v^{i,1}, v^{i,2}\)</span></p>
<p>第一个头的q和第一个头的k做self-attention，如下图所示。</p>
<figure>
<img src="../images/deep-learning-attention-is-all-you-need-transformer/multi-head-attention.png" alt="multi-head-attention" /><figcaption>multi-head-attention</figcaption>
</figure>
<p>用多个head使得有的head可以关注短的信息，而有的关注长的，各司其职。</p>
<h4 id="multi-head-self-attention-代码实现">Multi-head self-attention 代码实现</h4>
<p>这里介绍pytorch的实现，首先是计算attention的过程即上面的query、key、和value之间的计算，它们的维度都是[n_batch, head_num, seq_len, d_each_head]，分别代表batch_size的大小，multi-head中head的个书，句子的长度，每个head的向量维度（如<span class="math inline">\(q^{i,1}\)</span>的长度），mask的作用下面在讲，可以先当作None处理。返回的向量也是[n_batch, head_num, seq_len, d_each_head]的维度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span>(<span class="params">query, key, value, mask=None, dropout=None</span>):</span></span><br><span class="line">    <span class="string">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class="line">    d_k = query.size(<span class="number">-1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim = <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure>
<p>而pytorch类前向传播可以写为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span>(<span class="params">module, N</span>):</span></span><br><span class="line">    <span class="string">&quot;Produce N identical layers.&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)])</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="string">&quot;Take in model size and number of heads.&quot;</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, query, key, value, mask=None</span>):</span></span><br><span class="line">        <span class="string">&quot;Implements Figure 2&quot;</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)</span><br></pre></td></tr></table></figure>
<p>h, d_model代表head的个数，模型隐向量，如512。在forward中query, key, value的维度为：[n_batch, seq_len, d_model。在forward中：</p>
<ol type="1">
<li>首先对query, key, value分别做线性变换, 即<code>l(x)</code>，然后将维度转为[n_batch, seq_len, head_num, d_each_head]，即<code>l(x).view(nbatches, -1, self.h, self.d_k)</code>. 将第1维度和第2维度交换得到维度[n_batch, head_num, seq_len, d_each_head]</li>
<li>然后调用attention函数，就得到了attention的输出，就是上面讲解的B，维度为[n_batch, head_num, seq_len, d_each_head]</li>
<li>最后将他们concat起来，得到维度[n_batch, seq_len, d_model]，然后在做线性的变换</li>
</ol>
<p>整个过程如下面的图所示</p>
<figure>
<img src="../images/deep-learning-attention-is-all-you-need-transformer/multi-head-attention-pytorch.png" alt="multi-head-attention-pytorch" /><figcaption>multi-head-attention-pytorch</figcaption>
</figure>
<h2 id="transformer-encoder">Transformer Encoder</h2>
<p>讲完了self attention，就可以来讲Encoder的部分了：</p>
<figure>
<img src="../images/deep-learning-attention-is-all-you-need-transformer/transformer-encoder.png" alt="transformer-encoder" /><figcaption>transformer-encoder</figcaption>
</figure>
<h3 id="输入">输入</h3>
<p>首先输入经过embedding</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, vocab</span>):</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure>
<p>然后会有一个Positional Encoding，这个是因为self-attention完全是并行计算的，并没有考虑句子中word的排序和位置信息，就是说a b c和c b a是完全一样的。（而RNN是按照顺序对句子处理的，当前时刻有上一个时刻的hidden state）。</p>
<p>因此在transform中作者提出加入位置信息的编码。将位置信息的编码加上embedding后的结果就得到了最终的模型输入。那么具体是怎么做的呢？作者探索了下面两种方式：</p>
<ol type="1">
<li>通过训练学习 positional encoding 向量</li>
<li>使用公式来计算 positional encoding向量</li>
</ol>
<p>最终效果差不多，因此采用了第二种，因为不用通过训练，且即使在训练集中没有出现过的句子长度上也能用。</p>
<p>具体的公示为： <span class="math display">\[
PE_{pos, 2i} = \sin(pos / 10000^{2i/d_{model}})\\
PE_{pos, 2i + 1} = \cos(pos / 10000^{2i/d_{model}})
\]</span> pos代表word在这个句子中的位置，i代表模型的维度，从<span class="math inline">\([0...d_{model} - 1]\)</span></p>
<p>为什么选择 sin 和 cos ？positional encoding 的每一个维度都对应着一个正弦曲线，作者假设这样可以让模型相对轻松地通过对应位置来学习。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Implement the PE function.&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<p>波的频率和偏移对每个维度是不同的</p>
<figure>
<img src="../images/deep-learning-attention-is-all-you-need-transformer/positional-encoding.png" alt="positional-encoding" /><figcaption>positional-encoding</figcaption>
</figure>
<h3 id="encoder-layer细节">Encoder Layer细节</h3>
<p>上面的encoder的图中，注意到旁边有个N x, 这表明这部分其实是可以重复多次的，在论文中是重复了6次：第1个encoder的输出作为第2个encoder的输入，如此循环。而每个Encoder中包含两个sub-layer，一个是multi-head-attention，一个是简单的全连接网络(feed forward)</p>
<p>下面的代码定义了Eecoder，将sub-layer重复了N次，forward的过程也很简单，每一次的输出作为下一次的输入，最后用LayerNorm得到输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Core encoder is a stack of N layers&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layer, N</span>):</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask</span>):</span></span><br><span class="line">        <span class="string">&quot;Pass the input (and mask) through each layer in turn.&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450">LayerNorm</a>是Hinton发表的，有些类似Batch Normalization，Batch Normalization是对同一个batch下不同数据的同一纬度进行norm的，而layer则是对同一条data做的，区别见下面的图：</p>
<figure>
<img src="../images/deep-learning-attention-is-all-you-need-transformer/batch-normalization-vs-layer-normalization.png" alt="batch-normalization-vs-layer-normalization" /><figcaption>batch-normalization-vs-layer-normalization</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Construct a layernorm module (See citation for details).&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure>
<p>此外，还引入了SublayerConnection（上面图中的Add &amp; Norm），即类似ResNet的残差连接。因此每一个sub layer的输出为：<span class="math inline">\(\mathrm{LayerNorm}(x + \mathrm{Sublayer}(x))\)</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, dropout</span>):</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, sublayer</span>):</span></span><br><span class="line">        <span class="string">&quot;Apply residual connection to any sublayer with the same size.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure>
<p>最后是sublayer的具体定义：先进行self-attention 子层，然后用feed_forward子层，每个子层都会用SublayerConnection连接一下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Encoder is made up of self-attn and feed forward (defined below)&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, self_attn, feed_forward, dropout</span>):</span></span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask</span>):</span></span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (left) for connections.&quot;</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure>
<p>其中，self_atten之前已经讲过，是之前定义的MultiHeadedAttention类，feed_forward定义如下: <span class="math display">\[
FFN(x) = max(0, xW_1 + b_1) W_2 + b_2
\]</span> 其实就是两个全连接层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Implements FFN equation.&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure>
<h3 id="encoder-小结">Encoder 小结</h3>
<p>Encoder 包含6个 <code>EncoderLayer</code>。每一个 <code>EncoderLayer</code>包含<code>Multi-Headed Attention</code>和<code>Feed Forward</code>两个sub-layer，subLayer还用<code>SublayerConnection</code>进行类似残差网络的连接。</p>
<h2 id="transformer-decoder">Transformer Decoder</h2>
<p>接下来是Decoder的部分：</p>
<figure>
<img src="../images/deep-learning-attention-is-all-you-need-transformer/transformer-decoder.png" alt="transformer-decoder" /><figcaption>transformer-decoder</figcaption>
</figure>
<p>如果你弄懂了Encoder部分，Decoder部分也就没有那么可怕了：</p>
<ol type="1">
<li>输入也是 embedding + positional Encoding，</li>
<li>多层堆叠起来的(论文中为6)，每一层包含三个子层
<ol type="1">
<li>masked multi-head attention：由于在机器翻译中，Decode的过程是一个顺序的过程，也就是当解码第k个位置时，我们只能看到第k - 1 及其之前的解码结果，因此加了mask，这是防止模型看到要预测的数据。</li>
<li>Multi-Head Attention：和Encoder的类似，6层中都接受Encoder的最后输出，作为key和value？</li>
<li>FeedForward：和Encoder一样</li>
</ol></li>
<li>最后连接了LinearLayer和SoftmaxLayer</li>
</ol>
<p>因此，这里先给出Decoder的定义：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Generic N layer decoder with masking.&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layer, N</span>):</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<p>可以看到，相比Encoder到多了mask的标识，</p>
<p>而DecoderLayer的定义为三个子层，中间也用SublayerConnection连接起来</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Decoder is made of self-attn, src-attn, and feed forward (defined below)&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, self_attn, src_attn, feed_forward, dropout</span>):</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span></span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (right) for connections.&quot;</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure>
<h3 id="masked-multi-headed-attention">Masked Multi-Headed Attention</h3>
<p>这里对mask的过程做一个介绍，首先定义函数subsequent_mask</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span>(<span class="params">size</span>):</span></span><br><span class="line">    <span class="string">&quot;Mask out subsequent positions.&quot;</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>这个函数会形成一个下三角矩阵，用来说明每个target word（行）允许看的列，未来的word不被允许看到。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">subsequent_mask(<span class="number">5</span>)[<span class="number">0</span>].int()</span><br><span class="line">output:</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]], dtype=torch.int32)</span><br></pre></td></tr></table></figure>
<p>那么怎么用呢？</p>
<p>其实细心的读者会发现<code>DecoderLayer</code>的<code>forward</code>函数中，有src_mask, tgt_mask，他们是什么区别呢？前者是原文是否为pad（NLP经常会打pad使得句子长度一样），而后者就是之前说的不能看到未来的词，可以看Batch生成的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Batch</span>:</span></span><br><span class="line">    <span class="string">&quot;Object for holding a batch of data with mask during training.&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, src, trg=None, pad=<span class="number">0</span></span>):</span></span><br><span class="line">        self.src = src</span><br><span class="line">        self.src_mask = (src != pad).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">        <span class="keyword">if</span> trg <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.trg = trg[:, :<span class="number">-1</span>]</span><br><span class="line">            self.trg_y = trg[:, <span class="number">1</span>:]</span><br><span class="line">            self.trg_mask = \</span><br><span class="line">                self.make_std_mask(self.trg, pad)</span><br><span class="line">            self.ntokens = (self.trg_y != pad).data.sum()</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_std_mask</span>(<span class="params">tgt, pad</span>):</span></span><br><span class="line">        <span class="string">&quot;Create a mask to hide padding and future words.&quot;</span></span><br><span class="line">        tgt_mask = (tgt != pad).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">        tgt_mask = tgt_mask &amp; Variable(</span><br><span class="line">            subsequent_mask(tgt.size(<span class="number">-1</span>)).type_as(tgt_mask.data))</span><br><span class="line">        <span class="keyword">return</span> tgt_mask</span><br></pre></td></tr></table></figure>
<h3 id="完整的模型">完整的模型</h3>
<p>到这里，就可以给出整个模型的代码了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. Base for this and many </span></span><br><span class="line"><span class="string">    other models.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder, decoder, src_embed, tgt_embed, generator</span>):</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line">        self.generator = generator</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src, tgt, src_mask, tgt_mask</span>):</span></span><br><span class="line">        <span class="string">&quot;Take in and process masked src and target sequences.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, src, src_mask</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, memory, src_mask, tgt, tgt_mask</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure>
<p>Generator就是之前说的LinearLayer + SoftMaxLayer</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Define standard linear + softmax generation step.&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, vocab</span>):</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<p>创建模型的helper函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span>(<span class="params">src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">    <span class="string">&quot;Helper: Construct a model from hyperparameters.&quot;</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), </span><br><span class="line">                             c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">tmp_model = make_model(<span class="number">10</span>, <span class="number">10</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="小结">小结</h2>
<p>个人认为Transformer的设计非常的巧妙，尤其是Self-attention layer，这使得原来并排的一个RNN可以用一个self-attetion layer替代，并且效率非常的高。Transformer影响了后面的很多模型，包括刷新了11个NLP领域方向记录大名鼎鼎的BERT，但BERT本质上是基于Transformer的预训练，个人认为不如Transformer创新度高。</p>
<p>这里引用知乎刘岩的做一个小结：</p>
<blockquote>
<p><strong>优点</strong>：（1）虽然Transformer最终也没有逃脱传统学习的套路，Transformer也只是一个全连接（或者是一维卷积）加Attention的结合体。但是其设计已经足够有创新，因为其抛弃了在NLP中最根本的RNN或者CNN并且取得了非常不错的效果，算法的设计非常精彩，值得每个深度学习的相关人员仔细研究和品位。（2）Transformer的设计最大的带来性能提升的关键是将任意两个单词的距离是1，这对解决NLP中棘手的长期依赖问题是非常有效的。（3）Transformer不仅仅可以应用在NLP的机器翻译领域，甚至可以不局限于NLP领域，是非常有科研潜力的一个方向。（4）算法的并行性非常好，符合目前的硬件（主要指GPU）环境。</p>
<p><strong>缺点</strong>：（1）粗暴的抛弃RNN和CNN虽然非常炫技，但是它也使模型丧失了捕捉局部特征的能力，RNN + CNN + Transformer的结合可能会带来更好的效果。（2）Transformer失去的位置信息其实在NLP中非常重要，而论文中在特征向量中加入Position Embedding也只是一个权宜之计，并没有改变Transformer结构上的固有缺陷。</p>
</blockquote>
<h2 id="参考资料">参考资料</h2>
<ol type="1">
<li><a target="_blank" rel="noopener" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a> (推荐，本文的代码来自该链接)</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/80986272">10分钟带你深入理解Transformer原理及实现</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/48508221">详解Transformer （Attention Is All You Need）</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/8beafdf30f70">从Seq2seq到Attention模型到Self Attention（一）</a></li>
</ol>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>hrwhisper
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://www.hrwhisper.me/deep-learning-attention-is-all-you-need-transformer/" title="【深度学习】Attention is All You Need : Transformer模型">https://www.hrwhisper.me/deep-learning-attention-is-all-you-need-transformer/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        <div class="reward-container">
  <div>请我喝杯咖啡吧~</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/donate/wechat_pay.png" alt="hrwhisper 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/donate/alipay.jpg" alt="hrwhisper 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
              <a href="/tags/Machine-Learning-model/" rel="tag"># Machine Learning model</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/deep-learning-word2vec/" rel="prev" title="【深度学习】Word2Vec">
      <i class="fa fa-chevron-left"></i> 【深度学习】Word2Vec
    </a></div>
      <div class="post-nav-item">
    <a href="/deep-learning-bert/" rel="next" title="【深度学习】BERT">
      【深度学习】BERT <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#self-attention"><span class="nav-text">Self-Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#attetion-in-seq2seq-model"><span class="nav-text">Attetion in Seq2Seq model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#self-attention-1"><span class="nav-text">self-attention</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#self-attention-%E5%B9%B6%E8%A1%8C%E5%8C%96"><span class="nav-text">self-attention 并行化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multi-head-self-attention"><span class="nav-text">Multi-head self-attention</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#multi-head-self-attention-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="nav-text">Multi-head self-attention 代码实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#transformer-encoder"><span class="nav-text">Transformer Encoder</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5"><span class="nav-text">输入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#encoder-layer%E7%BB%86%E8%8A%82"><span class="nav-text">Encoder Layer细节</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#encoder-%E5%B0%8F%E7%BB%93"><span class="nav-text">Encoder 小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#transformer-decoder"><span class="nav-text">Transformer Decoder</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#masked-multi-headed-attention"><span class="nav-text">Masked Multi-Headed Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="nav-text">完整的模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93"><span class="nav-text">小结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-text">参考资料</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="hrwhisper"
      src="/images/site/avatar.jpg">
  <p class="site-author-name" itemprop="name">hrwhisper</p>
  <div class="site-description" itemprop="description">一个分享机器学习、算法与数据结构，个人学习心得、读书笔记、生活的博客。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">254</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">55</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hrwhisper" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hrwhisper" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/murmured" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;murmured" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
      <script data-ad-client="ca-pub-1580254183546533" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2013 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hrwhisper</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"fVcjWMD8aI6F0qEfKdUaHa4f-gzGzoHsz","app_key":"b26lBsbwmVyxTSnNrsBrnv3U","server_url":null,"security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      // script.setAttribute("data-pjax", "");
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'fVcjWMD8aI6F0qEfKdUaHa4f-gzGzoHsz',
      appKey     : 'b26lBsbwmVyxTSnNrsBrnv3U',
      placeholder: "在上方填上邮箱地址可以收到我回复的邮件哦~",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
