<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
       
      <meta name="keywords" content="c,c++,java,python,leetcode,algorithm,reading,life,moods,machine-learning,data-mining,deep-learning,AI" />
       
      <meta name="description" content="一个分享机器学习、算法与数据结构，个人学习心得、读书笔记、生活的博客。" />
      
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>『我爱机器学习』集成学习（二）Boosting与GBDT |  细语呢喃</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/images/site/avatar.jpg" />
       
<link rel="stylesheet" href="/dist/main.css">

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"
      />
      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
       

<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'fMKqXfnCsLFKKj0NjoZZApB_BuqLVUiJxtRkj-rznU4', 'auto');
ga('send', 'pageview');

</script>


 
<script>
var _hmt = _hmt || [];
(function() {
	var hm = document.createElement("script");
	hm.src = "https://hm.baidu.com/hm.js?d6a8cb42bd9ae728375b6726daa75e95";
	var s = document.getElementsByTagName("script")[0]; 
	s.parentNode.insertBefore(hm, s);
})();
</script>


      <!-- mermaid -->
      
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-machine-learning-model-ensemble-boostring-and-gbdt"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  『我爱机器学习』集成学习（二）Boosting与GBDT
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/machine-learning-model-ensemble-boostring-and-gbdt/" class="article-date">
  <time datetime="2018-05-10T11:17:08.000Z" itemprop="datePublished">2018-05-10</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/study/">study</a> / <a class="article-category-link" href="/categories/study/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">5.8k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">26 分钟</span>
        </span>
    </span>
</div>
 
       
        <div class="word_count">
    <span class="post-meta-item-icon">
        <i class="ri-eye-fill"></i> 
        阅读数:<span id="/machine-learning-model-ensemble-boostring-and-gbdt/" data-flag-title="『我爱机器学习』集成学习（二）Boosting与GBDT" class="leancloud_visitors">0</span>次
    </span>
</div>
      
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <p>在上一章节中，我们介绍了模型融合以及Bagging方法，本文主要介绍Boosting相关方法。包括：</p>
<ul>
<li>Boosting</li>
<li>Adaboost</li>
<li>GBDT</li>
</ul>
<a id="more"></a>
<h2 id="boosting">Boosting</h2>
<p>Boosting也叫提升法，是一类将弱学习器提升为强学习器的算法。</p>
<p>这类算法的工作机制类似：先从初始训练集中训练出一个基学习器，再根据学习器的表现<strong>对训练样本分布进行调整</strong>，使得先前基学习器做错的样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直到基学习器数目达到事先指定的T，最终将T个基学习器进行加权结合。</p>
<h2 id="adaboost">Adaboost</h2>
<p>首先给出Adaboost的算法：</p>
<blockquote>
<p>输入: 训练集 <span class="math inline">\(Data =\{({\bf x_1},y_1),({\bf x_2},y_2),\cdots,({\bf x_N},y_N)\}, y_i \in\{+1, -1\}\)</span>;</p>
<p>​ 训练轮数T，</p>
<p>​ 训练算法G</p>
<p>过程：</p>
<p>​ 初始化样本权重 <span class="math inline">\(D_{1,i} = \frac{1}{N}, i=1,2,\cdots,N\)</span></p>
<p>​ for t = 1 to T do</p>
<p>​ <span class="math inline">\(h_t = G({\rm Data} , D_t)\)</span> # 用训练算法G基于权重<span class="math inline">\(D_t\)</span>训练出当前的分类器<span class="math inline">\(h_t\)</span></p>
<p>​ <span class="math inline">\(\epsilon_t = \sum_{i=1}^N D_{t,i}I(h_t(x_i) \ne y_i)\)</span> # 计算误差</p>
<p>​ if <span class="math inline">\(\epsilon_t &gt; 0.5\)</span> then break</p>
<p>​ <span class="math inline">\(\alpha_t = \frac{1}{2}\ln(\frac{1 - \epsilon_t }{\epsilon_t})\)</span> #计算分类器<span class="math inline">\(h_t\)</span>的权重</p>
<p>​ <span class="math inline">\(D_{t+1,i} = \frac{1}{Z_t}\left( D_{t,i} \cdot exp(-y_i\alpha_th_t({\bf x_i}))\right)\)</span> # 更新样本权重，其中，归一化因子：<span class="math inline">\(Z_t = {\sum_{i=1}^N D_{t,i}}\cdot exp\left(-y_i\alpha_th_t({\bf x_i}) \right)\)</span></p>
<p>输出：</p>
<p>​ <span class="math inline">\(H({\bf x}) = sign(\sum_{t=1}^T \alpha_t h_t({\bf x}))\)</span></p>
</blockquote>
<p>一张直观的图如下：</p>
<figure>
<img src="../images/machine-learning-model-ensemble-boostring-and-gbdt/adboost2.png" alt="adboost" /><figcaption aria-hidden="true">adboost</figcaption>
</figure>
<p>描述起来好像很容易的样子：</p>
<ul>
<li>初始化每个样本权值均等，都为1 / N</li>
<li>然后迭代T轮，在每一轮中学习中根据错误率<span class="math inline">\(\epsilon_t\)</span>改变训练数据的权值分布，增加分类错误的权重，减少分类正确的样本权重。这样，新一轮的学习中将会更加重视对之前分错的样本。</li>
<li>将T轮得到的T个分类器<span class="math inline">\(h_t({\bf x})\)</span>和对应的分类器权重<span class="math inline">\(\alpha_t\)</span>进行线性组合，就得到最终结果<span class="math inline">\(H({\bf x}) = sign(\sum_{t=1}^T \alpha_t h_t({\bf x}))\)</span></li>
</ul>
<p>注意到当错误率<span class="math inline">\(\epsilon \gt 0.5\)</span>的时候，我们不再计算。因为对于二分类问题，错误率大于一半意味着不如随机乱猜，留之无用。</p>
<p>Adaboost模型让人困惑的地方有两个：</p>
<ol type="1">
<li>分类器权重<span class="math inline">\(\alpha_t\)</span>为啥等于<span class="math inline">\(\frac{1}{2}ln(\frac{1 - \epsilon_t }{\epsilon_t})\)</span></li>
<li>样本权重公式和归一化因子是怎么来的？</li>
</ol>
<h3 id="分类器权重的由来">分类器权重的由来</h3>
<p>Adaboost的分类器权重公式为：<span class="math inline">\(\alpha_t = \frac{1}{2}ln(\frac{1 - \epsilon_t }{\epsilon_t})\)</span> ，这是怎么来的呢？</p>
<p>可以从最小化训练误差界来推导，也可以从最小化损失函数进行推导。这两种本质上是一样的，都是极小化某式得到的<span class="math inline">\(\alpha_t\)</span>。</p>
<p>这里采用最小化训练误差界来推导。</p>
<h4 id="训练误差界">训练误差界</h4>
<p>Adaboost的训练误差界为： <span class="math display">\[
\frac{1}{N}\sum_{i=1}^N I \left(H({\bf x_i}) \ne y_i \right) \le  \frac{1}{N}\sum_{i=1}^N exp(-y_if({\bf x_i})) = \prod_{t}Z_t \tag{2-1}
\]</span> 其中<span class="math inline">\(f({\bf x}) = \sum_t \alpha_th_t({\bf x})； H({\bf x}) = \rm sign(f({\bf x}))\)</span></p>
<p>好像很复杂，我们先来不管最后那个等式，只看前面的不等式，就是说指数损失是0-1损失函数的上界。这个很好证明，当<span class="math inline">\(H({\bf x_i}) \ne y_i\)</span>时，<span class="math inline">\(-y_if({\bf x_i})&lt; 0 \Rightarrow exp(-y_if({\bf x_i})) \ge 1\)</span>。</p>
<p>在证明右边的等式之前，我们先回顾一下权重的定义： <span class="math display">\[
\begin{align*}
&amp; D_{t+1,i} = \frac{D_{t,i}}{Z_t}\cdot \exp(-y_i\alpha_th_t({\bf x_i})) \\
\Rightarrow \ &amp; Z_tD_{t+1,i} = D_{t,i}\cdot \exp(-y_i\alpha_th_t({\bf x_i})) \tag{2-2}
\end{align*}
\]</span> 现推导如下： <span class="math display">\[
\begin{align*}
   \frac{1}{N}\sum_{i=1}^N \exp(-y_if({\bf x_i})) &amp;=  \frac{1}{N}\sum_{i=1}^N \exp\left (\sum_{t=1}^T-y_i\alpha_th_t({\bf x})\right)
\\&amp; =\sum_{i=1}^N D_{1,i} \prod_{t=1}^{T} \exp\left (-y_i\alpha_th_t({\bf x})\right)   \hspace{5ex} D_{1,i}= \frac{1}{N}
\\&amp; =Z_1\sum_{i=1}^N D_{2,i} \prod_{t=2}^{T}  \exp\left (-y_i\alpha_th_t({\bf x})\right)  \hspace{5ex} 式2-2
\\&amp; =Z_1Z_2\sum_{i=1}^N D_{3,i} \prod_{t=3}^{T}  \exp\left (-y_i\alpha_th_t({\bf x})\right)
\\&amp; =Z_1Z_2\cdots Z_{T-1}\sum_{i=1}^N D_{T,i} \prod_{t=T}^{T}  \exp\left (-y_i\alpha_th_t({\bf x})\right)
\\&amp; =Z_1Z_2\cdots Z_{T}\sum_{i=1}^N D_{T + 1,i}
\\&amp; =Z_1Z_2\cdots Z_{T}\sum_{i=1}^N D_{T + 1,i}   \hspace{7ex}  \sum_{i=1}^N D_{T + 1,i}=1
\\&amp; =\prod_{t=1}^TZ_t
\end{align*}
\]</span> 即我们最小化<span class="math inline">\(\prod_{t=1}^TZ_t\)</span>等价于最小化指数损失函数，我们可以在每一轮都最小化<span class="math inline">\(Z_t\)</span> <span class="math display">\[
\begin{align*}
Z_t &amp;= {\sum_{i=1}^N D_{t,i}}\cdot \exp\left(-y_i\alpha_th_t({\bf x_i}) \right)
\\&amp;= {\sum_{i: \ y_i \ne h_t(x_i)} D_{t,i}}\exp(\alpha_t) + {\sum_{i: \ y_i = h_t(x_i)} D_{t,i}}\exp(-\alpha_t)
\\&amp;=  \epsilon_te^{\alpha_t} + (1 - \epsilon_t)e^{-\alpha_t}
\end{align*}\tag{2-3}
\]</span> <strong>对<span class="math inline">\(\alpha_t\)</span>求导得得到权重公式</strong>： <span class="math display">\[
\frac{\partial Z_t}{\partial \alpha_t} = \epsilon_te^{\alpha_t} - (1 - \epsilon_t)e^{-\alpha_t} = 0 \hspace{4ex}\Rightarrow  \alpha_t= \frac{1}{2}\ln\frac{1-\epsilon_t}{\epsilon_t}\tag{2-4}
\]</span> 将2-4带入2-3得： <span class="math display">\[
Z_t = 2\sqrt{\epsilon_t(1-\epsilon_t)} = \sqrt{1 - (1-2\epsilon_t)^2} = \sqrt{1 - 4\gamma_t^2} \hspace{4ex}  设\gamma_t = \frac{1}{2} - \epsilon_t
\]</span> 因此，2-1的训练误差界可以写为： <span class="math display">\[
\frac{1}{N}\sum_{i=1}^N I \left(H({\bf x_i}) \ne y_i \right) \le \prod_{t}Z_t = \prod_{t} \sqrt{1 - 4\gamma_t^2}  \le exp(-2\sum_{t=1}^T\gamma_t^2)\tag{2-5}
\]</span> 李航老师的书中说：2-5的最后一个式子可先由<span class="math inline">\(e^x\)</span>和<span class="math inline">\(\sqrt{1-x}\)</span>在点x = 0的泰勒展开式推出不等式<span class="math inline">\(\sqrt{1 - 4\gamma_t^2} \le exp(-2\gamma_t^2)\)</span>进而得到。</p>
<p>这表明Adaboost的训练误差是<strong>以指数速率</strong>下降的。</p>
<p>PS： Adaboost能适应弱分类器各自的训练误差，这也是它的名字（适应的提升）的由来。Ada是Adaptive的简写。</p>
<h2 id="加法模型和前向分布算法">加法模型和前向分布算法</h2>
<p>本小节主要介绍加法模型。</p>
<p>Adaboost可以看出是<strong>加法模型</strong>、损失函数为指数损失函数、学习算法为<strong>前向分布算法</strong>时的二分类学习方法。此外，之后要介绍的GBDT也可以看出是加法模型。</p>
<p>加法模型即： <span class="math display">\[
F({\bf x}) =\sum_{t=1}^T \alpha_t h_t({\bf x}; {\bf w_t}) = \sum_{i=1}^Tf_t({\bf x}; {\bf w_t}) \tag{3-1}
\]</span> 其中,<span class="math inline">\(h_t({\bf x}; {\bf w_t})\)</span>为基学习器， <span class="math inline">\(\bf x\)</span>为输入样本，<span class="math inline">\(\bf w\)</span>为基学习器的参数，而<span class="math inline">\(\alpha_t\)</span>为每个基学习器<span class="math inline">\(h_t\)</span>的权重</p>
<p>可以通过最小化损失函数进行求解，即求经验风险极小化问题： <span class="math display">\[
\min_{\alpha,\bf w}  \sum_{i=1}^N L\left(y_i, \sum_{t=1}^T \alpha_t h_t({\bf x}; {\bf w_t})\right)\tag{3-2}
\]</span> 要求解这个问题，是NP难的，一般用贪心法进行求解，即<strong>前向分布算法</strong>：从前往后，每一步只学习一个基函数及其系数，逐步优化3-2式。前向分布算法可以描述如下：</p>
<blockquote>
<p>输入: 训练集 <span class="math inline">\(Data =\{({\bf x_1},y_1),({\bf x_2},y_2),\cdots,({\bf x_N},y_N)\}, y_i \in\{+1, -1\}\)</span>;</p>
<p>​ 损失函数L</p>
<p>过程：</p>
<p>​ for t = 1 to T do</p>
<p>​ <span class="math inline">\((\alpha_t, w_t) = \arg\min_{\alpha_t,w_t} \sum_{i=1}^N L\left(y_i, f_{t-1}({\bf x_i}) + \alpha_t h_t({\bf x}; {\bf w_t})\right)\)</span> # 得到参数和基学习器权重</p>
<p>​ 更新<span class="math inline">\(f_t({\bf x}) = f_{t-1}({\bf x_i}) + \alpha_t h_t({\bf x}; {\bf w_t})\)</span></p>
<p>得到加法模型</p>
<p>​ <span class="math inline">\(F({\bf x}) = f_{t}({\bf x}) = \sum_{t=1}^T \alpha_t h_t({\bf x}; {\bf w_t})\)</span></p>
</blockquote>
<p>这样，便将同时求解t = 1到T的所有参数<span class="math inline">\(\alpha_t, \bf w_t\)</span>的优化问题转化为逐步求解各个<span class="math inline">\(\alpha_t, \bf w_t\)</span>的问题。</p>
<h3 id="再谈adaboost">再谈Adaboost</h3>
<p>上面说到，Adaboost是加法模型，损失函数为指数损失函数，并采用前向分布算法。</p>
<p>下面进行证明：</p>
<blockquote>
<p>前向分布分布算法学习的是加法模型，当基函数为基本分类器时，该加法模型等价于Adaboost的最终分类器。<span class="math inline">\(H({\bf x}) = \sum_{t=1}^T \alpha_t h_t({\bf x})\)</span></p>
</blockquote>
<h4 id="指数损失函数">指数损失函数</h4>
<p>首先证明前向分布算法的损失函数是指数损失函数<span class="math inline">\(L(y, F({\bf x})) = \exp(-yF({\bf x}))\)</span>时，其学习的具体操作等价于Adaboost算法学习的具体操作。</p>
<p>假设经过t - 1轮迭代前向分步算法已经得到<span class="math inline">\(f_{t-1}({\bf x}) = f_{t-2}({\bf x}) + \alpha_{t-1}h_{t-1}({\bf x}) = \alpha_{1}h_1({\bf x}) + \cdots + \alpha_{t-1}h_{t-1}({\bf x})\)</span></p>
<p>则在第t轮迭代应该寻找最优的<span class="math inline">\(\alpha_t, h_t\)</span>使得指数损失最小即 <span class="math inline">\((\alpha_t, h_t) = \arg\min_{\alpha_t,h_t} \sum_{i=1}^N L\left(y_i, f_{t-1}({\bf x_i}) + \alpha_t h_t({\bf x})\right)\)</span></p>
<p>从而得到<span class="math inline">\(f_{t}({\bf x}) = f_{t-1}({\bf x}) + \alpha_{t}h_t({\bf x})\)</span>。而指数损失最小可以写为： <span class="math display">\[
\begin{align*}
(\alpha_t, h_t) &amp;=\mathop{\arg\min}_{\alpha_t,h_t}  \sum_{i=1}^N \exp\left(-y_i(f_{t-1}({\bf x_i}) + \alpha_th_t({\bf x}))\right) \\
&amp;=\mathop{\arg\min}_{\alpha_t,h_t}  \sum_{i=1}^N D_{t,i}\exp\left( -y_i\alpha_th_t({\bf x})\right)\tag{3-3}
\end{align*}
\]</span> 其中<span class="math inline">\(D_{ti} = Error_{(t-1,i)} = \exp(-y_if_{t-1}({\bf x_i}))\)</span>与当前的优化目标无关，只与之前的有关，因此为指数损失函数。</p>
<h4 id="基分类器求解和权重推导">基分类器求解和权重推导</h4>
<p>接着证明使得式子3-3达到最小的<span class="math inline">\(\alpha_t^*\)</span>和<span class="math inline">\(h_t^*({\bf x})\)</span>就是Adaboost算法得到的<span class="math inline">\(\alpha_t\)</span>和<span class="math inline">\(h_t({\bf x})\)</span>。</p>
<p>求解3-3可以分为两步，首先求<span class="math inline">\(h_t({\bf x})\)</span>，对于任意的<span class="math inline">\(\alpha_t \gt 0\)</span>，使3-3最小的<span class="math inline">\(h_t({\bf x})\)</span>由下式得到： <span class="math display">\[
h_t^*({\bf x}) = \mathop{\arg\min}_h \sum_{i=1}^ND_{t,i}I(y_i\ne h({\bf x}))
\]</span> 其中<span class="math inline">\(D_{t,i}= \exp(-y_if_{t-1}(\bf x_i))\)</span>。</p>
<p>这样就得到了基分类器<span class="math inline">\(h_t^*({\bf x})\)</span>，而该分类器就是Adaboost算法的基本分类器<span class="math inline">\(h_t({\bf x})\)</span>，因为它是第t轮加权训练数据分类误差率最小的基本分类器。</p>
<p>接下来求<span class="math inline">\(\alpha_t\)</span>, 式在3-3中： <span class="math display">\[
\begin{align*}
 \sum_{i=1}^N D_{t,i}\exp\left( -y_i\alpha_th_t({\bf x})\right) 
 &amp; = \sum_{y_i=h_t(\bf x)}D_{t,i} e^{-\alpha} +  \sum_{y_i\ne h_t(\bf x)}D_{t,i}e^{\alpha}  \\
 &amp;=e^{-\alpha}\sum_{i=1}^ND_{t,i} + (e^{\alpha} - e^{-\alpha})\sum_{i=1}^N D_{t,i}I(y_i\ne h_t({\bf x_i})) \tag{3-4}
\end{align*}
\]</span> 最后一个变换可以认为是第一项多加了不相等时候的<span class="math inline">\(e^{-\alpha}\)</span>, 因此在第二项的时候减去。1-8对<span class="math inline">\(\alpha\)</span>求导，并使导数为0，得到使得3-3最小的<span class="math inline">\(\alpha_t\)</span> <span class="math display">\[
\alpha_t = \frac{1}{2} \ln(\frac{1 - \epsilon_t }{\epsilon_t})
\]</span> 其中<span class="math inline">\(\epsilon_t\)</span>为分类误差率 <span class="math display">\[
\epsilon_t = \frac{\sum_{i=1}^N D_{t,i}I(y_i\ne h_t({\bf x}))}{\sum_{i=1}^N D_{t,i}} = \sum_{i=1}^N \tilde D_{t,i}I(h_t(x_i) \ne y_i)
\]</span> 可以看出和之前更新<span class="math inline">\(\alpha_t\)</span>一致。</p>
<p>最后来看每一轮样本的权值更新，由<span class="math inline">\(f_{t}({\bf x}) = f_{t-1}({\bf x}) + \alpha_{t}h_t({\bf x})\)</span>和<span class="math inline">\(D_{t,i}= \exp(-y_if_{t-1}(\bf x_i))\)</span>可得： <span class="math display">\[
\begin{align*}
D_{t + 1,i} 
&amp;= \exp(-y_if_{t}(\bf x_i))\\
&amp;= \exp(-y_i( f_{t-1}({\bf x_i}) + \alpha_{t}h_t({\bf x_i}))\\
&amp;= D_{(t,i)}\exp(- \alpha_{t}y_{i}h_t({\bf x_i}))\\
\end{align*}
\]</span> 这与之前的Adaboost的样本权值更新公式相比，只差规范化因子，因而等价。</p>
<h2 id="gbdt">GBDT</h2>
<p>GBDT全称为：Gradient Boosting Decision Tree，即梯度提升决策树。可以理解为<strong>梯度 + 提升 + 决策树</strong>。</p>
<p>在介绍GBDT之前，先介绍比较简单的提升树（Boosting Decision Tree）</p>
<h3 id="提升树">提升树</h3>
<p>提升树实际上就是加法模型和前向分布算法，然后以CART决策树为基学习器。</p>
<p>可以表示为决策树的前向分布算法 <span class="math display">\[
\begin{align*}
F_0({\bf x}) &amp;= 0\\
F_t({\bf x}) &amp;= F_{t-1}({\bf x})  + h_t({\bf x}), t=1,2,\cdots ,T\\
F_T({\bf x}) &amp;= \sum_{t=1}^Th_t({\bf x})
\end{align*}
\]</span></p>
<p>在前向分布算法的第t步，给定当前模型<span class="math inline">\(F_{t-1}({\bf x})\)</span>，需要求解 <span class="math display">\[
h_t^*({\bf x}) = \mathop{\arg\min}_{h_t} \sum_{i=1}^NL(y_i, F_{t-1}({\bf x}) + h_t({\bf x}))
\]</span> 得到第t棵树<span class="math inline">\(F_{t}({\bf x})\)</span>。针对不同问题的提升树学习方法，<strong>主要区别在于使用的损失函数不同</strong>。比如平方误差损失函数的回归问题，用指数损失函数的分类问题，以及用一般损失函数的问题。</p>
<p>采用平方损失函数时，有： <span class="math display">\[
\begin{align*}
L\left(y,  F_{t-1}({\bf x}\right)  + h_t({\bf x})) 
&amp;=\left(y -  F_{t-1}({\bf x} ) - h_t({\bf x})\right)^2\\
&amp;=(r - h_t({\bf x}))^2
\end{align*}
\]</span> 其中<span class="math inline">\(r = y - F_{t-1}({\bf x})\)</span> 称为<strong>残差</strong>(Residual)。因此，对<strong>回归树的提升树算法来说，只需要简单的拟合当前模型的残差</strong>即可。注意这里的是回归树。</p>
<p>因此回归问题的提升树算法可以描述如下：</p>
<blockquote>
<p>输入: 训练集 <span class="math inline">\(Data =\{({\bf x_1},y_1),({\bf x_2},y_2),\cdots,({\bf x_N},y_N)\}, y_i \in\{+1, -1\}\)</span>;</p>
<p>过程：</p>
<p>​ 初始化<span class="math inline">\(F_0({\bf x}) = 0\)</span></p>
<p>​ for t = 1 to T do</p>
<p>​ 计算残差 <span class="math inline">\(r_{ti} = y_i - F_{t-1}({\bf x}) , \ i=1,2,\cdots,N\)</span></p>
<p>​ 拟合残差得到一个回归树：<span class="math inline">\(h_t({\bf x})\)</span></p>
<p>​ 更新<span class="math inline">\(F_t({\bf x}) = F_{t-1}({\bf x_i}) + h_t({\bf x})\)</span></p>
<p>得到加法模型<span class="math inline">\(F({\bf x}) = \sum_{t=1}^T h_t({\bf x})\)</span></p>
</blockquote>
<p>该算法可以用下面的图表示：</p>
<figure>
<img src="../images/machine-learning-model-ensemble-boostring-and-gbdt/Boosting-tree.png" alt="Boosting tree" /><figcaption aria-hidden="true">Boosting tree</figcaption>
</figure>
<p><strong>对比Adaboost来说，该算法可以说是修改样本的"label"，而AdaBoost则是修改样本的权重。</strong></p>
<h4 id="例子">例子</h4>
<p>继续搬出之前的决策树中的例子</p>
<table>
<thead>
<tr class="header">
<th>x</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>y</td>
<td>5.56</td>
<td>5.70</td>
<td>5.91</td>
<td>6.40</td>
<td>6.80</td>
<td>7.05</td>
<td>8.90</td>
<td>8.70</td>
<td>9.00</td>
<td>9.05</td>
</tr>
</tbody>
</table>
<p>第一轮计算后，我们算出（计算过程看决策树那章） <span class="math display">\[
h_1(x) =
\begin{cases}
6.24, &amp; x\le 6.5 \\
8.91, &amp; x \gt 6.5 \\
\end{cases}
\]</span> 可以算出残差（比如x=1就是5.56 - 6.24 = -0.68）</p>
<table>
<thead>
<tr class="header">
<th>x</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>y</td>
<td>-0.68</td>
<td>-0.54</td>
<td>-0.33</td>
<td>0.16</td>
<td>0.56</td>
<td>0.81</td>
<td>-0.01</td>
<td>-0.21</td>
<td>0.09</td>
<td>0.14</td>
</tr>
</tbody>
</table>
<p>接着，继续拟合数据，只是拟合的是上面的残差。得到： <span class="math display">\[
h_2(x) =
\begin{cases}
-0.52, &amp; x\le 3.5 \\
0.22, &amp; x \gt 3.5 \\
\end{cases}
\]</span> 则<span class="math inline">\(f_2({\bf x})\)</span>为： <span class="math display">\[
F_2({\bf x}) = f_1({\bf x}) + f_2({\bf x}) = 
\begin{cases}
5.72, &amp; x\le 3.5 \\
6.46, &amp; 3.5\lt x \le 6.5 \\
9.13, &amp; 6.5\lt x
\end{cases}
\]</span> 以此类推进行计算即可，直到损失<span class="math inline">\(L(y, F_t(\bf x))\)</span>满足要求，比如达到迭代次数T。</p>
<p>和决策树CART不同的地方在于，BDT每次拟合的是残差，而CART是经典的分治算法（divide and conquer），不断的划分子空间并在子空间中进一步精确的拟合。</p>
<h3 id="梯度提升">梯度提升</h3>
<p>前面提到过，GBDT全称为<strong>Gradient Boosting</strong> Decision Tree，<strong>梯度提升</strong>决策树。</p>
<p>现在我们来介绍通用的“梯度提升”算法。</p>
<p>当采用平方损失和指数损失的时候，每一步优化是很简单的，但是<strong>一般的损失函数来说，每一步的优化不容易</strong>，因此提出了梯度提升的方法，这是利用梯度下降的近似方法，其关键是利用<strong>损失函数的负梯度在当前模型的值</strong>： <span class="math display">\[
-\left[\frac{\partial L(y_i, F({\bf x_i}))}{\partial F({\bf x_i})}  \right]_{F({\bf x}) = F_{t-1}({\bf x})}\tag{4-1}
\]</span> 怎么理解这个近似呢？</p>
<p>以前面的均方损失函数为例，也是可以用这个方法来解释的。为了求导方便，我们在均方损失函数前乘以 1/2。 <span class="math display">\[
\begin{align*}
L(y_i, F({\bf x_i})) = \frac{1}{2}(y_i - F({\bf x_i}))^2
\end{align*}
\]</span> 注意到<span class="math inline">\(F({\bf x_i})\)</span>其实只是一些数字而已，我们可以将其像变量一样进行求导： <span class="math display">\[
\begin{align*}
\frac{\partial L(y_i, F({\bf x_i}))}{\partial F({\bf x_i})} = F({\bf x_i}) - y_i
\end{align*}
\]</span> 而前面所说的残差就是上式相反数，即<strong>负梯度</strong>： <span class="math display">\[
r_{ti} = y_i - F_{t-1}({\bf x}) =-\left[\frac{\partial L(y_i, F({\bf x_i}))}{\partial F({\bf x_i})}  \right]_{F({\bf x}) = F_{t-1}({\bf x})}
\]</span> 在梯度提升中，就是将式2-1作为残差来进行拟合。由此，我们给出一般的梯度提升算法：</p>
<blockquote>
<p>输入: 训练集 <span class="math inline">\(Data =\{({\bf x_1},y_1),({\bf x_2},y_2),\cdots,({\bf x_N},y_N)\}, y_i \in\{+1, -1\}\)</span>;</p>
<p>过程：</p>
<p>​ 初始化<span class="math inline">\(F_0({\bf x}) =\mathop{\arg\min}_{h_0} \sum_{i=1}^NL(y_i, h_0({\bf x}))\)</span></p>
<p>​ for t = 1 to T do</p>
<p>​ 计算负梯度 <span class="math inline">\({\tilde y}_{i} = -\left[\frac{\partial L(y_i, F({\bf x_i}))}{\partial F({\bf x_i})} \right]_{F({\bf x}) = F_{t-1}({\bf x})} , \ i=1,2,\cdots,N\)</span></p>
<p>​ <span class="math inline">\(w_t = \arg\min_{w_t} \sum_{i=1}^N \left({\tilde y}_{i} - h_t({\bf x}; {\bf w_t})\right)^2\)</span> # 拟合“残差“得到基学习器权重，也就得到了基学习器</p>
<p>​ <span class="math inline">\(\alpha_t= \arg\min_{\alpha_t} \sum_{i=1}^N L\left(y_i, f_{t-1}({\bf x_i}) + \alpha_t h_t({\bf x}; {\bf w_t})\right)\)</span> # 得到基学习器权重<span class="math inline">\(\alpha_t\)</span></p>
<p>​ 更新<span class="math inline">\(F_t({\bf x}) = F_{t-1}({\bf x_i}) + \alpha_th_t({\bf x}; {\bf w_t})\)</span></p>
</blockquote>
<p>对比提升树来说，提升树没有基学习器参权重<span class="math inline">\(\alpha_t\)</span></p>
<h3 id="gbdt-1">GBDT</h3>
<p>至此，我们可以给出GBDT的算法了。就是采用梯度提升的决策树（CART）而已嘛。PS: 上面给出的是梯度提升。</p>
<p>前面提到过，CART回归将空间划分为K个不相交的区域。可以用数学公式描述为： <span class="math display">\[
f(\mathbf{X}) = \sum_{k=1}^K c_k I(\mathbf{X} \in R_k)
\]</span> GBDT算法(回归)描述如下：</p>
<blockquote>
<p>输入: 训练集 <span class="math inline">\(Data =\{({\bf x_1},y_1),({\bf x_2},y_2),\cdots,({\bf x_N},y_N)\}, y_i \in\{+1, -1\}\)</span>;</p>
<p>过程：</p>
<p>​ 初始化<span class="math inline">\(F_0({\bf x}) =\mathop{\arg\min}_{h_0} \sum_{i=1}^NL(y_i, h_0({\bf x})) =\mathop{\arg\min}_{c} \sum_{i=1}^NL(y_i, c))\)</span></p>
<p>​ for t = 1 to T do</p>
<p>​ 计算残差 <span class="math inline">\({\tilde y}_{i} = -\left[\frac{\partial L(y_i, F({\bf x_i}))}{\partial F({\bf x_i})} \right]_{F({\bf x}) = F_{t-1}({\bf x})} , \ i=1,2,\cdots,N\)</span></p>
<p>​ 拟合残差<span class="math inline">\({\tilde y}_{i}\)</span>得到一个回归树，得到第t棵树的叶结点区域<span class="math inline">\(R_{tk}\)</span>：<span class="math inline">\(h_t({\bf x})= \sum_{k=1}^K c_k I(\mathbf{X} \in R_{tk})\)</span></p>
<p>​ 更新<span class="math inline">\(F_t({\bf x}) = F_{t-1}({\bf x_i}) + h_t({\bf x}) = F_{t-1}({\bf x_i}) + \sum_{k=1}^K c_k I(\mathbf{X} \in R_{tk})\)</span></p>
<p>得到加法模型</p>
<p>​ <span class="math inline">\(F({\bf x}) = \sum_{t=1}^T h_t({\bf x})\)</span></p>
</blockquote>
<p>可以说，如果我们的任务是回归的话，并且使用RMSE作为损失函数，就和上面的boosting tree一样。因为负梯度算出来就是残差:<span class="math inline">\(\left(y - F_{t-1}({\bf x} ) - h_t({\bf x})\right)^2=(r - h_t({\bf x}))^2\)</span></p>
<h3 id="gbdt-分类">GBDT 分类</h3>
<p>如果要将GBDT用于分类问题，怎么做呢？ 首先要明确的是，GBDT用于分类时使用的仍然是<strong>CART回归树</strong>。回想我们做回归问题的时候，每次对残差（负梯度）进行拟合。而分类问题要怎么每次对残差拟合？要知道类别相减是没有意义的。因此，可以用<strong>Softmax进行概率的映射</strong>，然后拟合<strong>概率的残差</strong>！</p>
<p>具体的做法如下：</p>
<ol type="1">
<li><strong>针对每个类别都先训练一个回归树</strong>，如三个类别，训练三棵树。就是比如对于样本<span class="math inline">\(\bf x_i\)</span>为第二类，则输入三棵树分别为：<span class="math inline">\(({\bf x_i}, 0), ({\bf x_i},1); ({\bf x_i}, 0)\)</span>这其实是典型的OneVsRest的多分类训练方式。 而每棵树的训练过程就是CART的训练过程。这样，对于样本<span class="math inline">\(\bf x_i\)</span>就得出了三棵树的预测值<span class="math inline">\(F_1({\bf x_i}),F_2({\bf x_i}),F_3({\bf x_i})\)</span>，模仿多分类的逻辑回归，用Softmax来产生概率，以类别1为例：<span class="math inline">\(p_{1}({\bf x_i})=\exp(F_{1}{({\bf x_i})})/\sum_{l= 1}^{3}\exp(F_{l}{({\bf x_i})})\)</span></li>
<li>对每个类别分别计算残差，如类别1：<span class="math inline">\({\tilde y}_{i1}= 0 - p_1({\bf x_i})\)</span>, 类别2： <span class="math inline">\({\tilde y}_{i2}= 1 - p_2({\bf x_i})\)</span>， 类别3：<span class="math inline">\({\tilde y}_{i3}= 0 - p_3({\bf x_i})\)</span></li>
<li>开始第二轮的训练，针对第一类 输入为<span class="math inline">\(({\bf x_i}, {\tilde y}_{i1})\)</span>, 针对第二类输入为<span class="math inline">\(({\bf x_i}, {\tilde y}_{i2})\)</span> ，针对第三类输入为 <span class="math inline">\(({\bf x_i}, {\tilde y}_{i3})\)</span>，继续训练出三颗树。</li>
<li>重复3直到迭代M轮，就得到了最后的模型。预测的时候只要找出概率最高的即为对应的类别。</li>
</ol>
<p>和上面的回归问题是大同小异的。</p>
<h4 id="被忽略的第三步">被忽略的第三步</h4>
<p>在原始的论文中，多分类的GBDT描述如下图（注意这里的m代表的是树的个数，N为样本数, K为类别数）</p>
<figure>
<img src="../images/machine-learning-model-ensemble-boostring-and-gbdt/GBDT-classification.png" alt="GBDT-classification" /><figcaption aria-hidden="true">GBDT-classification</figcaption>
</figure>
<p>可以看出，和上面的一样，首先计算各个类的概率。然后对于每个类，计算出残差，然后用CART回归树拟合。这里和上面描述的是一样的。但是之后的倒数第五行那是什么鬼？在原论文中，描述为：拟合完后，每棵树有J个叶结点，对应区域<span class="math inline">\(\{R_{jkm}\}_{j=1}^J\)</span>，模型通过下式的解更新这些<span class="math inline">\({\gamma}_{jkm}\)</span> <span class="math display">\[
{\gamma}_{jkm} =  \mathop{\arg\min}_{\gamma_{jk}} \sum_{i=1}^N\sum_{k=1}^{K}  \phi \left(y_{ik}, F_{k,m-1}({\bf x}) + \sum_{j=1}^J\gamma_{jk}I({\bf x_i} \in R_{jm})\right) \\
\phi(y_{k}, F_k)= -y_k\log p_k = p_{1}({\bf x_i})= -y_k\frac{exp(F_{k}{({\bf x_i})})}{\sum_{l= 1}^{K}exp(F_{l}{({\bf x_i})})}
\]</span> 但是这个式子没有闭式解。因此用牛顿法(Newton-Raphson step)进行近似。就是上面倒数第五行的结果。不过比较丢脸的是，这个我没有推出来。Sklearn中的GBDT实现中(MultinomialDeviance)，又略有不同，因此如果读者你会推的话就告诉博主把~</p>
<p>虽然说没有推出来，但是大多数情况下，可以通过设置步长（Step-size，也有的叫收缩率Shrinkage）的方式来省略这一步。因此很多资料很往往不介绍这一步。关于步长，本文最后有介绍。</p>
<h4 id="sklearn-gbdt二分类实现---binomialdeviance">Sklearn GBDT二分类实现 - BinomialDeviance</h4>
<p>在GBDT二分类中，可以用Logistic函数进行概率映射，而可以不用多分类的SoftMax。</p>
<p>为了挽回一点面子，讲讲sklearn二分类的BinomialDeviance的实现。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BinomialDeviance</span>(<span class="params">ClassificationLossFunction</span>):</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, y, pred, sample_weight=None</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Compute the deviance (= 2 * negative log-likelihood). &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># logaddexp(0, v) == log(1.0 + exp(v))</span></span><br><span class="line">        pred = pred.ravel()</span><br><span class="line">        <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-2.0</span> * np.mean((y * pred) - np.logaddexp(<span class="number">0.0</span>, pred))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (<span class="number">-2.0</span> / sample_weight.sum() *</span><br><span class="line">                    np.sum(sample_weight * ((y * pred) - np.logaddexp(<span class="number">0.0</span>, pred))))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">negative_gradient</span>(<span class="params">self, y, pred, **kargs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Compute the residual (= negative gradient). &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> y - expit(pred.ravel())</span><br></pre></td></tr></table></figure>
<p>首先是损失函数，忽略sample_weight权重设置，看看代码第八行。翻译过来就是 <span class="math display">\[
-2.0 \frac{1}{N}(yP - \log(1 + exp(P)))
\]</span> 怎么感觉和对率损失不太一样？ <span class="math display">\[
\begin{align*}
L = -(y \log(h) + (1-y) \log(1 - h) &amp;= -\left( \log(1 - h) + y \log \left( \frac{h}{1-h}\right)  \right) \\ 
&amp;=-\log(h(-P))- y P\hspace{4ex} 其中h(P) = \frac{1}{1+e^{-P}} \\
&amp;=\log(1+ \exp(P)) - yP
\end{align*}
\]</span> 这正是Sklearn使用的损失函数。</p>
<p>而负的梯度呢？ <span class="math display">\[
-\frac{\partial L}{\partial P}  = y - \frac{\exp(P)}{1+ \exp(P)} = y- h(P)
\]</span> 就是上面的y - expit(pred.ravel())</p>
<p>再来看看牛顿法更新：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_update_terminal_region</span>(<span class="params">self, tree, terminal_regions, leaf, X, y,</span></span></span><br><span class="line"><span class="function"><span class="params">                            residual, pred, sample_weight</span>):</span></span><br><span class="line">    terminal_region = np.where(terminal_regions == leaf)[<span class="number">0</span>]</span><br><span class="line">    residual = residual.take(terminal_region, axis=<span class="number">0</span>)</span><br><span class="line">    y = y.take(terminal_region, axis=<span class="number">0</span>)</span><br><span class="line">    sample_weight = sample_weight.take(terminal_region, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    numerator = np.sum(sample_weight * residual)</span><br><span class="line">    denominator = np.sum(sample_weight * (y - residual) * (<span class="number">1</span> - y + residual))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prevents overflow and division by zero</span></span><br><span class="line">    <span class="keyword">if</span> abs(denominator) &lt; <span class="number">1e-150</span>:</span><br><span class="line">        tree.value[leaf, <span class="number">0</span>, <span class="number">0</span>] = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tree.value[leaf, <span class="number">0</span>, <span class="number">0</span>] = numerator / denominator</span><br></pre></td></tr></table></figure>
<p>主要查看numerator和denominator的计算，这里<span class="math inline">\(residual = y - h(P)\)</span></p>
<p>分子就是上面的负梯度，即<span class="math inline">\(residual = y - h(P)\)</span></p>
<p>分母为二阶导，即<span class="math inline">\(h(P)(1-h(P))= (y-residual)*(1-y+residual))\)</span></p>
<h3 id="gbdt-正则化">GBDT 正则化</h3>
<p>为了避免过拟合，可以从两方面入手：</p>
<ul>
<li>弱算法的个数T， 上面描述的算法中，记得么？就是迭代T轮。T的大小就影响着算法的复杂度</li>
<li>步长（Shrinkage）在每一轮迭代中，原来采用<span class="math inline">\(F_t({\bf x}) = F_{t-1}({\bf x}) + \alpha_th_t({\bf x}; {\bf w_t})\)</span>进行更新，可以加入步长v，使得一次不更新那么多：<span class="math inline">\(F_t({\bf x}) = F_{t-1}({\bf x}) + v \  \alpha_th_t({\bf x}; {\bf w_t}); v\in(0,1]\)</span></li>
</ul>
<h2 id="参考资料">参考资料</h2>
<ul>
<li>机器学习技法 - 林轩田</li>
<li>《机器学习》 - 周志华</li>
<li>《统计学习方法》 - 李航</li>
</ul>
<p>关于Adaboost还可以查阅：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.cs.cmu.edu/~aarti/Class/10701/slides/Lecture10.pdf">Boosting Can we make dumb learners smart</a></li>
<li><a target="_blank" rel="noopener" href="http://www.csuldw.com/2016/08/28/2016-08-28-adaboost-algorithm-theory/">Adaboost - 新的角度理解权值更新策略</a></li>
</ul>
<p>关于GBDT可以查阅</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf">Greedy Function Approximation: A Gradient Boosting Machine</a></li>
<li><a target="_blank" rel="noopener" href="http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf">A Gentle Introduction to Gradient Boosting</a></li>
<li>https://www.cnblogs.com/ModifyRong/p/7744987.html</li>
</ul>
 
      <!-- reward -->
      
      <div id="reword-out">
        <div id="reward-btn">
          打赏
        </div>
      </div>
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>版权声明： </strong>
          
          本博客所有文章除特别声明外，著作权归作者所有。转载请注明出处！
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://www.hrwhisper.me/machine-learning-model-ensemble-boostring-and-gbdt/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning-model/" rel="tag">Machine Learning model</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/machine-learning-xgboost/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            『我爱机器学习』集成学习（三）XGBoost
          
        </div>
      </a>
    
    
      <a href="/machine-learning-model-ensemble-and-bagging/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">『我爱机器学习』集成学习（一）模型融合与Bagging</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "fVcjWMD8aI6F0qEfKdUaHa4f-gzGzoHsz",
    app_key: "b26lBsbwmVyxTSnNrsBrnv3U",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
   
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2013-2021
        <i class="ri-heart-fill heart_icon"></i> hrwhisper
      </li>
    </ul>
    <ul>
      <li>
        
        
        
        由 <a href="https://hexo.io" target="_blank">Hexo</a> 强力驱动
        <span class="division">|</span>
        主题 - <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>

 
  <script src="https://cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
<script src="//cdn1.lncld.net/static/js/2.5.0/av-min.js"></script>
<script type="text/javascript">
var leancloud_app_id  = 'fVcjWMD8aI6F0qEfKdUaHa4f-gzGzoHsz';
var leancloud_app_key = 'b26lBsbwmVyxTSnNrsBrnv3U';

AV.init({
    appId: leancloud_app_id,
    appKey: leancloud_app_key
});

// https://leancloud.cn/docs/leanstorage_guide-js.html#hash1873238850
function showTime(Counter) {
  console.log("show time");
  const query = new AV.Query(Counter);
  const obj = $(".leancloud_visitors");

  let urls = [];
  obj.each(function() {
    urls.push($(this).attr('id').trim());
  });
  query.containedIn('url', urls);
  query.find().then((results) => {
      if (results.length > 0) {
        let data = results;
        obj.each(function() {
          let url = $(this).attr('id').trim();		
          for (let i = 0; i < data.length; i++) {
            let object = data[i];
            let content = object.get('time');
            let _url = object.get('url');
            if(url == _url){
              $(this).text(content);
            }
          }
        });
      }
  }).catch((error) => {
    console.error(error);
  });
}

function addCount(Counter) {
  const obj = $(".leancloud_visitors");
	url = obj.attr('id').trim();
  title = obj.attr('data-flag-title').trim();

  const query = new AV.Query(Counter);
  query.equalTo("url", url);

	query.find().then((results) => {
			if (results.length > 0) {
				var counter = results[0];
        counter.increment("time", 1);
        counter.set("title", title);
				counter.save(null, {fetchWhenSave: true}).then(() => {
          let content = counter.get('time');
          $(document.getElementById(url)).text(content);
        }, (error)=> {
						console.log('Failed to save Visitor num, with error message: ' + error.message);
        });
			} else {
				var newcounter = new Counter();
				newcounter.set("title", title);
				newcounter.set("url", url);
        newcounter.set("time", 1);
        newcounter.save(null, {fetchWhenSave: true}).then(() => {
          var content = newcounter.get('time');
          $(document.getElementById(url)).text(content);
        }, (error)=> {
          console.log('Failed to create' + error.message);
        });
			}
	}).catch((error) => {
    console.error(error);
  });
}

$(function() {
  var Counter = AV.Object.extend("Counter");
	if ($('.leancloud_visitors').length == 1) {
		addCount(Counter);
	} else {
	  showTime(Counter);
  }
}); 
</script>


      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/site/avatar.jpg" alt="细语呢喃"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/blog-building">博客建设</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friend-link">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/leetcode-algorithm-solution">leetcode题解</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about-me">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <script>
      if (window.matchMedia("(max-width: 768px)").matches) {
        document.querySelector('.content').classList.remove('on');
        document.querySelector('.sidebar').classList.remove('on');
      }
    </script>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/donate/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/donate/wechat_pay.png">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3',
    hasInnerContainers: true,
    scrollSmooth: false,
	  scrollSmoothDuration: 420,
    scrollContainer: 'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto',
	  collapseDepth: 2,
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->

<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
  </div>
</body>

</html>