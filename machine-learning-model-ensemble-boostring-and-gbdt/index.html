<!DOCTYPE html>


<html lang="en">


<head>
  <meta charset="utf-8" />
   
  <meta name="keywords" content="c,c++,java,python,leetcode,algorithm,reading,life,moods,machine-learning,data-mining,deep-learning,AI" />
   
  <meta name="description" content="一个分享机器学习、算法与数据结构，个人学习心得、读书笔记、生活的博客。" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    『我爱机器学习』集成学习（二）Boosting与GBDT |  一个分享机器学习、算法与数据结构，个人学习心得、读书笔记、生活的博客。
  </title>
  <meta name="generator" content="hexo-theme-ayer">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/dist/main.css">

  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
  
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

  
  

  

</head>

</html>

<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-machine-learning-model-ensemble-boostring-and-gbdt"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  『我爱机器学习』集成学习（二）Boosting与GBDT
</h1>
 

    </header>
     
    <div class="article-meta">
      <a href="/machine-learning-model-ensemble-boostring-and-gbdt/" class="article-date">
  <time datetime="2018-05-10T11:17:08.000Z" itemprop="datePublished">2018-05-10</time>
</a> 
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/study/">study</a> / <a class="article-category-link" href="/categories/study/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>
 
       
        
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">5.8k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">26 min</span>
        </span>
    </span>
</div>

      
       
        <div class="word_count">
    <span class="post-meta-item-icon">
        <i class="ri-eye-fill"></i> 
        阅读数:<span id="/machine-learning-model-ensemble-boostring-and-gbdt/" data-flag-title="『我爱机器学习』集成学习（二）Boosting与GBDT" class="leancloud_visitors">0</span>次
    </span>
</div>
      
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <p>在上一章节中，我们介绍了模型融合以及Bagging方法，本文主要介绍Boosting相关方法。包括：</p>
<ul>
<li>Boosting</li>
<li>Adaboost</li>
<li>GBDT</li>
</ul>
<a id="more"></a>
<h2 id="Boosting">Boosting</h2>
<p>Boosting也叫提升法，是一类将弱学习器提升为强学习器的算法。</p>
<p>这类算法的工作机制类似：先从初始训练集中训练出一个基学习器，再根据学习器的表现<strong>对训练样本分布进行调整</strong>，使得先前基学习器做错的样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直到基学习器数目达到事先指定的T，最终将T个基学习器进行加权结合。</p>
<h2 id="Adaboost">Adaboost</h2>
<p>首先给出Adaboost的算法：</p>
<blockquote>
<p>输入:   训练集 $Data ={({\bf x_1},y_1),({\bf x_2},y_2),\cdots,({\bf x_N},y_N)}, y_i \in{+1, -1}$;</p>
<p>​            训练轮数T，</p>
<p>​            训练算法G</p>
<p>过程：</p>
<p>​            初始化样本权重 $D_{1,i} = \frac{1}{N}, i=1,2,\cdots,N$</p>
<p>​            for  t = 1 to T do</p>
<p>​                   $h_t = G({\rm Data} , D_t)$                      # 用训练算法G基于权重$D_t$训练出当前的分类器$h_t$</p>
<p>​                   $\epsilon_t = \sum_{i=1}^N D_{t,i}I(h_t(x_i) \ne y_i)$    # 计算误差</p>
<p>​                   if $\epsilon_t &gt; 0.5$ then break</p>
<p>​                   $\alpha_t = \frac{1}{2}\ln(\frac{1 - \epsilon_t }{\epsilon_t})$                              #计算分类器$h_t$的权重</p>
<p>​                   $D_{t+1,i} = \frac{1}{Z_t}\left( D_{t,i} \cdot exp(-y_i\alpha_th_t({\bf x_i}))\right)$   # 更新样本权重，其中，归一化因子：$Z_t = {\sum_{i=1}^N D_{t,i}}\cdot exp\left(-y_i\alpha_th_t({\bf x_i}) \right)$</p>
<p>输出：</p>
<p>​            $H({\bf x}) = sign(\sum_{t=1}^T \alpha_t h_t({\bf x}))$</p>
</blockquote>
<p>一张直观的图如下：</p>
<p><img src="../images/machine-learning-model-ensemble-boostring-and-gbdt/adboost2.png" alt="adboost"></p>
<p>描述起来好像很容易的样子：</p>
<ul>
<li>初始化每个样本权值均等，都为1 / N</li>
<li>然后迭代T轮，在每一轮中学习中根据错误率$\epsilon_t$改变训练数据的权值分布，增加分类错误的权重，减少分类正确的样本权重。这样，新一轮的学习中将会更加重视对之前分错的样本。</li>
<li>将T轮得到的T个分类器$h_t({\bf x})$和对应的分类器权重$\alpha_t$进行线性组合，就得到最终结果$H({\bf x}) = sign(\sum_{t=1}^T \alpha_t h_t({\bf x}))$</li>
</ul>
<p>注意到当错误率$\epsilon \gt 0.5$的时候，我们不再计算。因为对于二分类问题，错误率大于一半意味着不如随机乱猜，留之无用。</p>
<p>Adaboost模型让人困惑的地方有两个：</p>
<ol>
<li>分类器权重$\alpha_t$为啥等于$\frac{1}{2}ln(\frac{1 - \epsilon_t }{\epsilon_t})$</li>
<li>样本权重公式和归一化因子是怎么来的？</li>
</ol>
<h3 id="分类器权重的由来">分类器权重的由来</h3>
<p>Adaboost的分类器权重公式为：$\alpha_t = \frac{1}{2}ln(\frac{1 - \epsilon_t }{\epsilon_t})$ ，这是怎么来的呢？</p>
<p>可以从最小化训练误差界来推导，也可以从最小化损失函数进行推导。这两种本质上是一样的，都是极小化某式得到的$\alpha_t$。</p>
<p>这里采用最小化训练误差界来推导。</p>
<h4 id="训练误差界">训练误差界</h4>
<p>Adaboost的训练误差界为：<br>
$$<br>
\frac{1}{N}\sum_{i=1}^N I \left(H({\bf x_i}) \ne y_i \right) \le  \frac{1}{N}\sum_{i=1}^N exp(-y_if({\bf x_i})) = \prod_{t}Z_t \tag{2-1}<br>
$$<br>
其中$f({\bf x}) = \sum_t \alpha_th_t({\bf x})； H({\bf x}) = \rm sign(f({\bf x}))$</p>
<p>好像很复杂，我们先来不管最后那个等式，只看前面的不等式，就是说指数损失是0-1损失函数的上界。这个很好证明，当$H({\bf x_i}) \ne y_i$时，$-y_if({\bf x_i})&lt; 0  \Rightarrow exp(-y_if({\bf x_i})) \ge 1$。</p>
<p>在证明右边的等式之前，我们先回顾一下权重的定义：<br>
$$<br>
\begin{align*}<br>
&amp; D_{t+1,i} = \frac{D_{t,i}}{Z_t}\cdot \exp(-y_i\alpha_th_t({\bf x_i})) \<br>
\Rightarrow \ &amp; Z_tD_{t+1,i} = D_{t,i}\cdot \exp(-y_i\alpha_th_t({\bf x_i})) \tag{2-2}<br>
\end{align*}<br>
$$<br>
现推导如下：<br>
$$<br>
\begin{align*}<br>
\frac{1}{N}\sum_{i=1}^N \exp(-y_if({\bf x_i})) &amp;=  \frac{1}{N}\sum_{i=1}^N \exp\left (\sum_{t=1}^T-y_i\alpha_th_t({\bf x})\right)<br>
\&amp; =\sum_{i=1}^N D_{1,i} \prod_{t=1}^{T} \exp\left (-y_i\alpha_th_t({\bf x})\right)   \hspace{5ex} D_{1,i}= \frac{1}{N}<br>
\&amp; =Z_1\sum_{i=1}^N D_{2,i} \prod_{t=2}^{T}  \exp\left (-y_i\alpha_th_t({\bf x})\right)  \hspace{5ex} 式2-2<br>
\&amp; =Z_1Z_2\sum_{i=1}^N D_{3,i} \prod_{t=3}^{T}  \exp\left (-y_i\alpha_th_t({\bf x})\right)<br>
\&amp; =Z_1Z_2\cdots Z_{T-1}\sum_{i=1}^N D_{T,i} \prod_{t=T}^{T}  \exp\left (-y_i\alpha_th_t({\bf x})\right)<br>
\&amp; =Z_1Z_2\cdots Z_{T}\sum_{i=1}^N D_{T + 1,i}<br>
\&amp; =Z_1Z_2\cdots Z_{T}\sum_{i=1}^N D_{T + 1,i}   \hspace{7ex}  \sum_{i=1}^N D_{T + 1,i}=1<br>
\&amp; =\prod_{t=1}^TZ_t<br>
\end{align*}<br>
$$<br>
即我们最小化$\prod_{t=1}^TZ_t$等价于最小化指数损失函数，我们可以在每一轮都最小化$Z_t$<br>
$$<br>
\begin{align*}<br>
Z_t &amp;= {\sum_{i=1}^N D_{t,i}}\cdot \exp\left(-y_i\alpha_th_t({\bf x_i}) \right)<br>
\&amp;= {\sum_{i: \ y_i \ne h_t(x_i)} D_{t,i}}\exp(\alpha_t) + {\sum_{i: \ y_i = h_t(x_i)} D_{t,i}}\exp(-\alpha_t)<br>
\&amp;=  \epsilon_te^{\alpha_t} + (1 - \epsilon_t)e^{-\alpha_t}<br>
\end{align*}\tag{2-3}<br>
$$<br>
<strong>对$\alpha_t$求导得得到权重公式</strong>：<br>
$$<br>
\frac{\partial Z_t}{\partial \alpha_t} = \epsilon_te^{\alpha_t} - (1 - \epsilon_t)e^{-\alpha_t} = 0 \hspace{4ex}\Rightarrow  \alpha_t= \frac{1}{2}\ln\frac{1-\epsilon_t}{\epsilon_t}\tag{2-4}<br>
$$<br>
将2-4带入2-3得：<br>
$$<br>
Z_t = 2\sqrt{\epsilon_t(1-\epsilon_t)} = \sqrt{1 - (1-2\epsilon_t)^2} = \sqrt{1 - 4\gamma_t^2} \hspace{4ex}  设\gamma_t = \frac{1}{2} - \epsilon_t<br>
$$<br>
因此，2-1的训练误差界可以写为：<br>
$$<br>
\frac{1}{N}\sum_{i=1}^N I \left(H({\bf x_i}) \ne y_i \right) \le \prod_{t}Z_t = \prod_{t} \sqrt{1 - 4\gamma_t^2}  \le exp(-2\sum_{t=1}^T\gamma_t^2)\tag{2-5}<br>
$$<br>
李航老师的书中说：2-5的最后一个式子可先由$e^x$和$\sqrt{1-x}$在点x = 0的泰勒展开式推出不等式$\sqrt{1 - 4\gamma_t^2} \le exp(-2\gamma_t^2)$进而得到。</p>
<p>这表明Adaboost的训练误差是<strong>以指数速率</strong>下降的。</p>
<p>PS： Adaboost能适应弱分类器各自的训练误差，这也是它的名字（适应的提升）的由来。Ada是Adaptive的简写。</p>
<h2 id="加法模型和前向分布算法">加法模型和前向分布算法</h2>
<p>本小节主要介绍加法模型。</p>
<p>Adaboost可以看出是<strong>加法模型</strong>、损失函数为指数损失函数、学习算法为<strong>前向分布算法</strong>时的二分类学习方法。此外，之后要介绍的GBDT也可以看出是加法模型。</p>
<p>加法模型即：<br>
$$<br>
F({\bf x}) =\sum_{t=1}^T \alpha_t h_t({\bf x}; {\bf w_t}) = \sum_{i=1}^Tf_t({\bf x}; {\bf w_t}) \tag{3-1}<br>
$$<br>
其中,$h_t({\bf x}; {\bf w_t})$为基学习器， $\bf x$为输入样本，$\bf w$为基学习器的参数，而$\alpha_t$为每个基学习器$h_t$的权重</p>
<p>可以通过最小化损失函数进行求解，即求经验风险极小化问题：<br>
$$<br>
\min_{\alpha,\bf w}  \sum_{i=1}^N L\left(y_i, \sum_{t=1}^T \alpha_t h_t({\bf x}; {\bf w_t})\right)\tag{3-2}<br>
$$<br>
要求解这个问题，是NP难的，一般用贪心法进行求解，即<strong>前向分布算法</strong>：从前往后，每一步只学习一个基函数及其系数，逐步优化3-2式。前向分布算法可以描述如下：</p>
<blockquote>
<p>输入:   训练集 $Data ={({\bf x_1},y_1),({\bf x_2},y_2),\cdots,({\bf x_N},y_N)}, y_i \in{+1, -1}$;</p>
<p>​            损失函数L</p>
<p>过程：</p>
<p>​            for  t = 1 to T do</p>
<p>​                   $(\alpha_t, w_t) = \arg\min_{\alpha_t,w_t}  \sum_{i=1}^N L\left(y_i,  f_{t-1}({\bf x_i}) + \alpha_t h_t({\bf x}; {\bf w_t})\right)$ # 得到参数和基学习器权重</p>
<p>​               	    更新$f_t({\bf x}) = f_{t-1}({\bf x_i}) + \alpha_t h_t({\bf x}; {\bf w_t})$</p>
<p>得到加法模型</p>
<p>​            $F({\bf x}) = f_{t}({\bf x}) = \sum_{t=1}^T \alpha_t h_t({\bf x}; {\bf w_t})$</p>
</blockquote>
<p>这样，便将同时求解t = 1到T的所有参数$\alpha_t, \bf w_t$的优化问题转化为逐步求解各个$\alpha_t, \bf w_t$的问题。</p>
<h3 id="再谈Adaboost">再谈Adaboost</h3>
<p>上面说到，Adaboost是加法模型，损失函数为指数损失函数，并采用前向分布算法。</p>
<p>下面进行证明：</p>
<blockquote>
<p>前向分布分布算法学习的是加法模型，当基函数为基本分类器时，该加法模型等价于Adaboost的最终分类器。$H({\bf x}) = \sum_{t=1}^T \alpha_t h_t({\bf x})$</p>
</blockquote>
<h4 id="指数损失函数">指数损失函数</h4>
<p>首先证明前向分布算法的损失函数是指数损失函数$L(y, F({\bf x})) = \exp(-yF({\bf x}))$时，其学习的具体操作等价于Adaboost算法学习的具体操作。</p>
<p>假设经过t - 1轮迭代前向分步算法已经得到$f_{t-1}({\bf x}) = f_{t-2}({\bf x}) + \alpha_{t-1}h_{t-1}({\bf x}) =  \alpha_{1}h_1({\bf x}) + \cdots +  \alpha_{t-1}h_{t-1}({\bf x})$</p>
<p>则在第t轮迭代应该寻找最优的$\alpha_t, h_t$使得指数损失最小即  $(\alpha_t, h_t) = \arg\min_{\alpha_t,h_t}  \sum_{i=1}^N L\left(y_i,  f_{t-1}({\bf x_i}) + \alpha_t h_t({\bf x})\right)$</p>
<p>从而得到$f_{t}({\bf x}) = f_{t-1}({\bf x}) + \alpha_{t}h_t({\bf x})$。而指数损失最小可以写为：<br>
$$<br>
\begin{align*}<br>
(\alpha_t, h_t) &amp;=\mathop{\arg\min}<em>{\alpha_t,h_t}  \sum</em>{i=1}^N \exp\left(-y_i(f_{t-1}({\bf x_i}) + \alpha_th_t({\bf x}))\right) \<br>
&amp;=\mathop{\arg\min}<em>{\alpha_t,h_t}  \sum</em>{i=1}^N D_{t,i}\exp\left( -y_i\alpha_th_t({\bf x})\right)\tag{3-3}<br>
\end{align*}<br>
$$<br>
其中$D_{ti} = Error_{(t-1,i)} = \exp(-y_if_{t-1}({\bf x_i}))$与当前的优化目标无关，只与之前的有关，因此为指数损失函数。</p>
<h4 id="基分类器求解和权重推导">基分类器求解和权重推导</h4>
<p>接着证明使得式子3-3达到最小的$\alpha_t^<em>$和$h_t^</em>({\bf x})$就是Adaboost算法得到的$\alpha_t$和$h_t({\bf x})$。</p>
<p>求解3-3可以分为两步，首先求$h_t({\bf x})$，对于任意的$\alpha_t \gt 0$，使3-3最小的$h_t({\bf x})$由下式得到：<br>
$$<br>
h_t^*({\bf x}) = \mathop{\arg\min}<em>h \sum</em>{i=1}^ND_{t,i}I(y_i\ne h({\bf x}))<br>
$$<br>
其中$D_{t,i}= \exp(-y_if_{t-1}(\bf x_i))$。</p>
<p>这样就得到了基分类器$h_t^*({\bf x})$，而该分类器就是Adaboost算法的基本分类器$h_t({\bf x})$，因为它是第t轮加权训练数据分类误差率最小的基本分类器。</p>
<p>接下来求$\alpha_t$, 式在3-3中：<br>
$$<br>
\begin{align*}<br>
\sum_{i=1}^N D_{t,i}\exp\left( -y_i\alpha_th_t({\bf x})\right)<br>
&amp; = \sum_{y_i=h_t(\bf x)}D_{t,i} e^{-\alpha} +  \sum_{y_i\ne h_t(\bf x)}D_{t,i}e^{\alpha}  \<br>
&amp;=e^{-\alpha}\sum_{i=1}^ND_{t,i} + (e^{\alpha} - e^{-\alpha})\sum_{i=1}^N D_{t,i}I(y_i\ne h_t({\bf x_i})) \tag{3-4}<br>
\end{align*}<br>
$$<br>
最后一个变换可以认为是第一项多加了不相等时候的$e^{-\alpha}$, 因此在第二项的时候减去。1-8对$\alpha$求导，并使导数为0，得到使得3-3最小的$\alpha_t$<br>
$$<br>
\alpha_t = \frac{1}{2} \ln(\frac{1 - \epsilon_t }{\epsilon_t})<br>
$$<br>
其中$\epsilon_t$为分类误差率<br>
$$<br>
\epsilon_t = \frac{\sum_{i=1}^N D_{t,i}I(y_i\ne h_t({\bf x}))}{\sum_{i=1}^N D_{t,i}} = \sum_{i=1}^N \tilde D_{t,i}I(h_t(x_i) \ne y_i)<br>
$$<br>
可以看出和之前更新$\alpha_t$一致。</p>
<p>最后来看每一轮样本的权值更新，由$f_{t}({\bf x}) = f_{t-1}({\bf x}) + \alpha_{t}h_t({\bf x})$和$D_{t,i}= \exp(-y_if_{t-1}(\bf x_i))$可得：<br>
$$<br>
\begin{align*}<br>
D_{t + 1,i}<br>
&amp;= \exp(-y_if_{t}(\bf x_i))\<br>
&amp;= \exp(-y_i( f_{t-1}({\bf x_i}) + \alpha_{t}h_t({\bf x_i}))\<br>
&amp;= D_{(t,i)}\exp(- \alpha_{t}y_{i}h_t({\bf x_i}))\<br>
\end{align*}<br>
$$<br>
这与之前的Adaboost的样本权值更新公式相比，只差规范化因子，因而等价。</p>
<h2 id="GBDT">GBDT</h2>
<p>GBDT全称为：Gradient Boosting Decision Tree，即梯度提升决策树。可以理解为<strong>梯度 + 提升 + 决策树</strong>。</p>
<p>在介绍GBDT之前，先介绍比较简单的提升树（Boosting Decision Tree）</p>
<h3 id="提升树">提升树</h3>
<p>提升树实际上就是加法模型和前向分布算法，然后以CART决策树为基学习器。</p>
<p>可以表示为决策树的前向分布算法<br>
$$<br>
\begin{align*}<br>
F_0({\bf x}) &amp;= 0\<br>
F_t({\bf x}) &amp;= F_{t-1}({\bf x})  + h_t({\bf x}), t=1,2,\cdots ,T\<br>
F_T({\bf x}) &amp;= \sum_{t=1}^Th_t({\bf x})<br>
\end{align*}<br>
$$</p>
<p>在前向分布算法的第t步，给定当前模型$F_{t-1}({\bf x})$，需要求解<br>
$$<br>
h_t^*({\bf x}) = \mathop{\arg\min}<em>{h_t} \sum</em>{i=1}^NL(y_i, F_{t-1}({\bf x}) + h_t({\bf x}))<br>
$$<br>
得到第t棵树$F_{t}({\bf x})$。针对不同问题的提升树学习方法，<strong>主要区别在于使用的损失函数不同</strong>。比如平方误差损失函数的回归问题，用指数损失函数的分类问题，以及用一般损失函数的问题。</p>
<p>采用平方损失函数时，有：<br>
$$<br>
\begin{align*}<br>
L\left(y,  F_{t-1}({\bf x}\right)  + h_t({\bf x}))<br>
&amp;=\left(y -  F_{t-1}({\bf x} ) - h_t({\bf x})\right)^2\<br>
&amp;=(r - h_t({\bf x}))^2<br>
\end{align*}<br>
$$<br>
其中$r = y -  F_{t-1}({\bf x})$ 称为<strong>残差</strong>(Residual)。因此，对<strong>回归树的提升树算法来说，只需要简单的拟合当前模型的残差</strong>即可。注意这里的是回归树。</p>
<p>因此回归问题的提升树算法可以描述如下：</p>
<blockquote>
<p>输入:   训练集 $Data ={({\bf x_1},y_1),({\bf x_2},y_2),\cdots,({\bf x_N},y_N)}, y_i \in{+1, -1}$;</p>
<p>过程：</p>
<p>​         初始化$F_0({\bf x}) = 0$</p>
<p>​         for  t = 1 to T do</p>
<p>​                    计算残差 $r_{ti} = y_i - F_{t-1}({\bf x}) , \ i=1,2,\cdots,N$</p>
<p>​                    拟合残差得到一个回归树：$h_t({\bf x})$</p>
<p>​                    更新$F_t({\bf x}) = F_{t-1}({\bf x_i}) +  h_t({\bf x})$</p>
<p>得到加法模型$F({\bf x}) = \sum_{t=1}^T  h_t({\bf x})$</p>
</blockquote>
<p>该算法可以用下面的图表示：</p>
<p><img src="../images/machine-learning-model-ensemble-boostring-and-gbdt/Boosting-tree.png" alt="Boosting tree"></p>
<p><strong>对比Adaboost来说，该算法可以说是修改样本的&quot;label&quot;，而AdaBoost则是修改样本的权重。</strong></p>
<h4 id="例子">例子</h4>
<p>继续搬出之前的决策树中的例子</p>
<table>
<thead>
<tr>
<th>x</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr>
<td>y</td>
<td>5.56</td>
<td>5.70</td>
<td>5.91</td>
<td>6.40</td>
<td>6.80</td>
<td>7.05</td>
<td>8.90</td>
<td>8.70</td>
<td>9.00</td>
<td>9.05</td>
</tr>
</tbody>
</table>
<p>第一轮计算后，我们算出（计算过程看决策树那章）<br>
$$<br>
h_1(x) =<br>
\begin{cases}<br>
6.24, &amp; x\le 6.5 \<br>
8.91, &amp; x \gt 6.5 \<br>
\end{cases}<br>
$$<br>
可以算出残差（比如x=1就是5.56 - 6.24 = -0.68）</p>
<table>
<thead>
<tr>
<th>x</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr>
<td>y</td>
<td>-0.68</td>
<td>-0.54</td>
<td>-0.33</td>
<td>0.16</td>
<td>0.56</td>
<td>0.81</td>
<td>-0.01</td>
<td>-0.21</td>
<td>0.09</td>
<td>0.14</td>
</tr>
</tbody>
</table>
<p>接着，继续拟合数据，只是拟合的是上面的残差。得到：<br>
$$<br>
h_2(x) =<br>
\begin{cases}<br>
-0.52, &amp; x\le 3.5 \<br>
0.22, &amp; x \gt 3.5 \<br>
\end{cases}<br>
$$<br>
则$f_2({\bf x})$为：<br>
$$<br>
F_2({\bf x}) = f_1({\bf x}) + f_2({\bf x}) =<br>
\begin{cases}<br>
5.72, &amp; x\le 3.5 \<br>
6.46, &amp; 3.5\lt x \le 6.5 \<br>
9.13, &amp; 6.5\lt x<br>
\end{cases}<br>
$$<br>
以此类推进行计算即可，直到损失$L(y, F_t(\bf x))$满足要求，比如达到迭代次数T。</p>
<p>和决策树CART不同的地方在于，BDT每次拟合的是残差，而CART是经典的分治算法（divide and conquer），不断的划分子空间并在子空间中进一步精确的拟合。</p>
<h3 id="梯度提升">梯度提升</h3>
<p>前面提到过，GBDT全称为<strong>Gradient Boosting</strong> Decision Tree，<strong>梯度提升</strong>决策树。</p>
<p>现在我们来介绍通用的“梯度提升”算法。</p>
<p>当采用平方损失和指数损失的时候，每一步优化是很简单的，但是<strong>一般的损失函数来说，每一步的优化不容易</strong>，因此提出了梯度提升的方法，这是利用梯度下降的近似方法，其关键是利用<strong>损失函数的负梯度在当前模型的值</strong>：<br>
$$<br>
-\left[\frac{\partial L(y_i, F({\bf x_i}))}{\partial F({\bf x_i})}  \right]<em>{F({\bf x}) = F</em>{t-1}({\bf x})}\tag{4-1}<br>
$$<br>
怎么理解这个近似呢？</p>
<p>以前面的均方损失函数为例，也是可以用这个方法来解释的。为了求导方便，我们在均方损失函数前乘以 1/2。<br>
$$<br>
\begin{align*}<br>
L(y_i, F({\bf x_i})) = \frac{1}{2}(y_i - F({\bf x_i}))^2<br>
\end{align*}<br>
$$<br>
注意到$F({\bf x_i})$其实只是一些数字而已，我们可以将其像变量一样进行求导：<br>
$$<br>
\begin{align*}<br>
\frac{\partial L(y_i, F({\bf x_i}))}{\partial F({\bf x_i})} = F({\bf x_i}) - y_i<br>
\end{align*}<br>
$$<br>
而前面所说的残差就是上式相反数，即<strong>负梯度</strong>：<br>
$$<br>
r_{ti} = y_i - F_{t-1}({\bf x}) =-\left[\frac{\partial L(y_i, F({\bf x_i}))}{\partial F({\bf x_i})}  \right]<em>{F({\bf x}) = F</em>{t-1}({\bf x})}<br>
$$<br>
在梯度提升中，就是将式2-1作为残差来进行拟合。由此，我们给出一般的梯度提升算法：</p>
<blockquote>
<p>输入:   训练集 $Data ={({\bf x_1},y_1),({\bf x_2},y_2),\cdots,({\bf x_N},y_N)}, y_i \in{+1, -1}$;</p>
<p>过程：</p>
<p>​            初始化$F_0({\bf x}) =\mathop{\arg\min}<em>{h_0} \sum</em>{i=1}^NL(y_i,  h_0({\bf x}))$</p>
<p>​            for  t = 1 to T do</p>
<p>​                    计算负梯度 ${\tilde y}<em>{i} = -\left[\frac{\partial L(y_i, F({\bf x_i}))}{\partial F({\bf x_i})}  \right]</em>{F({\bf x}) = F_{t-1}({\bf x})} , \ i=1,2,\cdots,N$</p>
<p>​                    $w_t = \arg\min_{w_t}  \sum_{i=1}^N \left({\tilde y}_{i} - h_t({\bf x}; {\bf w_t})\right)^2$ # 拟合“残差“得到基学习器权重，也就得到了基学习器</p>
<p>​                    $\alpha_t= \arg\min_{\alpha_t}  \sum_{i=1}^N L\left(y_i,  f_{t-1}({\bf x_i}) + \alpha_t h_t({\bf x}; {\bf w_t})\right)$ # 得到基学习器权重$\alpha_t$</p>
<p>​               	    更新$F_t({\bf x}) = F_{t-1}({\bf x_i}) +  \alpha_th_t({\bf x}; {\bf w_t})$</p>
</blockquote>
<p>对比提升树来说，提升树没有基学习器参权重$\alpha_t$</p>
<h3 id="GBDT-2">GBDT</h3>
<p>至此，我们可以给出GBDT的算法了。就是采用梯度提升的决策树（CART）而已嘛。PS: 上面给出的是梯度提升。</p>
<p>前面提到过，CART回归将空间划分为K个不相交的区域。可以用数学公式描述为：<br>
$$<br>
f(\mathbf{X}) = \sum_{k=1}^K c_k I(\mathbf{X} \in R_k)<br>
$$<br>
GBDT算法(回归)描述如下：</p>
<blockquote>
<p>输入:   训练集 $Data ={({\bf x_1},y_1),({\bf x_2},y_2),\cdots,({\bf x_N},y_N)}, y_i \in{+1, -1}$;</p>
<p>过程：</p>
<p>​            初始化$F_0({\bf x}) =\mathop{\arg\min}<em>{h_0} \sum</em>{i=1}^NL(y_i,  h_0({\bf x})) =\mathop{\arg\min}<em>{c} \sum</em>{i=1}^NL(y_i, c))$</p>
<p>​            for  t = 1 to T do</p>
<p>​                    计算残差 ${\tilde y}<em>{i} = -\left[\frac{\partial L(y_i, F({\bf x_i}))}{\partial F({\bf x_i})}  \right]</em>{F({\bf x}) = F_{t-1}({\bf x})} , \ i=1,2,\cdots,N$</p>
<p>​                    拟合残差${\tilde y}<em>{i}$得到一个回归树，得到第t棵树的叶结点区域$R</em>{tk}$：$h_t({\bf x})= \sum_{k=1}^K c_k I(\mathbf{X} \in R_{tk})$</p>
<p>​               	    更新$F_t({\bf x}) = F_{t-1}({\bf x_i}) +  h_t({\bf x}) = F_{t-1}({\bf x_i}) + \sum_{k=1}^K c_k I(\mathbf{X} \in R_{tk})$</p>
<p>得到加法模型</p>
<p>​            $F({\bf x}) = \sum_{t=1}^T  h_t({\bf x})$</p>
</blockquote>
<p>可以说，如果我们的任务是回归的话，并且使用RMSE作为损失函数，就和上面的boosting tree一样。因为负梯度算出来就是残差:$\left(y -  F_{t-1}({\bf x} ) - h_t({\bf x})\right)^2=(r - h_t({\bf x}))^2$</p>
<h3 id="GBDT-分类">GBDT 分类</h3>
<p>如果要将GBDT用于分类问题，怎么做呢？ 首先要明确的是，GBDT用于分类时使用的仍然是<strong>CART回归树</strong>。回想我们做回归问题的时候，每次对残差（负梯度）进行拟合。而分类问题要怎么每次对残差拟合？要知道类别相减是没有意义的。因此，可以用<strong>Softmax进行概率的映射</strong>，然后拟合<strong>概率的残差</strong>！</p>
<p>具体的做法如下：</p>
<ol>
<li><strong>针对每个类别都先训练一个回归树</strong>，如三个类别，训练三棵树。就是比如对于样本$\bf x_i$为第二类，则输入三棵树分别为：$({\bf x_i}, 0), ({\bf x_i},1); ({\bf x_i}, 0)$这其实是典型的OneVsRest的多分类训练方式。 而每棵树的训练过程就是CART的训练过程。这样，对于样本$\bf x_i$就得出了三棵树的预测值$F_1({\bf x_i}),F_2({\bf x_i}),F_3({\bf x_i})$，模仿多分类的逻辑回归，用Softmax来产生概率，以类别1为例：$p_{1}({\bf x_i})=\exp(F_{1}{({\bf x_i})})/\sum_{l= 1}^{3}\exp(F_{l}{({\bf x_i})})$</li>
<li>对每个类别分别计算残差，如类别1：${\tilde y}<em>{i1}= 0 - p_1({\bf x_i})$,  类别2： ${\tilde y}</em>{i2}= 1 - p_2({\bf x_i})$， 类别3：${\tilde y}_{i3}= 0 - p_3({\bf x_i})$</li>
<li>开始第二轮的训练，针对第一类 输入为$({\bf x_i}, {\tilde y}<em>{i1})$, 针对第二类输入为$({\bf x_i}, {\tilde y}</em>{i2})$ ，针对第三类输入为 $({\bf x_i}, {\tilde y}_{i3})$，继续训练出三颗树。</li>
<li>重复3直到迭代M轮，就得到了最后的模型。预测的时候只要找出概率最高的即为对应的类别。</li>
</ol>
<p>和上面的回归问题是大同小异的。</p>
<h4 id="被忽略的第三步">被忽略的第三步</h4>
<p>在原始的论文中，多分类的GBDT描述如下图（注意这里的m代表的是树的个数，N为样本数, K为类别数）</p>
<p><img src="../images/machine-learning-model-ensemble-boostring-and-gbdt/GBDT-classification.png" alt="GBDT-classification"></p>
<p>可以看出，和上面的一样，首先计算各个类的概率。然后对于每个类，计算出残差，然后用CART回归树拟合。这里和上面描述的是一样的。但是之后的倒数第五行那是什么鬼？在原论文中，描述为：拟合完后，每棵树有J个叶结点，对应区域${R_{jkm}}<em>{j=1}^J$，模型通过下式的解更新这些${\gamma}</em>{jkm}$<br>
$$<br>
{\gamma}<em>{jkm} =  \mathop{\arg\min}</em>{\gamma_{jk}} \sum_{i=1}^N\sum_{k=1}^{K}  \phi \left(y_{ik}, F_{k,m-1}({\bf x}) + \sum_{j=1}^J\gamma_{jk}I({\bf x_i} \in R_{jm})\right) \<br>
\phi(y_{k}, F_k)= -y_k\log p_k = p_{1}({\bf x_i})= -y_k\frac{exp(F_{k}{({\bf x_i})})}{\sum_{l= 1}^{K}exp(F_{l}{({\bf x_i})})}<br>
$$<br>
但是这个式子没有闭式解。因此用牛顿法(Newton-Raphson step)进行近似。就是上面倒数第五行的结果。不过比较丢脸的是，这个我没有推出来。Sklearn中的GBDT实现中(MultinomialDeviance)，又略有不同，因此如果读者你会推的话就告诉博主把~</p>
<p>虽然说没有推出来，但是大多数情况下，可以通过设置步长（Step-size，也有的叫收缩率Shrinkage）的方式来省略这一步。因此很多资料很往往不介绍这一步。关于步长，本文最后有介绍。</p>
<h4 id="Sklearn-GBDT二分类实现-BinomialDeviance">Sklearn GBDT二分类实现 - BinomialDeviance</h4>
<p>在GBDT二分类中，可以用Logistic函数进行概率映射，而可以不用多分类的SoftMax。</p>
<p>为了挽回一点面子，讲讲sklearn二分类的BinomialDeviance的实现。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BinomialDeviance</span>(<span class="params">ClassificationLossFunction</span>):</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, y, pred, sample_weight=None</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Compute the deviance (= 2 * negative log-likelihood). &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># logaddexp(0, v) == log(1.0 + exp(v))</span></span><br><span class="line">        pred = pred.ravel()</span><br><span class="line">        <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-2.0</span> * np.mean((y * pred) - np.logaddexp(<span class="number">0.0</span>, pred))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (<span class="number">-2.0</span> / sample_weight.sum() *</span><br><span class="line">                    np.sum(sample_weight * ((y * pred) - np.logaddexp(<span class="number">0.0</span>, pred))))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">negative_gradient</span>(<span class="params">self, y, pred, **kargs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Compute the residual (= negative gradient). &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> y - expit(pred.ravel())</span><br></pre></td></tr></table></figure>
<p>首先是损失函数，忽略sample_weight权重设置，看看代码第八行。翻译过来就是<br>
$$<br>
-2.0 \frac{1}{N}(yP - \log(1 + exp§))<br>
$$<br>
怎么感觉和对率损失不太一样？<br>
$$<br>
\begin{align*}<br>
L = -(y \log(h) + (1-y) \log(1 - h) &amp;= -\left( \log(1 - h) + y \log \left( \frac{h}{1-h}\right)  \right) \<br>
&amp;=-\log(h(-P))- y P\hspace{4ex} 其中h§ = \frac{1}{1+e^{-P}} \<br>
&amp;=\log(1+ \exp§) - yP<br>
\end{align*}<br>
$$<br>
这正是Sklearn使用的损失函数。</p>
<p>而负的梯度呢？<br>
$$<br>
-\frac{\partial L}{\partial P}  = y - \frac{\exp§}{1+ \exp§} = y- h§<br>
$$<br>
就是上面的y - expit(pred.ravel())</p>
<p>再来看看牛顿法更新：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_update_terminal_region</span>(<span class="params">self, tree, terminal_regions, leaf, X, y,</span></span></span><br><span class="line"><span class="function"><span class="params">                            residual, pred, sample_weight</span>):</span></span><br><span class="line">    terminal_region = np.where(terminal_regions == leaf)[<span class="number">0</span>]</span><br><span class="line">    residual = residual.take(terminal_region, axis=<span class="number">0</span>)</span><br><span class="line">    y = y.take(terminal_region, axis=<span class="number">0</span>)</span><br><span class="line">    sample_weight = sample_weight.take(terminal_region, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    numerator = np.sum(sample_weight * residual)</span><br><span class="line">    denominator = np.sum(sample_weight * (y - residual) * (<span class="number">1</span> - y + residual))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prevents overflow and division by zero</span></span><br><span class="line">    <span class="keyword">if</span> abs(denominator) &lt; <span class="number">1e-150</span>:</span><br><span class="line">        tree.value[leaf, <span class="number">0</span>, <span class="number">0</span>] = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tree.value[leaf, <span class="number">0</span>, <span class="number">0</span>] = numerator / denominator</span><br></pre></td></tr></table></figure>
<p>主要查看numerator和denominator的计算，这里$residual = y - h§$</p>
<p>分子就是上面的负梯度，即$residual  = y - h§$</p>
<p>分母为二阶导，即$h§(1-h§)= (y-residual)*(1-y+residual))$</p>
<h3 id="GBDT-正则化">GBDT 正则化</h3>
<p>为了避免过拟合，可以从两方面入手：</p>
<ul>
<li>弱算法的个数T， 上面描述的算法中，记得么？就是迭代T轮。T的大小就影响着算法的复杂度</li>
<li>步长（Shrinkage）在每一轮迭代中，原来采用$F_t({\bf x}) = F_{t-1}({\bf x}) +  \alpha_th_t({\bf x}; {\bf w_t})$进行更新，可以加入步长v，使得一次不更新那么多：$F_t({\bf x}) = F_{t-1}({\bf x}) +  v \  \alpha_th_t({\bf x}; {\bf w_t}); v\in(0,1]$</li>
</ul>
<h2 id="参考资料">参考资料</h2>
<ul>
<li>机器学习技法 - 林轩田</li>
<li>《机器学习》 - 周志华</li>
<li>《统计学习方法》 - 李航</li>
</ul>
<p>关于Adaboost还可以查阅：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.cs.cmu.edu/~aarti/Class/10701/slides/Lecture10.pdf">Boosting Can we make dumb learners smart</a></li>
<li><a target="_blank" rel="noopener" href="http://www.csuldw.com/2016/08/28/2016-08-28-adaboost-algorithm-theory/">Adaboost - 新的角度理解权值更新策略</a></li>
</ul>
<p>关于GBDT可以查阅</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf">Greedy Function Approximation: A Gradient Boosting Machine</a></li>
<li><a target="_blank" rel="noopener" href="http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf">A Gentle Introduction to Gradient Boosting</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/ModifyRong/p/7744987.html">https://www.cnblogs.com/ModifyRong/p/7744987.html</a></li>
</ul>
 
      <!-- reward -->
      
      <div id="reword-out">
        <div id="reward-btn">
          Donate
        </div>
      </div>
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>Copyright： </strong>
          
          Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source.
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=http://example.com/machine-learning-model-ensemble-boostring-and-gbdt/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning-model/" rel="tag">Machine Learning model</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/machine-learning-xgboost/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            『我爱机器学习』集成学习（三）XGBoost
          
        </div>
      </a>
    
    
      <a href="/machine-learning-model-ensemble-and-bagging/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">『我爱机器学习』集成学习（一）模型融合与Bagging</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "fVcjWMD8aI6F0qEfKdUaHa4f-gzGzoHsz",
    app_key: "b26lBsbwmVyxTSnNrsBrnv3U",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2013-2020
        <i class="ri-heart-fill heart_icon"></i> hrwhisper
      </li>
    </ul>
    <ul>
      <li>
        
        
        
        Powered by <a href="https://hexo.io" target="_blank">Hexo</a>
        <span class="division">|</span>
        Theme - <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></s>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>

 
  <script src="https://cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
<script src="//cdn1.lncld.net/static/js/2.5.0/av-min.js"></script>
<script type="text/javascript">
var leancloud_app_id  = 'fVcjWMD8aI6F0qEfKdUaHa4f-gzGzoHsz';
var leancloud_app_key = 'b26lBsbwmVyxTSnNrsBrnv3U';

AV.init({
    appId: leancloud_app_id,
    appKey: leancloud_app_key
});

// https://leancloud.cn/docs/leanstorage_guide-js.html#hash1873238850
function showTime(Counter) {
  console.log("show time");
	let query = new AV.Query(Counter);
  query.greaterThanOrEqualTo("time", 0);		
  query.find().then((results) => {
      if (results.length > 0) {
        let data = results;
        $('.leancloud_visitors').each(function() {
          let url = $(this).attr('id').trim();		
          for (let i = 0; i < data.length; i++) {
            let object = data[i];
            let content = object.get('time');
            let _url = object.get('url');
            if(url == _url){
              $(this).text(content);
            }
          }
        });
      }
  });
}

function addCount(Counter) {
  const obj = $(".leancloud_visitors");
	url = obj.attr('id').trim();
  title = obj.attr('data-flag-title').trim();

  const query = new AV.Query('Counter');
  query.equalTo("url", url);

	query.find().then((results) => {
			if (results.length > 0) {
				var counter = results[0];
				counter.increment("time", 1);
				counter.save(null, {fetchWhenSave: true}).then(() => {
          let content = counter.get('time');
          $(document.getElementById(url)).text(content);
        }, (error)=> {
						console.log('Failed to save Visitor num, with error message: ' + error.message);
        });
			} else {
				var newcounter = new Counter();
				newcounter.set("title", title);
				newcounter.set("url", url);
        newcounter.set("time", 1);
        newcounter.save(null, {fetchWhenSave: true}).then(() => {
          var content = newcounter.get('time');
          $(document.getElementById(url)).text(content);
        }, (error)=> {
          console.log('Failed to create' + error.message);
        });
			}
	});
}

$(function() {
  var Counter = AV.Object.extend("Counter");
	if ($('.leancloud_visitors').length == 1) {
		addCount(Counter);
	} else {
	  showTime(Counter);
  }
}); 
</script>


      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/site/logo.jpg" alt="细语呢喃"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/blog-building">博客建设</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friend-link">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/leetcode-algorithm-solution">leetcode题解</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/messageboard">留言板</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about-me">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <script>
      if (window.matchMedia("(max-width: 768px)").matches) {
        document.querySelector('.content').classList.remove('on');
        document.querySelector('.sidebar').classList.remove('on');
      }
    </script>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/donate/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/donate/wechat_pay.png">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->


<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3',
    hasInnerContainers: true,
    scrollSmooth: false,
	  scrollSmoothDuration: 420,
    scrollContainer: 'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto',
	collapseDepth: 2,
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>

<!-- MathJax -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true
  }
</script>

<!-- Katex -->

<!-- busuanzi  -->


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->


<link rel="stylesheet" href="/css/clipboard.css">

<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>


<!-- CanvasBackground -->


    
  </div>
</body>

</html>