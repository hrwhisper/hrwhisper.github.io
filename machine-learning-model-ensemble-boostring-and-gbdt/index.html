<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/site/avatar.jpg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/site/avatar.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/site/avatar.jpg">
  <link rel="mask-icon" href="/images/site/avatar.jpg" color="#222">
  <meta name="google-site-verification" content="fMKqXfnCsLFKKj0NjoZZApB_BuqLVUiJxtRkj-rznU4">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
 <script data-ad-client="ca-pub-1580254183546533" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.hrwhisper.me","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="在上一章节中，我们介绍了模型融合以及Bagging方法，本文主要介绍Boosting相关方法。包括：  Boosting Adaboost GBDT">
<meta property="og:type" content="article">
<meta property="og:title" content="『我爱机器学习』集成学习（二）Boosting与GBDT">
<meta property="og:url" content="https://www.hrwhisper.me/machine-learning-model-ensemble-boostring-and-gbdt/index.html">
<meta property="og:site_name" content="细语呢喃">
<meta property="og:description" content="在上一章节中，我们介绍了模型融合以及Bagging方法，本文主要介绍Boosting相关方法。包括：  Boosting Adaboost GBDT">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-model-ensemble-boostring-and-gbdt/adboost2.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-model-ensemble-boostring-and-gbdt/Boosting-tree.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-model-ensemble-boostring-and-gbdt/GBDT-classification.png">
<meta property="article:published_time" content="2018-05-10T11:17:08.000Z">
<meta property="article:modified_time" content="2020-10-21T15:22:50.227Z">
<meta property="article:author" content="hrwhisper">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="Machine Learning model">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.hrwhisper.me/images/machine-learning-model-ensemble-boostring-and-gbdt/adboost2.png">

<link rel="canonical" href="https://www.hrwhisper.me/machine-learning-model-ensemble-boostring-and-gbdt/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>『我爱机器学习』集成学习（二）Boosting与GBDT | 细语呢喃</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-69270533-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-69270533-1');
      }
    </script>

  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d6a8cb42bd9ae728375b6726daa75e95";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">细语呢喃</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">技术改变生活</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">25</span></a>

  </li>
        <li class="menu-item menu-item-leetcode">

    <a href="/leetcode-algorithm-solution/" rel="section"><i class="fa fa-archive fa-fw"></i>leetcode</a>

  </li>
        <li class="menu-item menu-item-friends">

    <a href="/friend-link/" rel="section"><i class="fa fa-link fa-fw"></i>friends</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about-me/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.hrwhisper.me/machine-learning-model-ensemble-boostring-and-gbdt/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/site/avatar.jpg">
      <meta itemprop="name" content="hrwhisper">
      <meta itemprop="description" content="一个分享机器学习、算法与数据结构，个人学习心得、读书笔记、生活的博客。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="细语呢喃">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          『我爱机器学习』集成学习（二）Boosting与GBDT
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-05-10 19:17:08" itemprop="dateCreated datePublished" datetime="2018-05-10T19:17:08+08:00">2018-05-10</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/study/" itemprop="url" rel="index"><span itemprop="name">study</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/study/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span id="/machine-learning-model-ensemble-boostring-and-gbdt/" class="post-meta-item leancloud_visitors" data-flag-title="『我爱机器学习』集成学习（二）Boosting与GBDT" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">评论数：</span>
    
    <a title="valine" href="/machine-learning-model-ensemble-boostring-and-gbdt/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/machine-learning-model-ensemble-boostring-and-gbdt/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>在上一章节中，我们介绍了模型融合以及Bagging方法，本文主要介绍Boosting相关方法。包括：</p>
<ul>
<li>Boosting</li>
<li>Adaboost</li>
<li>GBDT</li>
</ul>
<a id="more"></a>
<h2 id="boosting">Boosting</h2>
<p>Boosting也叫提升法，是一类将弱学习器提升为强学习器的算法。</p>
<p>这类算法的工作机制类似：先从初始训练集中训练出一个基学习器，再根据学习器的表现<strong>对训练样本分布进行调整</strong>，使得先前基学习器做错的样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直到基学习器数目达到事先指定的T，最终将T个基学习器进行加权结合。</p>
<h2 id="adaboost">Adaboost</h2>
<p>首先给出Adaboost的算法：</p>
<blockquote>
<p>输入: 训练集 <span class="math inline">\(Data =\{({\bf x_1},y_1),({\bf x_2},y_2),\cdots,({\bf x_N},y_N)\}, y_i \in\{+1, -1\}\)</span>;</p>
<p>​ 训练轮数T，</p>
<p>​ 训练算法G</p>
<p>过程：</p>
<p>​ 初始化样本权重 <span class="math inline">\(D_{1,i} = \frac{1}{N}, i=1,2,\cdots,N\)</span></p>
<p>​ for t = 1 to T do</p>
<p>​ <span class="math inline">\(h_t = G({\rm Data} , D_t)\)</span> # 用训练算法G基于权重<span class="math inline">\(D_t\)</span>训练出当前的分类器<span class="math inline">\(h_t\)</span></p>
<p>​ <span class="math inline">\(\epsilon_t = \sum_{i=1}^N D_{t,i}I(h_t(x_i) \ne y_i)\)</span> # 计算误差</p>
<p>​ if <span class="math inline">\(\epsilon_t &gt; 0.5\)</span> then break</p>
<p>​ <span class="math inline">\(\alpha_t = \frac{1}{2}\ln(\frac{1 - \epsilon_t }{\epsilon_t})\)</span> #计算分类器<span class="math inline">\(h_t\)</span>的权重</p>
<p>​ <span class="math inline">\(D_{t+1,i} = \frac{1}{Z_t}\left( D_{t,i} \cdot exp(-y_i\alpha_th_t({\bf x_i}))\right)\)</span> # 更新样本权重，其中，归一化因子：<span class="math inline">\(Z_t = {\sum_{i=1}^N D_{t,i}}\cdot exp\left(-y_i\alpha_th_t({\bf x_i}) \right)\)</span></p>
<p>输出：</p>
<p>​ <span class="math inline">\(H({\bf x}) = sign(\sum_{t=1}^T \alpha_t h_t({\bf x}))\)</span></p>
</blockquote>
<p>一张直观的图如下：</p>
<figure>
<img src="../images/machine-learning-model-ensemble-boostring-and-gbdt/adboost2.png" alt="adboost" /><figcaption>adboost</figcaption>
</figure>
<p>描述起来好像很容易的样子：</p>
<ul>
<li>初始化每个样本权值均等，都为1 / N</li>
<li>然后迭代T轮，在每一轮中学习中根据错误率<span class="math inline">\(\epsilon_t\)</span>改变训练数据的权值分布，增加分类错误的权重，减少分类正确的样本权重。这样，新一轮的学习中将会更加重视对之前分错的样本。</li>
<li>将T轮得到的T个分类器<span class="math inline">\(h_t({\bf x})\)</span>和对应的分类器权重<span class="math inline">\(\alpha_t\)</span>进行线性组合，就得到最终结果<span class="math inline">\(H({\bf x}) = sign(\sum_{t=1}^T \alpha_t h_t({\bf x}))\)</span></li>
</ul>
<p>注意到当错误率<span class="math inline">\(\epsilon \gt 0.5\)</span>的时候，我们不再计算。因为对于二分类问题，错误率大于一半意味着不如随机乱猜，留之无用。</p>
<p>Adaboost模型让人困惑的地方有两个：</p>
<ol type="1">
<li>分类器权重<span class="math inline">\(\alpha_t\)</span>为啥等于<span class="math inline">\(\frac{1}{2}ln(\frac{1 - \epsilon_t }{\epsilon_t})\)</span></li>
<li>样本权重公式和归一化因子是怎么来的？</li>
</ol>
<h3 id="分类器权重的由来">分类器权重的由来</h3>
<p>Adaboost的分类器权重公式为：<span class="math inline">\(\alpha_t = \frac{1}{2}ln(\frac{1 - \epsilon_t }{\epsilon_t})\)</span> ，这是怎么来的呢？</p>
<p>可以从最小化训练误差界来推导，也可以从最小化损失函数进行推导。这两种本质上是一样的，都是极小化某式得到的<span class="math inline">\(\alpha_t\)</span>。</p>
<p>这里采用最小化训练误差界来推导。</p>
<h4 id="训练误差界">训练误差界</h4>
<p>Adaboost的训练误差界为： <span class="math display">\[
\frac{1}{N}\sum_{i=1}^N I \left(H({\bf x_i}) \ne y_i \right) \le  \frac{1}{N}\sum_{i=1}^N exp(-y_if({\bf x_i})) = \prod_{t}Z_t \tag{2-1}
\]</span> 其中<span class="math inline">\(f({\bf x}) = \sum_t \alpha_th_t({\bf x})； H({\bf x}) = \rm sign(f({\bf x}))\)</span></p>
<p>好像很复杂，我们先来不管最后那个等式，只看前面的不等式，就是说指数损失是0-1损失函数的上界。这个很好证明，当<span class="math inline">\(H({\bf x_i}) \ne y_i\)</span>时，<span class="math inline">\(-y_if({\bf x_i})&lt; 0 \Rightarrow exp(-y_if({\bf x_i})) \ge 1\)</span>。</p>
<p>在证明右边的等式之前，我们先回顾一下权重的定义： <span class="math display">\[
\begin{align*}
&amp; D_{t+1,i} = \frac{D_{t,i}}{Z_t}\cdot \exp(-y_i\alpha_th_t({\bf x_i})) \\
\Rightarrow \ &amp; Z_tD_{t+1,i} = D_{t,i}\cdot \exp(-y_i\alpha_th_t({\bf x_i})) \tag{2-2}
\end{align*}
\]</span> 现推导如下： <span class="math display">\[
\begin{align*}
   \frac{1}{N}\sum_{i=1}^N \exp(-y_if({\bf x_i})) &amp;=  \frac{1}{N}\sum_{i=1}^N \exp\left (\sum_{t=1}^T-y_i\alpha_th_t({\bf x})\right)
\\&amp; =\sum_{i=1}^N D_{1,i} \prod_{t=1}^{T} \exp\left (-y_i\alpha_th_t({\bf x})\right)   \hspace{5ex} D_{1,i}= \frac{1}{N}
\\&amp; =Z_1\sum_{i=1}^N D_{2,i} \prod_{t=2}^{T}  \exp\left (-y_i\alpha_th_t({\bf x})\right)  \hspace{5ex} 式2-2
\\&amp; =Z_1Z_2\sum_{i=1}^N D_{3,i} \prod_{t=3}^{T}  \exp\left (-y_i\alpha_th_t({\bf x})\right)
\\&amp; =Z_1Z_2\cdots Z_{T-1}\sum_{i=1}^N D_{T,i} \prod_{t=T}^{T}  \exp\left (-y_i\alpha_th_t({\bf x})\right)
\\&amp; =Z_1Z_2\cdots Z_{T}\sum_{i=1}^N D_{T + 1,i}
\\&amp; =Z_1Z_2\cdots Z_{T}\sum_{i=1}^N D_{T + 1,i}   \hspace{7ex}  \sum_{i=1}^N D_{T + 1,i}=1
\\&amp; =\prod_{t=1}^TZ_t
\end{align*}
\]</span> 即我们最小化<span class="math inline">\(\prod_{t=1}^TZ_t\)</span>等价于最小化指数损失函数，我们可以在每一轮都最小化<span class="math inline">\(Z_t\)</span> <span class="math display">\[
\begin{align*}
Z_t &amp;= {\sum_{i=1}^N D_{t,i}}\cdot \exp\left(-y_i\alpha_th_t({\bf x_i}) \right)
\\&amp;= {\sum_{i: \ y_i \ne h_t(x_i)} D_{t,i}}\exp(\alpha_t) + {\sum_{i: \ y_i = h_t(x_i)} D_{t,i}}\exp(-\alpha_t)
\\&amp;=  \epsilon_te^{\alpha_t} + (1 - \epsilon_t)e^{-\alpha_t}
\end{align*}\tag{2-3}
\]</span> <strong>对<span class="math inline">\(\alpha_t\)</span>求导得得到权重公式</strong>： <span class="math display">\[
\frac{\partial Z_t}{\partial \alpha_t} = \epsilon_te^{\alpha_t} - (1 - \epsilon_t)e^{-\alpha_t} = 0 \hspace{4ex}\Rightarrow  \alpha_t= \frac{1}{2}\ln\frac{1-\epsilon_t}{\epsilon_t}\tag{2-4}
\]</span> 将2-4带入2-3得： <span class="math display">\[
Z_t = 2\sqrt{\epsilon_t(1-\epsilon_t)} = \sqrt{1 - (1-2\epsilon_t)^2} = \sqrt{1 - 4\gamma_t^2} \hspace{4ex}  设\gamma_t = \frac{1}{2} - \epsilon_t
\]</span> 因此，2-1的训练误差界可以写为： <span class="math display">\[
\frac{1}{N}\sum_{i=1}^N I \left(H({\bf x_i}) \ne y_i \right) \le \prod_{t}Z_t = \prod_{t} \sqrt{1 - 4\gamma_t^2}  \le exp(-2\sum_{t=1}^T\gamma_t^2)\tag{2-5}
\]</span> 李航老师的书中说：2-5的最后一个式子可先由<span class="math inline">\(e^x\)</span>和<span class="math inline">\(\sqrt{1-x}\)</span>在点x = 0的泰勒展开式推出不等式<span class="math inline">\(\sqrt{1 - 4\gamma_t^2} \le exp(-2\gamma_t^2)\)</span>进而得到。</p>
<p>这表明Adaboost的训练误差是<strong>以指数速率</strong>下降的。</p>
<p>PS： Adaboost能适应弱分类器各自的训练误差，这也是它的名字（适应的提升）的由来。Ada是Adaptive的简写。</p>
<h2 id="加法模型和前向分布算法">加法模型和前向分布算法</h2>
<p>本小节主要介绍加法模型。</p>
<p>Adaboost可以看出是<strong>加法模型</strong>、损失函数为指数损失函数、学习算法为<strong>前向分布算法</strong>时的二分类学习方法。此外，之后要介绍的GBDT也可以看出是加法模型。</p>
<p>加法模型即： <span class="math display">\[
F({\bf x}) =\sum_{t=1}^T \alpha_t h_t({\bf x}; {\bf w_t}) = \sum_{i=1}^Tf_t({\bf x}; {\bf w_t}) \tag{3-1}
\]</span> 其中,<span class="math inline">\(h_t({\bf x}; {\bf w_t})\)</span>为基学习器， <span class="math inline">\(\bf x\)</span>为输入样本，<span class="math inline">\(\bf w\)</span>为基学习器的参数，而<span class="math inline">\(\alpha_t\)</span>为每个基学习器<span class="math inline">\(h_t\)</span>的权重</p>
<p>可以通过最小化损失函数进行求解，即求经验风险极小化问题： <span class="math display">\[
\min_{\alpha,\bf w}  \sum_{i=1}^N L\left(y_i, \sum_{t=1}^T \alpha_t h_t({\bf x}; {\bf w_t})\right)\tag{3-2}
\]</span> 要求解这个问题，是NP难的，一般用贪心法进行求解，即<strong>前向分布算法</strong>：从前往后，每一步只学习一个基函数及其系数，逐步优化3-2式。前向分布算法可以描述如下：</p>
<blockquote>
<p>输入: 训练集 <span class="math inline">\(Data =\{({\bf x_1},y_1),({\bf x_2},y_2),\cdots,({\bf x_N},y_N)\}, y_i \in\{+1, -1\}\)</span>;</p>
<p>​ 损失函数L</p>
<p>过程：</p>
<p>​ for t = 1 to T do</p>
<p>​ <span class="math inline">\((\alpha_t, w_t) = \arg\min_{\alpha_t,w_t} \sum_{i=1}^N L\left(y_i, f_{t-1}({\bf x_i}) + \alpha_t h_t({\bf x}; {\bf w_t})\right)\)</span> # 得到参数和基学习器权重</p>
<p>​ 更新<span class="math inline">\(f_t({\bf x}) = f_{t-1}({\bf x_i}) + \alpha_t h_t({\bf x}; {\bf w_t})\)</span></p>
<p>得到加法模型</p>
<p>​ <span class="math inline">\(F({\bf x}) = f_{t}({\bf x}) = \sum_{t=1}^T \alpha_t h_t({\bf x}; {\bf w_t})\)</span></p>
</blockquote>
<p>这样，便将同时求解t = 1到T的所有参数<span class="math inline">\(\alpha_t, \bf w_t\)</span>的优化问题转化为逐步求解各个<span class="math inline">\(\alpha_t, \bf w_t\)</span>的问题。</p>
<h3 id="再谈adaboost">再谈Adaboost</h3>
<p>上面说到，Adaboost是加法模型，损失函数为指数损失函数，并采用前向分布算法。</p>
<p>下面进行证明：</p>
<blockquote>
<p>前向分布分布算法学习的是加法模型，当基函数为基本分类器时，该加法模型等价于Adaboost的最终分类器。<span class="math inline">\(H({\bf x}) = \sum_{t=1}^T \alpha_t h_t({\bf x})\)</span></p>
</blockquote>
<h4 id="指数损失函数">指数损失函数</h4>
<p>首先证明前向分布算法的损失函数是指数损失函数<span class="math inline">\(L(y, F({\bf x})) = \exp(-yF({\bf x}))\)</span>时，其学习的具体操作等价于Adaboost算法学习的具体操作。</p>
<p>假设经过t - 1轮迭代前向分步算法已经得到<span class="math inline">\(f_{t-1}({\bf x}) = f_{t-2}({\bf x}) + \alpha_{t-1}h_{t-1}({\bf x}) = \alpha_{1}h_1({\bf x}) + \cdots + \alpha_{t-1}h_{t-1}({\bf x})\)</span></p>
<p>则在第t轮迭代应该寻找最优的<span class="math inline">\(\alpha_t, h_t\)</span>使得指数损失最小即 <span class="math inline">\((\alpha_t, h_t) = \arg\min_{\alpha_t,h_t} \sum_{i=1}^N L\left(y_i, f_{t-1}({\bf x_i}) + \alpha_t h_t({\bf x})\right)\)</span></p>
<p>从而得到<span class="math inline">\(f_{t}({\bf x}) = f_{t-1}({\bf x}) + \alpha_{t}h_t({\bf x})\)</span>。而指数损失最小可以写为： <span class="math display">\[
\begin{align*}
(\alpha_t, h_t) &amp;=\mathop{\arg\min}_{\alpha_t,h_t}  \sum_{i=1}^N \exp\left(-y_i(f_{t-1}({\bf x_i}) + \alpha_th_t({\bf x}))\right) \\
&amp;=\mathop{\arg\min}_{\alpha_t,h_t}  \sum_{i=1}^N D_{t,i}\exp\left( -y_i\alpha_th_t({\bf x})\right)\tag{3-3}
\end{align*}
\]</span> 其中<span class="math inline">\(D_{ti} = Error_{(t-1,i)} = \exp(-y_if_{t-1}({\bf x_i}))\)</span>与当前的优化目标无关，只与之前的有关，因此为指数损失函数。</p>
<h4 id="基分类器求解和权重推导">基分类器求解和权重推导</h4>
<p>接着证明使得式子3-3达到最小的<span class="math inline">\(\alpha_t^*\)</span>和<span class="math inline">\(h_t^*({\bf x})\)</span>就是Adaboost算法得到的<span class="math inline">\(\alpha_t\)</span>和<span class="math inline">\(h_t({\bf x})\)</span>。</p>
<p>求解3-3可以分为两步，首先求<span class="math inline">\(h_t({\bf x})\)</span>，对于任意的<span class="math inline">\(\alpha_t \gt 0\)</span>，使3-3最小的<span class="math inline">\(h_t({\bf x})\)</span>由下式得到： <span class="math display">\[
h_t^*({\bf x}) = \mathop{\arg\min}_h \sum_{i=1}^ND_{t,i}I(y_i\ne h({\bf x}))
\]</span> 其中<span class="math inline">\(D_{t,i}= \exp(-y_if_{t-1}(\bf x_i))\)</span>。</p>
<p>这样就得到了基分类器<span class="math inline">\(h_t^*({\bf x})\)</span>，而该分类器就是Adaboost算法的基本分类器<span class="math inline">\(h_t({\bf x})\)</span>，因为它是第t轮加权训练数据分类误差率最小的基本分类器。</p>
<p>接下来求<span class="math inline">\(\alpha_t\)</span>, 式在3-3中： <span class="math display">\[
\begin{align*}
 \sum_{i=1}^N D_{t,i}\exp\left( -y_i\alpha_th_t({\bf x})\right) 
 &amp; = \sum_{y_i=h_t(\bf x)}D_{t,i} e^{-\alpha} +  \sum_{y_i\ne h_t(\bf x)}D_{t,i}e^{\alpha}  \\
 &amp;=e^{-\alpha}\sum_{i=1}^ND_{t,i} + (e^{\alpha} - e^{-\alpha})\sum_{i=1}^N D_{t,i}I(y_i\ne h_t({\bf x_i})) \tag{3-4}
\end{align*}
\]</span> 最后一个变换可以认为是第一项多加了不相等时候的<span class="math inline">\(e^{-\alpha}\)</span>, 因此在第二项的时候减去。1-8对<span class="math inline">\(\alpha\)</span>求导，并使导数为0，得到使得3-3最小的<span class="math inline">\(\alpha_t\)</span> <span class="math display">\[
\alpha_t = \frac{1}{2} \ln(\frac{1 - \epsilon_t }{\epsilon_t})
\]</span> 其中<span class="math inline">\(\epsilon_t\)</span>为分类误差率 <span class="math display">\[
\epsilon_t = \frac{\sum_{i=1}^N D_{t,i}I(y_i\ne h_t({\bf x}))}{\sum_{i=1}^N D_{t,i}} = \sum_{i=1}^N \tilde D_{t,i}I(h_t(x_i) \ne y_i)
\]</span> 可以看出和之前更新<span class="math inline">\(\alpha_t\)</span>一致。</p>
<p>最后来看每一轮样本的权值更新，由<span class="math inline">\(f_{t}({\bf x}) = f_{t-1}({\bf x}) + \alpha_{t}h_t({\bf x})\)</span>和<span class="math inline">\(D_{t,i}= \exp(-y_if_{t-1}(\bf x_i))\)</span>可得： <span class="math display">\[
\begin{align*}
D_{t + 1,i} 
&amp;= \exp(-y_if_{t}(\bf x_i))\\
&amp;= \exp(-y_i( f_{t-1}({\bf x_i}) + \alpha_{t}h_t({\bf x_i}))\\
&amp;= D_{(t,i)}\exp(- \alpha_{t}y_{i}h_t({\bf x_i}))\\
\end{align*}
\]</span> 这与之前的Adaboost的样本权值更新公式相比，只差规范化因子，因而等价。</p>
<h2 id="gbdt">GBDT</h2>
<p>GBDT全称为：Gradient Boosting Decision Tree，即梯度提升决策树。可以理解为<strong>梯度 + 提升 + 决策树</strong>。</p>
<p>在介绍GBDT之前，先介绍比较简单的提升树（Boosting Decision Tree）</p>
<h3 id="提升树">提升树</h3>
<p>提升树实际上就是加法模型和前向分布算法，然后以CART决策树为基学习器。</p>
<p>可以表示为决策树的前向分布算法 <span class="math display">\[
\begin{align*}
F_0({\bf x}) &amp;= 0\\
F_t({\bf x}) &amp;= F_{t-1}({\bf x})  + h_t({\bf x}), t=1,2,\cdots ,T\\
F_T({\bf x}) &amp;= \sum_{t=1}^Th_t({\bf x})
\end{align*}
\]</span></p>
<p>在前向分布算法的第t步，给定当前模型<span class="math inline">\(F_{t-1}({\bf x})\)</span>，需要求解 <span class="math display">\[
h_t^*({\bf x}) = \mathop{\arg\min}_{h_t} \sum_{i=1}^NL(y_i, F_{t-1}({\bf x}) + h_t({\bf x}))
\]</span> 得到第t棵树<span class="math inline">\(F_{t}({\bf x})\)</span>。针对不同问题的提升树学习方法，<strong>主要区别在于使用的损失函数不同</strong>。比如平方误差损失函数的回归问题，用指数损失函数的分类问题，以及用一般损失函数的问题。</p>
<p>采用平方损失函数时，有： <span class="math display">\[
\begin{align*}
L\left(y,  F_{t-1}({\bf x}\right)  + h_t({\bf x})) 
&amp;=\left(y -  F_{t-1}({\bf x} ) - h_t({\bf x})\right)^2\\
&amp;=(r - h_t({\bf x}))^2
\end{align*}
\]</span> 其中<span class="math inline">\(r = y - F_{t-1}({\bf x})\)</span> 称为<strong>残差</strong>(Residual)。因此，对<strong>回归树的提升树算法来说，只需要简单的拟合当前模型的残差</strong>即可。注意这里的是回归树。</p>
<p>因此回归问题的提升树算法可以描述如下：</p>
<blockquote>
<p>输入: 训练集 <span class="math inline">\(Data =\{({\bf x_1},y_1),({\bf x_2},y_2),\cdots,({\bf x_N},y_N)\}, y_i \in\{+1, -1\}\)</span>;</p>
<p>过程：</p>
<p>​ 初始化<span class="math inline">\(F_0({\bf x}) = 0\)</span></p>
<p>​ for t = 1 to T do</p>
<p>​ 计算残差 <span class="math inline">\(r_{ti} = y_i - F_{t-1}({\bf x}) , \ i=1,2,\cdots,N\)</span></p>
<p>​ 拟合残差得到一个回归树：<span class="math inline">\(h_t({\bf x})\)</span></p>
<p>​ 更新<span class="math inline">\(F_t({\bf x}) = F_{t-1}({\bf x_i}) + h_t({\bf x})\)</span></p>
<p>得到加法模型<span class="math inline">\(F({\bf x}) = \sum_{t=1}^T h_t({\bf x})\)</span></p>
</blockquote>
<p>该算法可以用下面的图表示：</p>
<figure>
<img src="../images/machine-learning-model-ensemble-boostring-and-gbdt/Boosting-tree.png" alt="Boosting tree" /><figcaption>Boosting tree</figcaption>
</figure>
<p><strong>对比Adaboost来说，该算法可以说是修改样本的&quot;label&quot;，而AdaBoost则是修改样本的权重。</strong></p>
<h4 id="例子">例子</h4>
<p>继续搬出之前的决策树中的例子</p>
<table>
<thead>
<tr class="header">
<th>x</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>y</td>
<td>5.56</td>
<td>5.70</td>
<td>5.91</td>
<td>6.40</td>
<td>6.80</td>
<td>7.05</td>
<td>8.90</td>
<td>8.70</td>
<td>9.00</td>
<td>9.05</td>
</tr>
</tbody>
</table>
<p>第一轮计算后，我们算出（计算过程看决策树那章） <span class="math display">\[
h_1(x) =
\begin{cases}
6.24, &amp; x\le 6.5 \\
8.91, &amp; x \gt 6.5 \\
\end{cases}
\]</span> 可以算出残差（比如x=1就是5.56 - 6.24 = -0.68）</p>
<table>
<thead>
<tr class="header">
<th>x</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>y</td>
<td>-0.68</td>
<td>-0.54</td>
<td>-0.33</td>
<td>0.16</td>
<td>0.56</td>
<td>0.81</td>
<td>-0.01</td>
<td>-0.21</td>
<td>0.09</td>
<td>0.14</td>
</tr>
</tbody>
</table>
<p>接着，继续拟合数据，只是拟合的是上面的残差。得到： <span class="math display">\[
h_2(x) =
\begin{cases}
-0.52, &amp; x\le 3.5 \\
0.22, &amp; x \gt 3.5 \\
\end{cases}
\]</span> 则<span class="math inline">\(f_2({\bf x})\)</span>为： <span class="math display">\[
F_2({\bf x}) = f_1({\bf x}) + f_2({\bf x}) = 
\begin{cases}
5.72, &amp; x\le 3.5 \\
6.46, &amp; 3.5\lt x \le 6.5 \\
9.13, &amp; 6.5\lt x
\end{cases}
\]</span> 以此类推进行计算即可，直到损失<span class="math inline">\(L(y, F_t(\bf x))\)</span>满足要求，比如达到迭代次数T。</p>
<p>和决策树CART不同的地方在于，BDT每次拟合的是残差，而CART是经典的分治算法（divide and conquer），不断的划分子空间并在子空间中进一步精确的拟合。</p>
<h3 id="梯度提升">梯度提升</h3>
<p>前面提到过，GBDT全称为<strong>Gradient Boosting</strong> Decision Tree，<strong>梯度提升</strong>决策树。</p>
<p>现在我们来介绍通用的“梯度提升”算法。</p>
<p>当采用平方损失和指数损失的时候，每一步优化是很简单的，但是<strong>一般的损失函数来说，每一步的优化不容易</strong>，因此提出了梯度提升的方法，这是利用梯度下降的近似方法，其关键是利用<strong>损失函数的负梯度在当前模型的值</strong>： <span class="math display">\[
-\left[\frac{\partial L(y_i, F({\bf x_i}))}{\partial F({\bf x_i})}  \right]_{F({\bf x}) = F_{t-1}({\bf x})}\tag{4-1}
\]</span> 怎么理解这个近似呢？</p>
<p>以前面的均方损失函数为例，也是可以用这个方法来解释的。为了求导方便，我们在均方损失函数前乘以 1/2。 <span class="math display">\[
\begin{align*}
L(y_i, F({\bf x_i})) = \frac{1}{2}(y_i - F({\bf x_i}))^2
\end{align*}
\]</span> 注意到<span class="math inline">\(F({\bf x_i})\)</span>其实只是一些数字而已，我们可以将其像变量一样进行求导： <span class="math display">\[
\begin{align*}
\frac{\partial L(y_i, F({\bf x_i}))}{\partial F({\bf x_i})} = F({\bf x_i}) - y_i
\end{align*}
\]</span> 而前面所说的残差就是上式相反数，即<strong>负梯度</strong>： <span class="math display">\[
r_{ti} = y_i - F_{t-1}({\bf x}) =-\left[\frac{\partial L(y_i, F({\bf x_i}))}{\partial F({\bf x_i})}  \right]_{F({\bf x}) = F_{t-1}({\bf x})}
\]</span> 在梯度提升中，就是将式2-1作为残差来进行拟合。由此，我们给出一般的梯度提升算法：</p>
<blockquote>
<p>输入: 训练集 <span class="math inline">\(Data =\{({\bf x_1},y_1),({\bf x_2},y_2),\cdots,({\bf x_N},y_N)\}, y_i \in\{+1, -1\}\)</span>;</p>
<p>过程：</p>
<p>​ 初始化<span class="math inline">\(F_0({\bf x}) =\mathop{\arg\min}_{h_0} \sum_{i=1}^NL(y_i, h_0({\bf x}))\)</span></p>
<p>​ for t = 1 to T do</p>
<p>​ 计算负梯度 <span class="math inline">\({\tilde y}_{i} = -\left[\frac{\partial L(y_i, F({\bf x_i}))}{\partial F({\bf x_i})} \right]_{F({\bf x}) = F_{t-1}({\bf x})} , \ i=1,2,\cdots,N\)</span></p>
<p>​ <span class="math inline">\(w_t = \arg\min_{w_t} \sum_{i=1}^N \left({\tilde y}_{i} - h_t({\bf x}; {\bf w_t})\right)^2\)</span> # 拟合“残差“得到基学习器权重，也就得到了基学习器</p>
<p>​ <span class="math inline">\(\alpha_t= \arg\min_{\alpha_t} \sum_{i=1}^N L\left(y_i, f_{t-1}({\bf x_i}) + \alpha_t h_t({\bf x}; {\bf w_t})\right)\)</span> # 得到基学习器权重<span class="math inline">\(\alpha_t\)</span></p>
<p>​ 更新<span class="math inline">\(F_t({\bf x}) = F_{t-1}({\bf x_i}) + \alpha_th_t({\bf x}; {\bf w_t})\)</span></p>
</blockquote>
<p>对比提升树来说，提升树没有基学习器参权重<span class="math inline">\(\alpha_t\)</span></p>
<h3 id="gbdt-1">GBDT</h3>
<p>至此，我们可以给出GBDT的算法了。就是采用梯度提升的决策树（CART）而已嘛。PS: 上面给出的是梯度提升。</p>
<p>前面提到过，CART回归将空间划分为K个不相交的区域。可以用数学公式描述为： <span class="math display">\[
f(\mathbf{X}) = \sum_{k=1}^K c_k I(\mathbf{X} \in R_k)
\]</span> GBDT算法(回归)描述如下：</p>
<blockquote>
<p>输入: 训练集 <span class="math inline">\(Data =\{({\bf x_1},y_1),({\bf x_2},y_2),\cdots,({\bf x_N},y_N)\}, y_i \in\{+1, -1\}\)</span>;</p>
<p>过程：</p>
<p>​ 初始化<span class="math inline">\(F_0({\bf x}) =\mathop{\arg\min}_{h_0} \sum_{i=1}^NL(y_i, h_0({\bf x})) =\mathop{\arg\min}_{c} \sum_{i=1}^NL(y_i, c))\)</span></p>
<p>​ for t = 1 to T do</p>
<p>​ 计算残差 <span class="math inline">\({\tilde y}_{i} = -\left[\frac{\partial L(y_i, F({\bf x_i}))}{\partial F({\bf x_i})} \right]_{F({\bf x}) = F_{t-1}({\bf x})} , \ i=1,2,\cdots,N\)</span></p>
<p>​ 拟合残差<span class="math inline">\({\tilde y}_{i}\)</span>得到一个回归树，得到第t棵树的叶结点区域<span class="math inline">\(R_{tk}\)</span>：<span class="math inline">\(h_t({\bf x})= \sum_{k=1}^K c_k I(\mathbf{X} \in R_{tk})\)</span></p>
<p>​ 更新<span class="math inline">\(F_t({\bf x}) = F_{t-1}({\bf x_i}) + h_t({\bf x}) = F_{t-1}({\bf x_i}) + \sum_{k=1}^K c_k I(\mathbf{X} \in R_{tk})\)</span></p>
<p>得到加法模型</p>
<p>​ <span class="math inline">\(F({\bf x}) = \sum_{t=1}^T h_t({\bf x})\)</span></p>
</blockquote>
<p>可以说，如果我们的任务是回归的话，并且使用RMSE作为损失函数，就和上面的boosting tree一样。因为负梯度算出来就是残差:<span class="math inline">\(\left(y - F_{t-1}({\bf x} ) - h_t({\bf x})\right)^2=(r - h_t({\bf x}))^2\)</span></p>
<h3 id="gbdt-分类">GBDT 分类</h3>
<p>如果要将GBDT用于分类问题，怎么做呢？ 首先要明确的是，GBDT用于分类时使用的仍然是<strong>CART回归树</strong>。回想我们做回归问题的时候，每次对残差（负梯度）进行拟合。而分类问题要怎么每次对残差拟合？要知道类别相减是没有意义的。因此，可以用<strong>Softmax进行概率的映射</strong>，然后拟合<strong>概率的残差</strong>！</p>
<p>具体的做法如下：</p>
<ol type="1">
<li><strong>针对每个类别都先训练一个回归树</strong>，如三个类别，训练三棵树。就是比如对于样本<span class="math inline">\(\bf x_i\)</span>为第二类，则输入三棵树分别为：<span class="math inline">\(({\bf x_i}, 0), ({\bf x_i},1); ({\bf x_i}, 0)\)</span>这其实是典型的OneVsRest的多分类训练方式。 而每棵树的训练过程就是CART的训练过程。这样，对于样本<span class="math inline">\(\bf x_i\)</span>就得出了三棵树的预测值<span class="math inline">\(F_1({\bf x_i}),F_2({\bf x_i}),F_3({\bf x_i})\)</span>，模仿多分类的逻辑回归，用Softmax来产生概率，以类别1为例：<span class="math inline">\(p_{1}({\bf x_i})=\exp(F_{1}{({\bf x_i})})/\sum_{l= 1}^{3}\exp(F_{l}{({\bf x_i})})\)</span></li>
<li>对每个类别分别计算残差，如类别1：<span class="math inline">\({\tilde y}_{i1}= 0 - p_1({\bf x_i})\)</span>, 类别2： <span class="math inline">\({\tilde y}_{i2}= 1 - p_2({\bf x_i})\)</span>， 类别3：<span class="math inline">\({\tilde y}_{i3}= 0 - p_3({\bf x_i})\)</span></li>
<li>开始第二轮的训练，针对第一类 输入为<span class="math inline">\(({\bf x_i}, {\tilde y}_{i1})\)</span>, 针对第二类输入为<span class="math inline">\(({\bf x_i}, {\tilde y}_{i2})\)</span> ，针对第三类输入为 <span class="math inline">\(({\bf x_i}, {\tilde y}_{i3})\)</span>，继续训练出三颗树。</li>
<li>重复3直到迭代M轮，就得到了最后的模型。预测的时候只要找出概率最高的即为对应的类别。</li>
</ol>
<p>和上面的回归问题是大同小异的。</p>
<h4 id="被忽略的第三步">被忽略的第三步</h4>
<p>在原始的论文中，多分类的GBDT描述如下图（注意这里的m代表的是树的个数，N为样本数, K为类别数）</p>
<figure>
<img src="../images/machine-learning-model-ensemble-boostring-and-gbdt/GBDT-classification.png" alt="GBDT-classification" /><figcaption>GBDT-classification</figcaption>
</figure>
<p>可以看出，和上面的一样，首先计算各个类的概率。然后对于每个类，计算出残差，然后用CART回归树拟合。这里和上面描述的是一样的。但是之后的倒数第五行那是什么鬼？在原论文中，描述为：拟合完后，每棵树有J个叶结点，对应区域<span class="math inline">\(\{R_{jkm}\}_{j=1}^J\)</span>，模型通过下式的解更新这些<span class="math inline">\({\gamma}_{jkm}\)</span> <span class="math display">\[
{\gamma}_{jkm} =  \mathop{\arg\min}_{\gamma_{jk}} \sum_{i=1}^N\sum_{k=1}^{K}  \phi \left(y_{ik}, F_{k,m-1}({\bf x}) + \sum_{j=1}^J\gamma_{jk}I({\bf x_i} \in R_{jm})\right) \\
\phi(y_{k}, F_k)= -y_k\log p_k = p_{1}({\bf x_i})= -y_k\frac{exp(F_{k}{({\bf x_i})})}{\sum_{l= 1}^{K}exp(F_{l}{({\bf x_i})})}
\]</span> 但是这个式子没有闭式解。因此用牛顿法(Newton-Raphson step)进行近似。就是上面倒数第五行的结果。不过比较丢脸的是，这个我没有推出来。Sklearn中的GBDT实现中(MultinomialDeviance)，又略有不同，因此如果读者你会推的话就告诉博主把~</p>
<p>虽然说没有推出来，但是大多数情况下，可以通过设置步长（Step-size，也有的叫收缩率Shrinkage）的方式来省略这一步。因此很多资料很往往不介绍这一步。关于步长，本文最后有介绍。</p>
<h4 id="sklearn-gbdt二分类实现---binomialdeviance">Sklearn GBDT二分类实现 - BinomialDeviance</h4>
<p>在GBDT二分类中，可以用Logistic函数进行概率映射，而可以不用多分类的SoftMax。</p>
<p>为了挽回一点面子，讲讲sklearn二分类的BinomialDeviance的实现。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BinomialDeviance</span>(<span class="params">ClassificationLossFunction</span>):</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, y, pred, sample_weight=None</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Compute the deviance (= 2 * negative log-likelihood). &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># logaddexp(0, v) == log(1.0 + exp(v))</span></span><br><span class="line">        pred = pred.ravel()</span><br><span class="line">        <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-2.0</span> * np.mean((y * pred) - np.logaddexp(<span class="number">0.0</span>, pred))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (<span class="number">-2.0</span> / sample_weight.sum() *</span><br><span class="line">                    np.sum(sample_weight * ((y * pred) - np.logaddexp(<span class="number">0.0</span>, pred))))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">negative_gradient</span>(<span class="params">self, y, pred, **kargs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Compute the residual (= negative gradient). &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> y - expit(pred.ravel())</span><br></pre></td></tr></table></figure>
<p>首先是损失函数，忽略sample_weight权重设置，看看代码第八行。翻译过来就是 <span class="math display">\[
-2.0 \frac{1}{N}(yP - \log(1 + exp(P)))
\]</span> 怎么感觉和对率损失不太一样？ <span class="math display">\[
\begin{align*}
L = -(y \log(h) + (1-y) \log(1 - h) &amp;= -\left( \log(1 - h) + y \log \left( \frac{h}{1-h}\right)  \right) \\ 
&amp;=-\log(h(-P))- y P\hspace{4ex} 其中h(P) = \frac{1}{1+e^{-P}} \\
&amp;=\log(1+ \exp(P)) - yP
\end{align*}
\]</span> 这正是Sklearn使用的损失函数。</p>
<p>而负的梯度呢？ <span class="math display">\[
-\frac{\partial L}{\partial P}  = y - \frac{\exp(P)}{1+ \exp(P)} = y- h(P)
\]</span> 就是上面的y - expit(pred.ravel())</p>
<p>再来看看牛顿法更新：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_update_terminal_region</span>(<span class="params">self, tree, terminal_regions, leaf, X, y,</span></span></span><br><span class="line"><span class="function"><span class="params">                            residual, pred, sample_weight</span>):</span></span><br><span class="line">    terminal_region = np.where(terminal_regions == leaf)[<span class="number">0</span>]</span><br><span class="line">    residual = residual.take(terminal_region, axis=<span class="number">0</span>)</span><br><span class="line">    y = y.take(terminal_region, axis=<span class="number">0</span>)</span><br><span class="line">    sample_weight = sample_weight.take(terminal_region, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    numerator = np.sum(sample_weight * residual)</span><br><span class="line">    denominator = np.sum(sample_weight * (y - residual) * (<span class="number">1</span> - y + residual))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prevents overflow and division by zero</span></span><br><span class="line">    <span class="keyword">if</span> abs(denominator) &lt; <span class="number">1e-150</span>:</span><br><span class="line">        tree.value[leaf, <span class="number">0</span>, <span class="number">0</span>] = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tree.value[leaf, <span class="number">0</span>, <span class="number">0</span>] = numerator / denominator</span><br></pre></td></tr></table></figure>
<p>主要查看numerator和denominator的计算，这里<span class="math inline">\(residual = y - h(P)\)</span></p>
<p>分子就是上面的负梯度，即<span class="math inline">\(residual = y - h(P)\)</span></p>
<p>分母为二阶导，即<span class="math inline">\(h(P)(1-h(P))= (y-residual)*(1-y+residual))\)</span></p>
<h3 id="gbdt-正则化">GBDT 正则化</h3>
<p>为了避免过拟合，可以从两方面入手：</p>
<ul>
<li>弱算法的个数T， 上面描述的算法中，记得么？就是迭代T轮。T的大小就影响着算法的复杂度</li>
<li>步长（Shrinkage）在每一轮迭代中，原来采用<span class="math inline">\(F_t({\bf x}) = F_{t-1}({\bf x}) + \alpha_th_t({\bf x}; {\bf w_t})\)</span>进行更新，可以加入步长v，使得一次不更新那么多：<span class="math inline">\(F_t({\bf x}) = F_{t-1}({\bf x}) + v \  \alpha_th_t({\bf x}; {\bf w_t}); v\in(0,1]\)</span></li>
</ul>
<h2 id="参考资料">参考资料</h2>
<ul>
<li>机器学习技法 - 林轩田</li>
<li>《机器学习》 - 周志华</li>
<li>《统计学习方法》 - 李航</li>
</ul>
<p>关于Adaboost还可以查阅：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.cs.cmu.edu/~aarti/Class/10701/slides/Lecture10.pdf">Boosting Can we make dumb learners smart</a></li>
<li><a target="_blank" rel="noopener" href="http://www.csuldw.com/2016/08/28/2016-08-28-adaboost-algorithm-theory/">Adaboost - 新的角度理解权值更新策略</a></li>
</ul>
<p>关于GBDT可以查阅</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf">Greedy Function Approximation: A Gradient Boosting Machine</a></li>
<li><a target="_blank" rel="noopener" href="http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf">A Gentle Introduction to Gradient Boosting</a></li>
<li>https://www.cnblogs.com/ModifyRong/p/7744987.html</li>
</ul>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>hrwhisper
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://www.hrwhisper.me/machine-learning-model-ensemble-boostring-and-gbdt/" title="『我爱机器学习』集成学习（二）Boosting与GBDT">https://www.hrwhisper.me/machine-learning-model-ensemble-boostring-and-gbdt/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        <div class="reward-container">
  <div>请我喝杯咖啡吧~</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/donate/wechat_pay.png" alt="hrwhisper 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/donate/alipay.jpg" alt="hrwhisper 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
              <a href="/tags/Machine-Learning-model/" rel="tag"># Machine Learning model</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/machine-learning-model-ensemble-and-bagging/" rel="prev" title="『我爱机器学习』集成学习（一）模型融合与Bagging">
      <i class="fa fa-chevron-left"></i> 『我爱机器学习』集成学习（一）模型融合与Bagging
    </a></div>
      <div class="post-nav-item">
    <a href="/machine-learning-xgboost/" rel="next" title="『我爱机器学习』集成学习（三）XGBoost">
      『我爱机器学习』集成学习（三）XGBoost <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#boosting"><span class="nav-text">Boosting</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#adaboost"><span class="nav-text">Adaboost</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E5%99%A8%E6%9D%83%E9%87%8D%E7%9A%84%E7%94%B1%E6%9D%A5"><span class="nav-text">分类器权重的由来</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E7%95%8C"><span class="nav-text">训练误差界</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A0%E6%B3%95%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%89%8D%E5%90%91%E5%88%86%E5%B8%83%E7%AE%97%E6%B3%95"><span class="nav-text">加法模型和前向分布算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%8D%E8%B0%88adaboost"><span class="nav-text">再谈Adaboost</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E6%95%B0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">指数损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E5%88%86%E7%B1%BB%E5%99%A8%E6%B1%82%E8%A7%A3%E5%92%8C%E6%9D%83%E9%87%8D%E6%8E%A8%E5%AF%BC"><span class="nav-text">基分类器求解和权重推导</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gbdt"><span class="nav-text">GBDT</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8F%90%E5%8D%87%E6%A0%91"><span class="nav-text">提升树</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BE%8B%E5%AD%90"><span class="nav-text">例子</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87"><span class="nav-text">梯度提升</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gbdt-1"><span class="nav-text">GBDT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gbdt-%E5%88%86%E7%B1%BB"><span class="nav-text">GBDT 分类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A2%AB%E5%BF%BD%E7%95%A5%E7%9A%84%E7%AC%AC%E4%B8%89%E6%AD%A5"><span class="nav-text">被忽略的第三步</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sklearn-gbdt%E4%BA%8C%E5%88%86%E7%B1%BB%E5%AE%9E%E7%8E%B0---binomialdeviance"><span class="nav-text">Sklearn GBDT二分类实现 - BinomialDeviance</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gbdt-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-text">GBDT 正则化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-text">参考资料</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="hrwhisper"
      src="/images/site/avatar.jpg">
  <p class="site-author-name" itemprop="name">hrwhisper</p>
  <div class="site-description" itemprop="description">一个分享机器学习、算法与数据结构，个人学习心得、读书笔记、生活的博客。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">250</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">52</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hrwhisper" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hrwhisper" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/murmured" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;murmured" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2013 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hrwhisper</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"fVcjWMD8aI6F0qEfKdUaHa4f-gzGzoHsz","app_key":"b26lBsbwmVyxTSnNrsBrnv3U","server_url":null,"security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      // script.setAttribute("data-pjax", "");
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'fVcjWMD8aI6F0qEfKdUaHa4f-gzGzoHsz',
      appKey     : 'b26lBsbwmVyxTSnNrsBrnv3U',
      placeholder: "填上邮箱可以收到我回复的邮件哦~",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
