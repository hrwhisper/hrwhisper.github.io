<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/site/avatar.jpg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/site/avatar.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/site/avatar.jpg">
  <link rel="mask-icon" href="/images/site/avatar.jpg" color="#222">
  <meta name="google-site-verification" content="fMKqXfnCsLFKKj0NjoZZApB_BuqLVUiJxtRkj-rznU4">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.hrwhisper.me","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="本文尽可能通俗、详细的介绍支持向量机SVM内容。 包括  核函数 软边距SVM 其它  Hinge损失 概率SVM 核化逻辑回归">
<meta property="og:type" content="article">
<meta property="og:title" content="『我爱机器学习』深入理解SVM(二) - 核函数和软边距">
<meta property="og:url" content="https://www.hrwhisper.me/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/index.html">
<meta property="og:site_name" content="细语呢喃">
<meta property="og:description" content="本文尽可能通俗、详细的介绍支持向量机SVM内容。 包括  核函数 软边距SVM 其它  Hinge损失 概率SVM 核化逻辑回归">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/polynomial-kernel-function.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/gaussian-kernel-function.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/soft-margin-svm.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/soft-margin-svm-support-vector.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/soft-margin-svm-gaussian-kernel-parameter.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/hinge-exponential-logistic-loss-function.png">
<meta property="article:published_time" content="2018-03-24T03:54:18.000Z">
<meta property="article:modified_time" content="2020-10-25T14:12:23.319Z">
<meta property="article:author" content="hrwhisper">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="Machine Learning model">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.hrwhisper.me/images/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/polynomial-kernel-function.png">

<link rel="canonical" href="https://www.hrwhisper.me/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>『我爱机器学习』深入理解SVM(二) - 核函数和软边距 | 细语呢喃</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-69270533-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-69270533-1');
      }
    </script>
 <script data-ad-client="ca-pub-ca-pub-1580254183546533" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d6a8cb42bd9ae728375b6726daa75e95";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">细语呢喃</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">技术改变生活</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">25</span></a>

  </li>
        <li class="menu-item menu-item-leetcode">

    <a href="/leetcode-algorithm-solution/" rel="section"><i class="fa fa-archive fa-fw"></i>leetcode</a>

  </li>
        <li class="menu-item menu-item-friends">

    <a href="/friend-link/" rel="section"><i class="fa fa-link fa-fw"></i>friends</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about-me/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.hrwhisper.me/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/site/avatar.jpg">
      <meta itemprop="name" content="hrwhisper">
      <meta itemprop="description" content="一个分享机器学习、算法与数据结构，个人学习心得、读书笔记、生活的博客。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="细语呢喃">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          『我爱机器学习』深入理解SVM(二) - 核函数和软边距
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-03-24 11:54:18" itemprop="dateCreated datePublished" datetime="2018-03-24T11:54:18+08:00">2018-03-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/study/" itemprop="url" rel="index"><span itemprop="name">study</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/study/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span id="/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/" class="post-meta-item leancloud_visitors" data-flag-title="『我爱机器学习』深入理解SVM(二) - 核函数和软边距" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">评论数：</span>
    
    <a title="valine" href="/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本文尽可能通俗、详细的介绍支持向量机SVM内容。</p>
<p>包括</p>
<ul>
<li>核函数</li>
<li>软边距SVM</li>
<li><p>其它</p>
<ul>
<li>Hinge损失</li>
<li>概率SVM</li>
<li>核化逻辑回归</li>
</ul></li>
</ul>
<a id="more"></a>
<h2 id="核函数">核函数</h2>
<p>若我们将原始数据从<span class="math inline">\(R^d\)</span>空间通过<span class="math inline">\(\bf z = \Phi(x)\)</span>映射到高维空间<span class="math inline">\(R^\tilde d\)</span>以解决线性不可分的问题，则SVM原始问题为：</p>
<p><span class="math display">\[
\begin{align*}
\min_{\bf w,b}\hspace{3ex} &amp; \frac{1}{2} {\bf w^Tw}\\
{\rm st.}\hspace{3ex} &amp; y_i({\bf w^T z} + b)  \ge 1, \hspace{3ex}i=1,2,\cdots ,n
\end{align*}
\]</span> SVM的对偶问题为： <span class="math display">\[
\begin{align*}
 \max_{ {\boldsymbol \alpha} }&amp; \hspace{3ex}\sum_{i=1}^n\alpha_i-\frac{1}{2} \sum_{i=1}^n\sum_{j=1}^n \alpha_i\alpha_jy_iy_j{\bf z_i^Tz_j}\\
{\rm s.t.} 
&amp;  \hspace{3ex} \alpha_i \ge 0, i = 1,2,\cdots ,n\\
&amp;  \hspace{3ex} \sum_{i=1}^n \alpha_iy_i = 0
\end{align*}
\]</span> 上一篇博文最后提到，对偶问题仍然依赖于数据维度<span class="math inline">\(\tilde d\)</span>，因为有<span class="math inline">\(\bf z_i^Tz_j\)</span>内积运算。当新空间维度很大，甚至是无穷维的时候，这将成为一个计算的瓶颈。</p>
<h3 id="核技巧和核函数">核技巧和核函数</h3>
<p>注意<span class="math inline">\({\bf z_i^Tz_j = \Phi(x_i)^T \Phi(x_j)}\)</span>的计算顺序如下：</p>
<ol type="1">
<li>先进行空间变换</li>
<li>进行内积</li>
</ol>
<p>如果将两步合并，会不会快一点？</p>
<p>林轩田老师举了一个简单的例子（为了简单表示，将<span class="math inline">\(x_1x_2\)</span>和<span class="math inline">\(x_2x_1\)</span>都放了进来） <span class="math display">\[
\boldsymbol{\Phi}({\bf x}) = (1, x_1, x_2, \ldots, x_d, x_1^2, x_1x_2, \ldots ,x_1x_d, x_2x_1, x_2^2, \ldots ,x_2x_d, \ldots, x_d^2)
\]</span> 假设两个数据点<span class="math inline">\({\bf x, x&#39;}\)</span>进行内积，结果如下（常数项、一次项、二次项） <span class="math display">\[
\begin{align*}
\boldsymbol{\Phi}({\bf x})^\mathsf{T}\boldsymbol{\Phi}({\bf x}&#39;) &amp;= 1 +\sum_{i=1}^dx_ix_i&#39; + \sum_{i=1}^d\sum_{j=1}^dx_ix_jx_i&#39;x_j&#39; \\
&amp;= 1+\sum_{i=1}^dx_ix_i&#39; + \sum_{i=1}^d x_ix_i&#39;\sum_{j=1}^dx_jx_j&#39; \\
&amp;= 1 + {\bf x}^\mathsf{T}{\bf x}&#39; + ({\bf x}^\mathsf{T}{\bf x}&#39;)({\bf x}^\mathsf{T}{\bf x}&#39;)
\end{align*}
\]</span> 可以看出，先转换再进行内积的操作和我们直接在原空间进行<span class="math inline">\(1 + {\bf x}^\mathsf{T}{\bf x}&#39; + ({\bf x}^\mathsf{T}{\bf x}&#39;)({\bf x}^\mathsf{T}{\bf x}&#39;)\)</span>的内积结果是一样的。</p>
<p>因此，在某些情况下，如果把变换和内积这两个步骤合起来，计算效率可以得到提高。</p>
<p>转换+内积合并为一步称为<strong>核技巧</strong>，换句话说，核技巧是要找一个<strong>核函数K</strong>，使得 <span class="math display">\[
K_\boldsymbol{\Phi}({\bf x}, {\bf x}&#39;) \equiv \boldsymbol{\Phi}({\bf x})^\mathsf{T}\boldsymbol{\Phi}({\bf x}&#39;)
\]</span> 上面简单的例子中，核函数为 <span class="math display">\[
K_{\boldsymbol{\Phi} }({\bf x}, {\bf x}&#39;) = 1 + {\bf x}^\mathsf{T}{\bf x}&#39; + ({\bf x}^\mathsf{T}{\bf x}&#39;)^2
\]</span> 有了核函数后，可以<strong>直接用核函数K在原本的d维空间计算</strong>，而不用在高维空间中计算很复杂的内积。</p>
<p>注意到我们的SVM对偶问题中，无论是目标函数还是决策函数，都只有输入实例核实例的内积，因此可以用核函数写为： <span class="math display">\[
\begin{align*}
 \max_{ {\boldsymbol \alpha} }&amp; \hspace{3ex}\sum_{i=1}^n\alpha_i-\frac{1}{2} \sum_{i=1}^n\sum_{j=1}^n \alpha_i\alpha_j y_iy_j K({\bf x_i},{\bf x_j})\\
{\rm s.t.} 
&amp;  \hspace{3ex} \alpha_i \ge 0, i = 1,2,\cdots ,m\\
&amp;  \hspace{3ex} \sum_{i=1}^n \alpha_iy_i = 0
\end{align*}
\]</span> 而决策函数可以写为： <span class="math display">\[
\begin{align}f({\bf {\bf x} }) 
&amp;= \rm sign  \left( {\bf w}^T\Phi({\bf x}) + b \right)\\
&amp;=  \rm sign  \left( \sum_{i=1}^n \alpha_iy_i\Phi({\bf x}_i)^T \Phi({\bf x}) + b\right)\\
&amp;= \rm sign  \left( \sum_{i=1}^n \alpha_iy_iK({\bf x}_i,{\bf x}) + b\right) \\
&amp;= \rm sign  \left( \sum_{SV} \alpha_iy_iK({\bf x}_i,{\bf x}) + b\right)
\end{align}
\]</span></p>
<p>最后的SV表示支持向量。</p>
<p>于是就得到了<strong>核支持向量机</strong>。</p>
<h3 id="核函数的选择">核函数的选择</h3>
<p>SVM引入了核函数之后，摆脱了对映射后高纬度<span class="math inline">\(\tilde d\)</span>的内积计算。因此在实际问题中，我们通过指定核函数而非映射方式<span class="math inline">\(\Phi(x)\)</span>。</p>
<p>核函数隐式的定义了一个特征空间，而我们希望在这个特征空间中，我们的样本线性可分。因此核函数的选择是十分重要的，如果<strong>核函数选择不合适，意味着将样本映射到了一个不合适的特征空间，很可能性能不佳。</strong></p>
<p>下面介绍常见的多项式核和高斯核。</p>
<h4 id="多项式核">多项式核</h4>
<p>多项式核写作: <span class="math display">\[
K({\bf x}, {\bf x}&#39;) = (\zeta + \gamma {\bf x^\mathsf{T}x}&#39;)^d \tag{1-1}
\]</span> 当多项式核中的<span class="math inline">\(\zeta = 0, \gamma = 1, d = 1\)</span>时，退化为线性核。</p>
<p>下图为林轩田老师给出的多项式核中不同参数的效果，其中带方框的点是支持向量。</p>
<figure>
<img src="../images/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/polynomial-kernel-function.png" alt="Polynomial-kernel" /><figcaption>Polynomial-kernel</figcaption>
</figure>
<p>我们很难在训练前说哪一个比较好，往往需要进行交叉验证等来判断。</p>
<p>但是这个例子说明了我们把核函数换掉，SVM的边界也随之改变（说明我们需要进行核函数选择）。在实践中，应记住&quot;linear first&quot;， 从线性核这种简单模型开始尝试。</p>
<h4 id="高斯核">高斯核</h4>
<p>高斯核可以方便的计算<strong>无限维度</strong>的内积，如下： <span class="math display">\[
\begin{align*}
K({\bf x}, {\bf x}&#39;) &amp;= \exp(-({\bf x}-{\bf x}&#39;)^2) \\
&amp;= \exp(-{\bf x}^2)\exp(-{\bf x}&#39;^2)\exp(2{\bf xx&#39;}) \\
&amp;= \exp(-{\bf x}^2)\exp(-{\bf x}&#39;^2)\left(\sum_{i=0}^\infty\frac{(2{\bf xx&#39;})^i}{i!}\right) \hspace{3ex}{\rm Taylor\  展开} \ \exp(x) = \sum_{n=0}^{\infty} \frac{x^n}{n!}\\
&amp;= \sum_{i=0}^\infty\left(\exp(-{\bf x}^2)\exp(-{\bf x}&#39;^2)\sqrt{\frac{2^i}{i!} }\sqrt{\frac{2^i}{i!} } ({\bf x})^i({\bf x}&#39;)^i\right) \\
&amp;= \Phi({\bf x})^\mathsf{T}\Phi({\bf x}&#39;)
\end{align*}
\]</span> 即多项式变换将<strong>x</strong>映射到一个无限维的空间中： <span class="math display">\[
\Phi({\bf x}) = \exp(-{\bf x}^2)\cdot \left(1, \sqrt{\frac{2}{1!} }{\bf x}, \sqrt{\frac{2^2}{2!} }{\bf x}^2,\ldots\right)
\]</span> 更一般的，<strong>高斯核函数</strong>写为： <span class="math display">\[
K({\bf x},{\bf x}&#39;) = \rm exp \left(- \frac{|\!|{\bf x} - {\bf x}&#39;|\!|^2}{2\sigma^2}\ \right) \tag{1-2}
\]</span> 带入核支持向量机得到： <span class="math display">\[
\begin{align*}
g_{\rm SVM}({\bf x}) &amp;= {\rm sign}\left(\sum_{\rm SV}\alpha_iy_i K({\bf x}_i, {\bf x})+b\right)  \\
&amp;= {\rm sign}\left(\sum_{\rm SV}\alpha_iy_i \exp\left(\frac{|\!|{\bf x}-{\bf x}_i|\!|^2}{2\sigma^2}\right)+b\right)  
\end{align*}
\]</span> 上述的式子中，此时SVM实际上是以支持向量为中心的高斯函数的线性组合，因此高斯核函数通常也称为<strong>RBF核</strong>（Radial Basis Function kernel, Radial 就是长得像高斯函数的(只和<span class="math inline">\(x_i\)</span>到中心<span class="math inline">\(x\)</span>的距离有关)，Basis function就是拿来做线性组合的)。</p>
<p>至此，可以小小总结一下我们SVM一路的”进化“：</p>
<ul>
<li><p>使用<span class="math inline">\(\bf z = \Phi(x)\)</span> =&gt; 高效的核函数<strong>K(x, x')</strong></p></li>
<li><p>保存最优的<strong>w</strong>， =&gt; 保存较少的支持向量和相应的<span class="math inline">\(\alpha_i\)</span></p></li>
</ul>
<p>而使用高斯核函数可以映射到无限多维空间！而我们的鲁棒性保证则是最大边界的性质。</p>
<p>当然，SVM也是会过拟合的。</p>
<p>林轩田老师又给出了下面的图：</p>
<figure>
<img src="../images/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/gaussian-kernel-function.png" alt="gaussian-kernel-function" /><figcaption>gaussian-kernel-function</figcaption>
</figure>
<p>注：林轩田老师的高斯核函数为<span class="math inline">\(K({\bf x},{\bf x}&#39;) = \rm e{\bf x}p \left(- \gamma|\!|{\bf x} - {\bf x}&#39;|\!|^2\ \right)\)</span>，和本文的其实是一样的</p>
<p>可以看出，<span class="math inline">\(\gamma\)</span>越大，高斯函数越尖锐，模型越容易过拟合。可以想象，当<span class="math inline">\(\gamma \rightarrow \infty\)</span>时，高斯函数<span class="math inline">\(K({\bf x}, {\bf x}&#39;) = [\![{\bf x} = {\bf x}&#39;]\!]\)</span></p>
<h4 id="核函数比较">核函数比较</h4>
<ul>
<li><strong>线性核函数</strong><span class="math inline">\(K({\bf x, x}&#39;) = {\bf x^Tx}&#39;\)</span> 实际上没有进行特征空间变换。最安全也最快。最后得到的模型可解释性强：<strong>w</strong>会给出每个特征的权重，支持向量会给出每个数据点的重要性。缺点是：数据可能不是线性可分的。<strong>对文本数据通常使用线性核函数</strong>。</li>
<li><strong>多项式核</strong><span class="math inline">\(K({\bf x}, {\bf x}&#39;) = (\zeta + \gamma {\bf x^\mathsf{T}x}&#39;)^d\)</span>比线性核更通用，参数d直接描述了被映射空间的复杂度。但它参数较多，调参较难。<strong>多项式核可能适用于d比较小的场景</strong>，因为当d很大的时候计算不稳定。不稳定表现在：当<span class="math inline">\(|\zeta + \gamma {\bf x^\mathsf{T}x}&#39; | \lt 1\)</span>时，核函数逼近0，当<span class="math inline">\(|\zeta + \gamma {\bf x^\mathsf{T}x}&#39; | \gt 1\)</span>,核函数值会非常大。</li>
<li><strong>高斯核</strong><span class="math inline">\(K({\bf x},{\bf x}&#39;) = \rm e{\bf x}p \left(- \frac{|\!|{\bf x} - {\bf x}&#39;|\!|^2}{2\sigma^2}\ \right)\)</span>,可以计算出更复杂的边界，相比多项式核只有一个参数，容易调参。但是由于映射到了无限维空间，没有一个直观的<strong>w</strong>来解释模型。此外，它的计算也比较复杂，容易过拟合。</li>
</ul>
<h3 id="核函数的性质">核函数的性质</h3>
<p>核函数实际上是x和x'映射到新的特征空间的相似度衡量。（向量本身就表示相似性，如正交的时候我们就说一点都不像）。当然不是所有的相似性都能用一个合法的核函数表示。如果一个函数K是合法的核函数，K必须满足下面两个条件（<strong>Mercer条件</strong>）：</p>
<ul>
<li>K是对称的</li>
<li>K必须是半正定的</li>
</ul>
<p>此外，核函数还可以通过函数组合得到，</p>
<ul>
<li>K1，K2为核函数，对于任意正数<span class="math inline">\(\gamma_1, \gamma_2\)</span>，其线性组合也是核函数： <span class="math inline">\(\gamma_1K_1 + \gamma_2K_2\)</span></li>
<li>K1，K2为核函数，核函数的直积也是核函数：<span class="math inline">\(K1\otimes K_2({\bf x,z}) = K_1({\bf x,z})K_2({\bf x,z})\)</span></li>
<li>K1为核函数，则对于任意函数g(<strong>x</strong>)， <span class="math inline">\(g({\bf x})k_1({\bf x, z})g({\bf z})\)</span>也是核函数</li>
</ul>
<h2 id="软间隔svm">软间隔SVM</h2>
<p>前面提到的SVM都是硬间隔的，所谓的硬间隔，就是要求所有的样本都分对，此外还要求每个样本离分离超平面有一定的距离。为了解决线性不可分的情况，使用了核函数进行特征空间的变换，然而，即使我们进行特征空间变换后把样本完全的分开，也有可能这个“线性可分”是因为噪声点而过拟合造成的。（很多情况下，训练数据中有一些噪声点，把这些噪声点去除后，剩下的大部分样本组成的特征空间是线性可分的。）</p>
<p>因此，我们可以对SVM适当的放松条件，不再要求所有的点都分对，而是对不能满足约束的样本进行一些“惩罚”，为此引入惩罚因子<span class="math inline">\(\xi_i \ge 0\)</span>，并写出<strong>软间隔SVM</strong>的形式如下： <span class="math display">\[
\begin{align*}
\min_{ {\bf w}, b,\xi }\hspace{2ex}&amp;\frac{1}{2}{\bf w^Tw} + C\cdot \sum_{i=1}^n \xi_i\\
{\rm s.t.} \hspace{2ex} &amp;y_i({\bf w^\mathsf{T}x}_i + b) \ge 1 -\xi_i \\
\hspace{2ex} &amp;\xi_i \ge0, i=1,2,....,n
\end{align*}\tag{2-1}
\]</span> C为惩罚参数，<strong>C越大说明越不能容忍犯错，尽可能的分对所有的点</strong>，C越小说明对错误的容忍程度越大，间隔可以越宽。</p>
<p>下图就直观的描述了软间隔SVM，紫色的violation说明该点在间隔内，而紫色的长度就记录了对间隔的违反程度，即<span class="math inline">\(\xi_i\)</span>（啰嗦一句，原先我们要求都在间隔外）。</p>
<figure>
<img src="../images/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/soft-margin-svm.png" alt="soft-margin-svm" /><figcaption>soft-margin-svm</figcaption>
</figure>
<h3 id="软间隔svm的对偶问题">软间隔SVM的对偶问题</h3>
<p>和之前硬边界的SVM一样，我们求解对偶问题。</p>
<p>首先同样写出拉格朗日函数： <span class="math display">\[
\mathcal{L}({\bf w},b, {\boldsymbol \xi}, {\boldsymbol \alpha}, {\boldsymbol \beta}) =  \frac{1}{2}{\bf w^\mathsf{T}w} + C\cdot \sum_{i=1}^n \xi_i + \sum_{i=1}^n\alpha_i(1 - \xi_i - y_i({\bf w^Tx_i}+b)) -\sum_{i=1}^n\beta_i\xi_i
\]</span> <span class="math inline">\(\mathcal{L}({\bf w},b, {\boldsymbol \xi}, {\boldsymbol \alpha}, {\boldsymbol \beta})\)</span>对<span class="math inline">\({\bf w}, b, \xi_i\)</span>的极小得 <span class="math display">\[
\begin{align*}
\frac{\partial \mathcal{L} }{\partial \bf w}  &amp;= {\bf w}-\sum_{i=1}^n\alpha_iy_i{\bf x_i} =0\\
\frac{\partial \mathcal{L} }{\partial b } &amp;=  -\sum_{i=1}^n\alpha_iy_i  =0\\
\frac{\partial \mathcal{L} }{\partial \xi_i} &amp;=  C  - \alpha_i  - \beta_i =0\\
\end{align*}
\]</span> 整理得： <span class="math display">\[
\begin{align*}
{\bf w} =\sum_{i=1}^n\alpha_iy_i{\bf x_i}\\
 \sum_{i=1}^n\alpha_iy_i  =0\\
 C  - \alpha_i  - \beta_i =0\tag{2-2}
\end{align*}
\]</span> 带入拉格朗日函数得： <span class="math display">\[
\min_{ {\bf w},b, {\boldsymbol \xi} }\mathcal{L}({\bf w},b, {\boldsymbol \xi}, {\boldsymbol \alpha}, {\boldsymbol \beta}) = - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j{\bf x_i^T x_j} + \sum_{i=1}^n\alpha_i
\]</span> 在对<span class="math inline">\(\min_{ {\bf w},b, {\boldsymbol \xi} }\mathcal{L}({\bf w},b, {\boldsymbol \xi}, {\boldsymbol \alpha}, {\boldsymbol \beta})\)</span>求极大，得到下式： <span class="math display">\[
\begin{align*}
\max_{ {\boldsymbol \alpha} }\hspace{2ex}&amp; \sum_{i=1}^n\alpha_i -  \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j{\bf x_i^T x_j} \\
{\rm s.t.} 
\hspace{2ex} &amp; \sum_{i=1}^n\alpha_iy_i  =0\\
\hspace{2ex} &amp; \alpha_i \ge 0\\ 
\hspace{2ex} &amp; \beta_i \ge 0\\
\hspace{2ex} &amp; C - \alpha_i -\beta_i = 0\\\tag{2-3}
\end{align*}
\]</span> 前面提到过，<strong>w</strong>无关了，所以不写<span class="math inline">\({\bf w} =\sum_{i=1}^n\alpha_iy_i{\bf x_i}\)</span> 又因为可以利用等式约束去除<span class="math inline">\(\beta_i\)</span>， <span class="math inline">\(\beta_i \ge 0 \Rightarrow C - \alpha_i \ge 0\)</span></p>
<p>于是2-3可以化简为: <span class="math display">\[
\begin{align*}
\max_{ {\boldsymbol \alpha} }\hspace{2ex}&amp; \sum_{i=1}^n\alpha_i -  \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j{\bf x_i^T x_j} \\
{\rm s.t.} 
\hspace{2ex} &amp; \sum_{i=1}^n\alpha_iy_i  =0\\
\hspace{2ex} &amp; 0  \le \alpha_i \le C\tag{2-4}
\end{align*}
\]</span> 2-4即为我们的<strong>软间隔SVM对偶问题</strong>。其对应的KKT条件为：</p>
<ul>
<li>主问题可行：<span class="math inline">\(y_i({\bf w^\mathsf{T}x}_i + b) \ge 1 -\xi_i ; \hspace{2ex} \xi_i \ge 0\)</span></li>
<li>对偶问题可行：<span class="math inline">\(\alpha_i \ge 0; \hspace{2ex} \beta_i \ge 0\)</span></li>
<li>互补松弛：<span class="math inline">\(\alpha_i(1 - \xi_i - y_i({\bf w^Tx_i}+b)) = 0; \hspace{2ex} \beta_i\xi_i = 0\)</span></li>
<li>2-2的条件: <span class="math inline">\({\bf w} = \sum_{i=1}^n\alpha_iy_i{\bf x_i} ; \hspace{2ex} \sum_{i=1}^n\alpha_iy_i =0 ;\hspace{2ex} C - \alpha_i - \beta_i =0\\\)</span></li>
</ul>
<p>2-4和硬间隔的对偶SVM对比发现，其实就是多了限制<span class="math inline">\(\alpha_i \le C\)</span>。</p>
<h3 id="软间隔svm的支持向量">软间隔SVM的支持向量</h3>
<p>分析软间隔SVM对偶问题的KKT条件，可以得出结论： - 当<span class="math inline">\(\alpha_i = 0\)</span> 此时不是支持向量 - 当<span class="math inline">\(\alpha_i \gt 0 \Rightarrow 1 - \xi_i - y_i({\bf w^Tx_i}+b) = 0\)</span>，此时为支持向量，但可以细分为两种情况 - <span class="math inline">\(\alpha_i &lt; C \Rightarrow \beta_i\ne0 ; \ \xi_i =0\)</span>, 此时样本i正好在间隔上，称为<strong>自由的支持向量free support vector</strong>。 - <span class="math inline">\(\alpha_i = C \Rightarrow \beta_i=0 ; \ \xi_i &gt;0\)</span>，此时<span class="math inline">\({\bf x_i}\)</span>在间隔内，<span class="math inline">\(\xi_i\)</span>反应了和间隔的距离。称为<strong>受限支持向量bounded support vector</strong>。此时又分为三种情况： - <span class="math inline">\(\xi_i \lt 1\)</span> ，分类正确，<span class="math inline">\({\bf x_i}\)</span>在间隔边界和超平面之间 - <span class="math inline">\(\xi_i = 1\)</span> ，<span class="math inline">\({\bf x_i}\)</span>在超平面上 - <span class="math inline">\(\xi_i \gt 1\)</span> ，分类错误，<span class="math inline">\({\bf x_i}\)</span>在超平面误分一侧</p>
<p>由此可以看出，软间隔SVM最终模型仅和支持向量有关，引入软间隔后仍保持稀疏性。</p>
<p>下面是林轩田老师给的图，自由支持向量用方框标出，受限支持向量以三角标出。</p>
<figure>
<img src="../images/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/soft-margin-svm-support-vector.png" alt="soft-margin-svm-support-vector" /><figcaption>soft-margin-svm-support-vector</figcaption>
</figure>
<h3 id="求解对偶问题">求解对偶问题</h3>
<p>假设求解出了<span class="math inline">\(\alpha\)</span>, 那么根据KKT条件, <span class="math inline">\({\bf w} = \sum_{i=1}^n \alpha_iy_i{\bf x_i}\)</span></p>
<p>那b呢？模仿之前的硬间隔SVM对偶问题，选取一个<span class="math inline">\(\alpha_i \gt 0\)</span> 可得<span class="math inline">\(b = y_i - y_i\xi_i - {\bf w^\mathsf{T}x}_i\)</span>，然而这样需要求解<span class="math inline">\(\xi_i\)</span>,因此可以看看另外一个互补松弛条件。<span class="math inline">\(\beta_i\xi_i = 0 \Rightarrow (C - \alpha_i)\xi_i = 0\)</span>,即当<span class="math inline">\(\alpha_i \lt C \Rightarrow \xi_i = 0\)</span>，因此<strong>选取任意的自由的支持向量</strong>即可求解<span class="math inline">\(b = y_i - {\bf w^\mathsf{T}x}_i\)</span></p>
<p>当然，极少数的情况下，没有自由的支持向量，那么b就只能通过一系列不等式得出，此时可能有不止一个b存在。</p>
<h3 id="模型选择">模型选择</h3>
<p>值得注意的是，虽然是软间隔，但仍然可能过拟合。</p>
<p>下面是林轩田老师给出的使用高斯核函数的软间隔SVM的不同参数效果图：</p>
<figure>
<img src="../images/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/soft-margin-svm-gaussian-kernel-parameter.png" alt="soft-margin-svm-gaussian-kernel-parameter" /><figcaption>soft-margin-svm-gaussian-kernel-parameter</figcaption>
</figure>
<p>小的C可能会欠拟合，大的C可能会过拟合。因此要小心的选择参数。</p>
<p>如何选择参数呢，常用的就是交叉验证的方法。</p>
<p>这里推导一个有趣的公式---使用留一交叉验证（即样本数m，m折的交叉验证）的留一误差的上界是: <span class="math display">\[
\rm E_{LOOCV} =\frac{\text{SV_num}}{m}\tag{2-5}
\]</span> 简单证明如下：</p>
<ul>
<li>设总的数据集为<span class="math inline">\(g\)</span> ，此时验证集为样本点<span class="math inline">\(({\bf x_n}, y_n)\)</span></li>
<li>若最优的<span class="math inline">\(\alpha_n = 0\)</span> ,则该点不是支持向量。
<ul>
<li>若把该点去掉，求解出的<span class="math inline">\(\{\alpha_1,\alpha_2,\cdots, \alpha_{n-1}\}\)</span>仍然是最优的</li>
<li>也就是去除非支持向量的点求解出的<span class="math inline">\(g^-\)</span>和不去掉求解出的<span class="math inline">\(g\)</span>是一样的，又因为非支持向量肯定分类正确，于是有：<span class="math inline">\(e_{\rm non-SV} = {\rm err}(g^-, {\rm non\text{-}SV}) = {\rm err}(g, {\rm non\text{-}SV}) = 0\)</span></li>
</ul></li>
<li>而如果是支持向量有：<span class="math inline">\(e_{\rm SV} \le 1\)</span></li>
<li>求和取平均得到式2-5</li>
</ul>
<p>因此，留一交叉验证的error的上界是支持向量个数的比例。如果一个算法的支持向量数比较少，那么它过拟合的风险可能就比较小。因此在调参的时候，可以排除一些支持向量数太多的模型，然后再去验证剩下的模型。</p>
<h2 id="其它">其它</h2>
<p>回顾一下软间隔SVM原始问题2-1： <span class="math display">\[
\begin{align*}
\min_{ {\bf w}, b,\xi}\hspace{2ex}&amp;\frac{1}{2}{\bf w^Tw} + C\cdot \sum_{i=1}^n \xi_i\\
{\rm s.t.} \hspace{2ex} &amp;y_i({\bf w^\mathsf{T}x}_i + b) \ge 1 -\xi_i \\
\hspace{2ex} &amp;\xi_i \ge0, i=1,2,....,n
\end{align*}
\]</span> <span class="math inline">\(\xi_i\)</span>描述了对间隔的破坏程度，啰嗦的分析一下有两种情况：</p>
<ul>
<li>该点破坏了间隔边界，<span class="math inline">\(\xi_i = 1 - y_i({\bf w^T}{\bf x}_i + b) \gt 0\)</span></li>
<li>该点没有破坏边界，<span class="math inline">\(\xi_i = 0\)</span></li>
</ul>
<p>因此，我们可以把软间隔SVM 2-1写为一个没有限制的优化问题： <span class="math display">\[
\min_{ {\bf w}, b}\hspace{2ex}\frac{1}{2}{\bf w^\mathsf{T}w} + C\cdot \sum_{i=1}^n \max \left(  1 - y_i({\bf w^T}{\bf x}_i + b) , 0\right)\\
\tag{3-1}
\]</span> 如果你学过<a href="https://www.hrwhisper.me/machine-learning-regularization/">正则化</a>，会发现上面的式子和正则化是十分相似的，正则化更一般的形式是： <span class="math display">\[
\min_f \hspace{2ex} \Omega(f) + C\sum_{i=1}^n l\left( f({\bf x_i}) , y_i\right) \tag{3-2}
\]</span> 式3-2中<span class="math inline">\(\Omega(f)\)</span>称为<strong>结构风险</strong>，用于描述模型f的某些性质，第二项<span class="math inline">\(l\left( f({\bf x_i}) , y_i\right)\)</span>则称<strong>为经验风险</strong>，用于描述模型和训练数据的契合程度。</p>
<p>我们的式3-1的第一项是<strong>w</strong>的长度，描述了模型本身的复杂度，第二项则是样本中的误差和。换句话说，<strong>大间隔其实就是正则化的一种体现</strong>，代表选择的超平面要少。而软间隔则是一种特殊的损失函数，称为<strong>hinge损失</strong>。参数C出现在3-1和3-2中，若C越大，则代表越小的正则化。</p>
<p>将软间隔SVM看作一种正则化，我们可以将其理论延伸到其它模型，与其它模型建立联系。</p>
<h3 id="hinge损失">Hinge损失</h3>
<p>上一小节已经剧透过，3-1第二项为hinge损失，其它常见的0-1损失替代函数有：</p>
<ul>
<li>hinge损失： <span class="math inline">\(l_{hinge}(z) = \max \left( 1 - z , 0\right)\)</span></li>
<li>指数损失： <span class="math inline">\(l_{exp}(z) = \exp(-z)\)</span></li>
<li>对率损失：<span class="math inline">\(l_{log}(z) = \log(1+\exp(-z))\)</span></li>
</ul>
<p>我们什么始化用到这些损失函数呢？ 对率损失其实是逻辑回归采用的，而指数损失将在后面介绍的Adaboost模型中用到。</p>
<p>这里使用了周志华老师的图来作为对比(PS: 该图的log是以2为底的，这样正好为0-1损失的上界):</p>
<figure>
<img src="../images/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/hinge-exponential-logistic-loss-function.png" alt="hinge-exponential-logistic-loss-function" /><figcaption>hinge-exponential-logistic-loss-function</figcaption>
</figure>
<p>上面提到的hinge、指数、对率损失（以2为底）函数都是0-1损失函数的上界。</p>
<p>hinge和逻辑回归使用的损失函数（对率损失）十分相似：</p>
<ul>
<li>当z趋于正无穷，hinge为0，对率损失约等于0</li>
<li>当z趋于负无穷，它们都趋于负无穷。</li>
</ul>
<p>因此可以把软间隔SVM看作是L2正则化的逻辑回归。而正则化的逻辑回归也像是在做SVM。它们的一些区别和相似周志华老师概况如下：</p>
<blockquote>
<p>支持向量机和对率回归的优化目标相近。通常情形下它们的性能也相当。</p>
<p>对率回归的优势主要在于其输出具有自然的的概率意义，即在给出预测标记的同时也给出了概率，而支持向量机的输出不具有概率意义，要得到概率输出需要进行特殊处理。此外对率回归可以直接用于多分类任务，支持向量机为此则需要进行推广[Platt, 2000]。</p>
<p>另一方面，hinge损失有一快“平坦”的区域（就是z &gt;=1），这使得支持向量机的解具有稀疏性。而对率损失函数是光滑的单调递减函数，不能导出类似支持向量的概念，因此，对率回归的解依赖于更多的训练样本，其预测开销更大。</p>
<p>----From《机器学习》 周志华 P132-133</p>
</blockquote>
<h3 id="概率svm--platt模型">概率SVM--Platt模型</h3>
<p>周志华老师上面总结得很好，逻辑回归一个好处就是直接输出概率，现在我们来讨论如何让SVM模型也有概率输出。</p>
<p>有两种naive的想法：</p>
<ol type="1">
<li>先跑软间隔SVM得到超平面参数<strong>w</strong>和b，然后将它们直接放在Logistic函数（就是那个s的曲线也叫sigmoid函数）计算。这种做法在实际中效果不错，但是失去了我们推导逻辑回归时采用的最大似然的思想等。</li>
<li>先跑软间隔SVM得到超平面参数<strong>w</strong>和b，将这个结果作为逻辑回归的初始权重，然后使用逻辑回归算法得到最后的分类函数。但是这种方法和直接使用逻辑回归的方法结果差不多，并且丢失了SVM的核方法等特性。</li>
</ol>
<p>如果能融合上面两种想法就好了。分析一下：</p>
<p>SVM得到的结果<span class="math inline">\({\bf w}_{\rm SVM}^\mathsf{T}{\boldsymbol{\Phi}({\bf x})} + b_{\rm SVM}\)</span>本质上是一个分数，把这个分数进行放缩和平移，加大自由度。即<span class="math inline">\(g({\bf x}) = \theta(A \cdot ({\bf w}_{\rm SVM}^\mathsf{T}{\boldsymbol{\Phi}({\bf x})} + b_{\rm SVM}) + B)\)</span>。然后通过逻辑回归训练A和B两个参数来达到使用MLE的目的和效果。这个算法本质上还是SVM，因此对偶、核技巧都可以使用。从几何上说，SVM找出分割面的法向量，然后用逻辑回归进行平移放缩微调。通常，如果SVM的解比较好，A &gt; 0 而B接近0.</p>
<p>因此，新的问题为： <span class="math display">\[
\min_{A,B}\hspace{2ex}\frac{1}{N}\sum_{n=1}^N \log\left(1+\exp\left(-y_n(A\cdot (\underbrace{ {\bf w}_{\rm SVM}^\mathsf{T}{\boldsymbol{\Phi}({\bf x}_n)} + b_{\rm SVM} }_{\boldsymbol{\Phi}_{\rm SVM}({\bf x}_n)}) + B)\right)\right)\tag{3-3}
\]</span> 很复杂？其实是两步：</p>
<ol type="1">
<li>解SVM</li>
<li>解逻辑回归（梯度下降等都行）</li>
</ol>
<p>这个方法称为Platt模型。</p>
<h3 id="核logistic回归">核Logistic回归</h3>
<p>上面的问题3-3并没有直接在转换后的空间<span class="math inline">\(\mathcal{Z}\)</span>求解逻辑回归，而是通过核函数把数据变换到<span class="math inline">\(\mathcal{Z}\)</span>空间中。由于没有直接在<span class="math inline">\(\mathcal{Z}\)</span>空间求解逻辑回归，因此可能不是该空间最好的逻辑回归的解。如果要在<span class="math inline">\(\mathcal{Z}\)</span>空间中找逻辑回归的最优解怎么做？</p>
<p>像SVM一样用核方法？但是逻辑回归不是二次规划问题。</p>
<p>回想之前的SVM，不仅在求解<strong>w</strong>的时候用到了<span class="math inline">\(\mathcal{Z}\)</span>空间的内积，更重要的是，在之后预测的时候，将<strong>w</strong>表示成了一堆已经看过的<strong>z</strong>的线性组合，即<span class="math inline">\({\bf w^Tz} + b= \sum_{i=1}^n \alpha_iy_i\Phi({\bf x_i}) \Phi({\bf x}) + b=\sum_{i=1}^n \alpha_iy_iK({\bf x_i,x}) + b\)</span>。换句话说，<strong>w表示成了z的线性组合，是能使用核方法的关键</strong>。</p>
<p>那么，如果我们的<strong>w</strong>能被<strong>z</strong>表示，那不就可以使用核函数了么！</p>
<p>接下来证明如下（这个定理称为<strong>表示定理</strong>）：</p>
<blockquote>
<p>对于任何带有L2正则化的线性模型 <span class="math display">\[
\min_{\bf w}\hspace{2ex}\frac{\lambda}{n}{\bf w^\mathsf{T}w} + \frac{1}{n}\sum_{i=1}^n {\rm err}(y_i, {\bf w^\mathsf{T}z}_i)
\]</span> 其最优的<span class="math inline">\({\bf w_*}=\sum_{i=1}^n\beta_i{\bf z_i}\)</span></p>
</blockquote>
<p>我们可以将最优的<span class="math inline">\({\bf w_*}\)</span>分解为两个向量<span class="math inline">\({\bf w}_{||}\)</span>和<span class="math inline">\({\bf w}_{\perp}\)</span>，即<span class="math inline">\({\bf w}_\ast = {\bf w}_{||} + {\bf w}_\perp\)</span>。其中<span class="math inline">\({\bf w}_{||} \in {\rm span}({\bf z}_i), {\bf w}_{\perp} \perp {\rm span}({\bf z}_i)\)</span></p>
<ul>
<li>由于正交的内积为0，因此有<span class="math inline">\({\rm err}(y_n, {\bf w_\ast^\mathsf{T}z}_n) = {\rm err}(y_n, ({\bf w_\|} + {\bf w}_\perp)^\mathsf{T}{\bf z}_n) = {\rm err}(y_n, {\bf w_\|^\mathsf{T}z}_n)\)</span>，即<span class="math inline">\(\bf w_*\)</span>和<span class="math inline">\(\bf w_{||}\)</span>有相同的error</li>
<li>又因为<span class="math inline">\({\bf w}_\ast^\mathsf{T}{\bf w}_\ast = {\bf w_\|^\mathsf{T}w_\|} + 2{\bf w_\|^\mathsf{T}w_\perp} + {\bf w_\perp^\mathsf{T}w_\perp} = {\bf w_\|^\mathsf{T}w_\|} + {\bf w_\perp^\mathsf{T}w_\perp} &gt; {\bf w_\|^\mathsf{T}w_\|}\)</span>这意味着<span class="math inline">\(\bf w_{||}\)</span>比最优的<span class="math inline">\(\bf w_*\)</span>的目标值更小，是矛盾的，因此<span class="math inline">\({\bf w}_\perp = {\bf 0}\)</span></li>
<li>所以最优解可以用<span class="math inline">\(\bf z\)</span>的线性组合表示。由此得证。</li>
</ul>
<p>接下来定义L2正则项的逻辑回归， <span class="math display">\[
\min_{\bf w}\hspace{2ex}\frac{\lambda}{n}{\bf w}^\mathsf{T}{\bf w} + \frac{1}{n}\sum_{i=1}^n \log(1+\exp(-y_i{\bf w}^\mathsf{T}{\bf z}_i))
\]</span> 由表示定理，最优的<span class="math inline">\({\bf w_*}=\sum_{i=1}^n\beta_i{\bf z_i}\)</span>。将该式代入原始问题，可以将原来关于w的问题转化成关于β的问题，即<strong>核逻辑回归（Kernel Logistic Regression）</strong>： <span class="math display">\[
\min_{\boldsymbol{\beta} } 
\frac{\lambda}{n} \sum_{i=1}^n\sum_{i=1}^n\beta_i\beta_j K({\bf x}_i, {\bf x}_j)
+ \frac{1}{n}\sum_{i=1}^n\log\left(1+\exp\left(-y_i\sum_{j=1}^n\beta_j K({\bf x}_i, {\bf x}_j)\right)\right)\tag{3-4}
\]</span> 这是没有约束的优化问题，可以用梯度下降等方法求解。</p>
<p>林轩田老师讲解了另一个视角来理解核逻辑回归问题：</p>
<ul>
<li>3-4最后一项<span class="math inline">\(\sum_{j=1}^n\beta_j K({\bf x}_i, {\bf x}_j)\)</span>可以看成是先求<span class="math inline">\(\bf x_i\)</span>和其它数据点<span class="math inline">\(\bf x_1,\cdots x_m\)</span>的相似度（前面讲过核函数从某种意义上讲是相似度的一种体现），然后将这个相似度和变量<span class="math inline">\(\beta\)</span>求内积，其实是一种线性模型。</li>
<li>前面的项<span class="math inline">\(\sum_{i=1}^n\sum_{i=1}^n\beta_i\beta_j K({\bf x}_i, {\bf x}_j)\)</span>写成矩阵的形式是：<span class="math inline">\(\boldsymbol{\beta}^\mathsf{T}{\rm K}\boldsymbol{\beta}\)</span>，本质上是一个正则化。</li>
<li>因此核逻辑回归可以看成是关于<span class="math inline">\(\beta\)</span>的线性模型，用核函数转换数据，并且用核函数来正则化。(我们原先的理解是将<strong>w</strong>嵌入到核函数的隐式转换并作L2正则)</li>
</ul>
<p>核逻辑回归和SVM相比，其系数<span class="math inline">\(\beta_i\)</span>通常不为0。</p>
<h2 id="参考资料">参考资料</h2>
<ul>
<li>机器学习技法 - 林轩田</li>
<li>《统计学习方法》 - 李航</li>
<li>《机器学习》 - 周志华</li>
<li>《从零构建支持向量机(SVM)》 - 张皓</li>
</ul>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>hrwhisper
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://www.hrwhisper.me/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/" title="『我爱机器学习』深入理解SVM(二) - 核函数和软边距">https://www.hrwhisper.me/machine-learning-support-vector-machine-2-kernel-function-and-soft-margin-svm/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        <div class="reward-container">
  <div>请我喝杯咖啡吧~</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/donate/wechat_pay.png" alt="hrwhisper 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/donate/alipay.jpg" alt="hrwhisper 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
              <a href="/tags/Machine-Learning-model/" rel="tag"># Machine Learning model</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/machine-learning-support-vector-machine-1/" rel="prev" title="『我爱机器学习』深入理解SVM(一) 原始问题和对偶问题">
      <i class="fa fa-chevron-left"></i> 『我爱机器学习』深入理解SVM(一) 原始问题和对偶问题
    </a></div>
      <div class="post-nav-item">
    <a href="/machine-learning-support-vector-machine-2-kernel-function-regression/" rel="next" title="『我爱机器学习』深入理解SVM(三) - 支持向量机回归">
      『我爱机器学习』深入理解SVM(三) - 支持向量机回归 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="nav-number">1.</span> <span class="nav-text">核函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E6%8A%80%E5%B7%A7%E5%92%8C%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="nav-number">1.1.</span> <span class="nav-text">核技巧和核函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9"><span class="nav-number">1.2.</span> <span class="nav-text">核函数的选择</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%A0%B8"><span class="nav-number">1.2.1.</span> <span class="nav-text">多项式核</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%AB%98%E6%96%AF%E6%A0%B8"><span class="nav-number">1.2.2.</span> <span class="nav-text">高斯核</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0%E6%AF%94%E8%BE%83"><span class="nav-number">1.2.3.</span> <span class="nav-text">核函数比较</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0%E7%9A%84%E6%80%A7%E8%B4%A8"><span class="nav-number">1.3.</span> <span class="nav-text">核函数的性质</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BD%AF%E9%97%B4%E9%9A%94svm"><span class="nav-number">2.</span> <span class="nav-text">软间隔SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BD%AF%E9%97%B4%E9%9A%94svm%E7%9A%84%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98"><span class="nav-number">2.1.</span> <span class="nav-text">软间隔SVM的对偶问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BD%AF%E9%97%B4%E9%9A%94svm%E7%9A%84%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F"><span class="nav-number">2.2.</span> <span class="nav-text">软间隔SVM的支持向量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%82%E8%A7%A3%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98"><span class="nav-number">2.3.</span> <span class="nav-text">求解对偶问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="nav-number">2.4.</span> <span class="nav-text">模型选择</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E5%AE%83"><span class="nav-number">3.</span> <span class="nav-text">其它</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#hinge%E6%8D%9F%E5%A4%B1"><span class="nav-number">3.1.</span> <span class="nav-text">Hinge损失</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E7%8E%87svm--platt%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.2.</span> <span class="nav-text">概率SVM--Platt模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8logistic%E5%9B%9E%E5%BD%92"><span class="nav-number">3.3.</span> <span class="nav-text">核Logistic回归</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">4.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="hrwhisper"
      src="/images/site/avatar.jpg">
  <p class="site-author-name" itemprop="name">hrwhisper</p>
  <div class="site-description" itemprop="description">一个分享机器学习、算法与数据结构，个人学习心得、读书笔记、生活的博客。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">228</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">51</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hrwhisper" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hrwhisper" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/murmured" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;murmured" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2013 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hrwhisper</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"fVcjWMD8aI6F0qEfKdUaHa4f-gzGzoHsz","app_key":"b26lBsbwmVyxTSnNrsBrnv3U","server_url":null,"security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      // script.setAttribute("data-pjax", "");
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'fVcjWMD8aI6F0qEfKdUaHa4f-gzGzoHsz',
      appKey     : 'b26lBsbwmVyxTSnNrsBrnv3U',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
