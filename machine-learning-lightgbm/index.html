<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/site/avatar.jpg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/site/avatar.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/site/avatar.jpg">
  <link rel="mask-icon" href="/images/site/avatar.jpg" color="#222">
  <meta name="google-site-verification" content="fMKqXfnCsLFKKj0NjoZZApB_BuqLVUiJxtRkj-rznU4">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.hrwhisper.me","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="本文介绍LightGBM，它是一款常用的GBDT工具包，由微软亚洲研究院（MSRA）进行开发，在Github上开源的三天内收获1000 star。其速度比XGBoost快，并且精度也相当的不错。 接下来看看其算法的内容。 注意其设计理念：   单个机器在不牺牲速度的情况下，尽可能多地用上更多的数据； 多机并行的时候，通信的代价尽可能地低，并且在计算上可以做到线性加速。   于是其">
<meta property="og:type" content="article">
<meta property="og:title" content="『我爱机器学习』集成学习（四）LightGBM">
<meta property="og:url" content="https://www.hrwhisper.me/machine-learning-lightgbm/index.html">
<meta property="og:site_name" content="细语呢喃">
<meta property="og:description" content="本文介绍LightGBM，它是一款常用的GBDT工具包，由微软亚洲研究院（MSRA）进行开发，在Github上开源的三天内收获1000 star。其速度比XGBoost快，并且精度也相当的不错。 接下来看看其算法的内容。 注意其设计理念：   单个机器在不牺牲速度的情况下，尽可能多地用上更多的数据； 多机并行的时候，通信的代价尽可能地低，并且在计算上可以做到线性加速。   于是其">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-histogram.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-histogram-algorithm.jpg">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-histogram-subtraction.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-Gradient-based-One-Side-Sampling.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-greedy-bundling.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-merge-exclusive-features.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-level-wise-tree-growth.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-leaf-wise-tree-growth.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-feature-parallelization.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-data-parallelization.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-pv-tree.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-voting-parallelization.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-VS-XGBoost.png">
<meta property="article:published_time" content="2018-05-30T07:10:08.000Z">
<meta property="article:modified_time" content="2020-10-19T14:35:48.803Z">
<meta property="article:author" content="hrwhisper">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="Machine Learning model">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-histogram.png">

<link rel="canonical" href="https://www.hrwhisper.me/machine-learning-lightgbm/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>『我爱机器学习』集成学习（四）LightGBM | 细语呢喃</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d6a8cb42bd9ae728375b6726daa75e95";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">细语呢喃</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">技术改变生活</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">25</span></a>

  </li>
        <li class="menu-item menu-item-leetcode">

    <a href="/leetcode-algorithm-solution/" rel="section"><i class="fa fa-archive fa-fw"></i>leetcode</a>

  </li>
        <li class="menu-item menu-item-friends">

    <a href="/friend-link/" rel="section"><i class="fa fa-link fa-fw"></i>friends</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about-me/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.hrwhisper.me/machine-learning-lightgbm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/site/avatar.jpg">
      <meta itemprop="name" content="hrwhisper">
      <meta itemprop="description" content="一个分享机器学习、算法与数据结构，个人学习心得、读书笔记、生活的博客。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="细语呢喃">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          『我爱机器学习』集成学习（四）LightGBM
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-05-30 15:10:08" itemprop="dateCreated datePublished" datetime="2018-05-30T15:10:08+08:00">2018-05-30</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/study/" itemprop="url" rel="index"><span itemprop="name">study</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/study/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span id="/machine-learning-lightgbm/" class="post-meta-item leancloud_visitors" data-flag-title="『我爱机器学习』集成学习（四）LightGBM" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">评论数：</span>
    
    <a title="valine" href="/machine-learning-lightgbm/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/machine-learning-lightgbm/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本文介绍LightGBM，它是一款常用的GBDT工具包，由微软亚洲研究院（MSRA）进行开发，在Github上开源的三天内收获1000 star。其速度比XGBoost快，并且精度也相当的不错。</p>
<p>接下来看看其算法的内容。</p>
<p>注意其设计理念：</p>
<blockquote>
<ol type="1">
<li>单个机器在不牺牲速度的情况下，尽可能多地用上更多的数据；</li>
<li>多机并行的时候，通信的代价尽可能地低，并且在计算上可以做到线性加速。</li>
</ol>
</blockquote>
<p>于是其使用分布式 GBDT，选择了基于 histogram 的决策树算法。 <a id="more"></a></p>
<h2 id="直方图算法">直方图算法</h2>
<p>回顾一下XGBoost中的Exact greedy算法：</p>
<ol type="1">
<li>对每个特征都按照特征值进行排序</li>
<li>在每个排好序的特征都寻找最优切分点</li>
<li>用最优切分点进行切分</li>
</ol>
<p>这个算法比较精确，但是缺点明显：</p>
<ol type="1">
<li>空间消耗大。需要保存数据的特征值。XGBoost采用Block结构，存储指向样本的索引，需要消耗两倍的内存。</li>
<li>时间开销大。在寻找最优切分点时，要对每个特征都进行排序，还要对每个特征的每个值都进行了遍历，并计算增益。</li>
<li>对Cache不友好。使用Block块预排序后，特征对梯度的访问是按照索引来获取的，是一种随机访问，而不同特征访问顺序也不一样，容易照成命中率低的问题。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的Cachemiss。</li>
</ol>
<p>使用直方图算法进行划分点的查找可以很好的克服这些缺点。</p>
<p>PS: XGBoost不是有Cache aware access优化么？但是看LightGBM的对比实验，还是直方图的快。</p>
<h3 id="直方图算法-1">直方图算法</h3>
<p>直方图算法(Histogram algorithm)的做法是把连续的浮点特征值离散化为k个整数（其实又是分桶的思想，而这些桶称为bin）比如<span class="math inline">\([0,0.1) \rightarrow 0,\ [0.1,0.3) \rightarrow 1\)</span>。</p>
<p>同时，将特征根据其所在的bin进行<strong>梯度累加</strong>。这样，遍历一次数据后，直方图累积了需要的梯度信息，然后可以直接根据直方图，寻找最优的切分点。</p>
<figure>
<img src="../images/machine-learning-lightgbm/LightGBM-histogram.png" alt="LightGBM-histogram" /><figcaption>LightGBM-histogram</figcaption>
</figure>
<p>其算法大致描述如下：</p>
<figure>
<img src="../images/machine-learning-lightgbm/LightGBM-histogram-algorithm.jpg" alt="LightGBM-histogram-algorithm" /><figcaption>LightGBM-histogram-algorithm</figcaption>
</figure>
<p>仔细看上面的伪代码，相信你有几个问题：</p>
<ul>
<li>如何将特征映射到bin呢？即如何分桶？</li>
<li>如何构建直方图？直方图算法累加的g是什么？</li>
<li>构建完直方图如何找最优特征，有用到二阶信息么？</li>
</ul>
<h3 id="如何分桶呢">如何分桶呢？</h3>
<p>首先，在读取数据后，就决定每个特征如何分桶。（在<code>feature_group.h</code>文件中，FeatureGroup 的其中一个构造函数）</p>
<p>那么如何分桶呢？对于数值型特征和类别特征采用不同的做法。</p>
<h4 id="数值型特征">数值型特征</h4>
<p>对于<strong>数值型</strong>的特征，关键是寻找分割点，关键代码如下，其中max_bin为最大bin的数量：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; <span class="title">GreedyFindBin</span><span class="params">(<span class="keyword">const</span> <span class="keyword">double</span>* distinct_values, <span class="keyword">const</span> <span class="keyword">int</span>* counts,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> num_distinct_values, <span class="keyword">int</span> max_bin, <span class="keyword">size_t</span> total_cnt, <span class="keyword">int</span> min_data_in_bin)</span> </span>&#123;</span><br><span class="line">	  <span class="comment">// counts为特征取值计数</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; bin_upper_bound;</span><br><span class="line">    CHECK(max_bin &gt; <span class="number">0</span>);</span><br><span class="line">	<span class="comment">// 特征取值数比max_bin数量少，直接取distinct_values的中点放置</span></span><br><span class="line">    <span class="keyword">if</span> (num_distinct_values &lt;= max_bin) &#123;</span><br><span class="line">      bin_upper_bound.clear();</span><br><span class="line">      <span class="keyword">int</span> cur_cnt_inbin = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_distinct_values - <span class="number">1</span>; ++i) &#123;</span><br><span class="line">        cur_cnt_inbin += counts[i];</span><br><span class="line">        <span class="keyword">if</span> (cur_cnt_inbin &gt;= min_data_in_bin) &#123;</span><br><span class="line">          <span class="keyword">auto</span> val = Common::GetDoubleUpperBound((distinct_values[i] + distinct_values[i + <span class="number">1</span>]) / <span class="number">2.0</span>);</span><br><span class="line">          <span class="keyword">if</span> (bin_upper_bound.empty() || !Common::CheckDoubleEqualOrdered(bin_upper_bound.back(), val)) &#123;</span><br><span class="line">            bin_upper_bound.push_back(val);</span><br><span class="line">            cur_cnt_inbin = <span class="number">0</span>;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      cur_cnt_inbin += counts[num_distinct_values - <span class="number">1</span>];</span><br><span class="line">      bin_upper_bound.push_back(<span class="built_in">std</span>::numeric_limits&lt;<span class="keyword">double</span>&gt;::infinity());</span><br><span class="line">    &#125; </span><br><span class="line">	<span class="keyword">else</span> &#123; <span class="comment">// 特征取值比max_bin来得大，说明几个特征取值要共用一个bin</span></span><br><span class="line">      <span class="keyword">if</span> (min_data_in_bin &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        max_bin = <span class="built_in">std</span>::min(max_bin, <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(total_cnt / min_data_in_bin));</span><br><span class="line">        max_bin = <span class="built_in">std</span>::max(max_bin, <span class="number">1</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">double</span> mean_bin_size = <span class="keyword">static_cast</span>&lt;<span class="keyword">double</span>&gt;(total_cnt) / max_bin;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// mean size for one bin</span></span><br><span class="line">      <span class="keyword">int</span> rest_bin_cnt = max_bin;</span><br><span class="line">      <span class="keyword">int</span> rest_sample_cnt = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(total_cnt);</span><br><span class="line">      <span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; <span class="title">is_big_count_value</span><span class="params">(num_distinct_values, <span class="literal">false</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">	  <span class="comment">// 标记一个特征取值数超过mean，因为这些特征需要单独一个bin</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_distinct_values; ++i) &#123;</span><br><span class="line">        <span class="keyword">if</span> (counts[i] &gt;= mean_bin_size) &#123;</span><br><span class="line">          is_big_count_value[i] = <span class="literal">true</span>;</span><br><span class="line">          --rest_bin_cnt;</span><br><span class="line">          rest_sample_cnt -= counts[i];</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">	  <span class="comment">//剩下的特征取值中平均每个bin的取值个数</span></span><br><span class="line">      mean_bin_size = <span class="keyword">static_cast</span>&lt;<span class="keyword">double</span>&gt;(rest_sample_cnt) / rest_bin_cnt;</span><br><span class="line">      <span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; <span class="title">upper_bounds</span><span class="params">(max_bin, <span class="built_in">std</span>::numeric_limits&lt;<span class="keyword">double</span>&gt;::infinity())</span></span>;</span><br><span class="line">      <span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; <span class="title">lower_bounds</span><span class="params">(max_bin, <span class="built_in">std</span>::numeric_limits&lt;<span class="keyword">double</span>&gt;::infinity())</span></span>;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">int</span> bin_cnt = <span class="number">0</span>;</span><br><span class="line">      lower_bounds[bin_cnt] = distinct_values[<span class="number">0</span>];</span><br><span class="line">      <span class="keyword">int</span> cur_cnt_inbin = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_distinct_values - <span class="number">1</span>; ++i) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!is_big_count_value[i]) &#123;</span><br><span class="line">          rest_sample_cnt -= counts[i];</span><br><span class="line">        &#125;</span><br><span class="line">        cur_cnt_inbin += counts[i];</span><br><span class="line">        <span class="comment">// need a new bin，当前的特征如果是需要单独成一个bin，或者当前几个特征计数超过了mean_bin_size，或者下一个是需要独立成列的</span></span><br><span class="line">        <span class="keyword">if</span> (is_big_count_value[i] || cur_cnt_inbin &gt;= mean_bin_size ||</span><br><span class="line">          (is_big_count_value[i + <span class="number">1</span>] &amp;&amp; cur_cnt_inbin &gt;= <span class="built_in">std</span>::max(<span class="number">1.0</span>, mean_bin_size * <span class="number">0.5f</span>))) &#123;</span><br><span class="line">          upper_bounds[bin_cnt] = distinct_values[i]; <span class="comment">// 第i个bin的最大就是 distinct_values[i]了</span></span><br><span class="line">          ++bin_cnt;</span><br><span class="line">          lower_bounds[bin_cnt] = distinct_values[i + <span class="number">1</span>]; <span class="comment">//下一个bin的最小就是distinct_values[i + 1]，注意先++bin了</span></span><br><span class="line">          <span class="keyword">if</span> (bin_cnt &gt;= max_bin - <span class="number">1</span>) &#123; <span class="keyword">break</span>; &#125;</span><br><span class="line">          cur_cnt_inbin = <span class="number">0</span>;</span><br><span class="line">          <span class="keyword">if</span> (!is_big_count_value[i]) &#123;</span><br><span class="line">            --rest_bin_cnt;</span><br><span class="line">            mean_bin_size = rest_sample_cnt / <span class="keyword">static_cast</span>&lt;<span class="keyword">double</span>&gt;(rest_bin_cnt);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      ++bin_cnt;</span><br><span class="line">      <span class="comment">// update bin upper bound</span></span><br><span class="line">      bin_upper_bound.clear();</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; bin_cnt - <span class="number">1</span>; ++i) &#123;</span><br><span class="line">        <span class="keyword">auto</span> val = Common::GetDoubleUpperBound((upper_bounds[i] + lower_bounds[i + <span class="number">1</span>]) / <span class="number">2.0</span>);</span><br><span class="line">        <span class="keyword">if</span> (bin_upper_bound.empty() || !Common::CheckDoubleEqualOrdered(bin_upper_bound.back(), val)) &#123;</span><br><span class="line">          bin_upper_bound.push_back(val);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// last bin upper bound</span></span><br><span class="line">      bin_upper_bound.push_back(<span class="built_in">std</span>::numeric_limits&lt;<span class="keyword">double</span>&gt;::infinity());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> bin_upper_bound;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>上述的代码找到了数值型特征取值的各个bin的切分点，即bin_upper_bound，之后只需要根据这个对特征的取值查找其相应的bin中即可（用二分搜索）。</p>
<h4 id="类别特征">类别特征</h4>
<p>对于<strong>类别</strong>特征来说，主要代码如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sort by counts</span></span><br><span class="line">Common::SortForPair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;(counts_int, distinct_values_int, <span class="number">0</span>, <span class="literal">true</span>);</span><br><span class="line"><span class="comment">// avoid first bin is zero</span></span><br><span class="line"><span class="keyword">if</span> (distinct_values_int[<span class="number">0</span>] == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (counts_int.size() == <span class="number">1</span>) &#123;</span><br><span class="line">        counts_int.push_back(<span class="number">0</span>);</span><br><span class="line">        distinct_values_int.push_back(distinct_values_int[<span class="number">0</span>] + <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">std</span>::swap(counts_int[<span class="number">0</span>], counts_int[<span class="number">1</span>]);</span><br><span class="line">    <span class="built_in">std</span>::swap(distinct_values_int[<span class="number">0</span>], distinct_values_int[<span class="number">1</span>]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// will ignore the categorical of small counts</span></span><br><span class="line"><span class="keyword">int</span> cut_cnt = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;((total_sample_cnt - na_cnt) * <span class="number">0.99f</span>);</span><br><span class="line"><span class="keyword">size_t</span> cur_cat = <span class="number">0</span>;</span><br><span class="line">categorical_2_bin_.clear();</span><br><span class="line">bin_2_categorical_.clear();</span><br><span class="line">num_bin_ = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> used_cnt = <span class="number">0</span>;</span><br><span class="line">max_bin = <span class="built_in">std</span>::min(<span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(distinct_values_int.size()), max_bin);</span><br><span class="line">cnt_in_bin.clear();</span><br><span class="line"><span class="keyword">while</span> (cur_cat &lt; distinct_values_int.size()</span><br><span class="line">       &amp;&amp; (used_cnt &lt; cut_cnt || num_bin_ &lt; max_bin)) &#123;</span><br><span class="line">    <span class="keyword">if</span> (counts_int[cur_cat] &lt; min_data_in_bin &amp;&amp; cur_cat &gt; <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    bin_2_categorical_.push_back(distinct_values_int[cur_cat]);</span><br><span class="line">    categorical_2_bin_[distinct_values_int[cur_cat]] = <span class="keyword">static_cast</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt;(num_bin_);</span><br><span class="line">    used_cnt += counts_int[cur_cat];</span><br><span class="line">    cnt_in_bin.push_back(counts_int[cur_cat]);</span><br><span class="line">    ++num_bin_;</span><br><span class="line">    ++cur_cat;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>关键点有：</p>
<ol type="1">
<li>首先对特征取值按出现的次数排序（大到小）,</li>
<li>取前min(max_bin, distinct_values_int.size())中的每个特征做第3步（这样可能忽略一些出现次数很少的特征取值）：</li>
<li>然后用<span class="math inline">\(bin\_2\_categorical\_\)</span>（vector类型）记录b对应的特征取值，以及用<span class="math inline">\(categorical\_2\_bin\_\)</span>(unordered_map类型) 将特征取值到哪个bin和一一对应起来。这样，以后就能很方便的进行bin到特征取值和特征取值到bin的转化。</li>
</ol>
<h3 id="构建直方图">构建直方图</h3>
<p>给定一个特征的取值，我们现在已经可以转化为对应的bin了。现在我们就可以构建直方图了。</p>
<p>代码如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ConstructHistogram</span><span class="params">(<span class="keyword">const</span> <span class="keyword">data_size_t</span>* data_indices, <span class="keyword">data_size_t</span> num_data,</span></span></span><br><span class="line"><span class="function"><span class="params">                          <span class="keyword">const</span> <span class="keyword">score_t</span>* ordered_gradients, <span class="keyword">const</span> <span class="keyword">score_t</span>* ordered_hessians,</span></span></span><br><span class="line"><span class="function"><span class="params">                          HistogramBinEntry* out)</span> <span class="keyword">const</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">data_size_t</span> rest = num_data &amp; <span class="number">0x3</span>;</span><br><span class="line">    <span class="keyword">data_size_t</span> i = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (; i &lt; num_data - rest; i += <span class="number">4</span>) &#123;</span><br><span class="line">        <span class="keyword">const</span> VAL_T bin0 = data_[data_indices[i]];</span><br><span class="line">        <span class="keyword">const</span> VAL_T bin1 = data_[data_indices[i + <span class="number">1</span>]];</span><br><span class="line">        <span class="keyword">const</span> VAL_T bin2 = data_[data_indices[i + <span class="number">2</span>]];</span><br><span class="line">        <span class="keyword">const</span> VAL_T bin3 = data_[data_indices[i + <span class="number">3</span>]];</span><br><span class="line"></span><br><span class="line">        out[bin0].sum_gradients += ordered_gradients[i];</span><br><span class="line">        out[bin1].sum_gradients += ordered_gradients[i + <span class="number">1</span>];</span><br><span class="line">        out[bin2].sum_gradients += ordered_gradients[i + <span class="number">2</span>];</span><br><span class="line">        out[bin3].sum_gradients += ordered_gradients[i + <span class="number">3</span>];</span><br><span class="line"></span><br><span class="line">        out[bin0].sum_hessians += ordered_hessians[i];</span><br><span class="line">        out[bin1].sum_hessians += ordered_hessians[i + <span class="number">1</span>];</span><br><span class="line">        out[bin2].sum_hessians += ordered_hessians[i + <span class="number">2</span>];</span><br><span class="line">        out[bin3].sum_hessians += ordered_hessians[i + <span class="number">3</span>];</span><br><span class="line"></span><br><span class="line">        ++out[bin0].cnt;</span><br><span class="line">        ++out[bin1].cnt;</span><br><span class="line">        ++out[bin2].cnt;</span><br><span class="line">        ++out[bin3].cnt;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (; i &lt; num_data; ++i) &#123;</span><br><span class="line">        <span class="keyword">const</span> VAL_T bin = data_[data_indices[i]];</span><br><span class="line">        out[bin].sum_gradients += ordered_gradients[i];</span><br><span class="line">        out[bin].sum_hessians += ordered_hessians[i];</span><br><span class="line">        ++out[bin].cnt;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到，累加了一阶和二阶梯度，同时还累加了梯度的和还有个数。（当然还有其它的版本，当<span class="math inline">\(\bf is\_constant\_hessian\)</span>为true的时候是不用二阶梯度的）</p>
<h3 id="寻找最优切分点">寻找最优切分点</h3>
<p>对每个特征都构建好直方图后，就可以进行最优切分点的构建了。</p>
<p>遍历所有的特征，对于每个特征调用FindBestThreshold如下函数：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">FindBestThreshold</span><span class="params">(<span class="keyword">double</span> sum_gradient, <span class="keyword">double</span> sum_hessian, <span class="keyword">data_size_t</span> num_data, <span class="keyword">double</span> min_constraint, <span class="keyword">double</span> max_constraint, SplitInfo* output)</span> </span>&#123;</span><br><span class="line">    output-&gt;default_left = <span class="literal">true</span>;</span><br><span class="line">    output-&gt;gain = kMinScore;</span><br><span class="line">    find_best_threshold_fun_(sum_gradient, sum_hessian + <span class="number">2</span> * kEpsilon, num_data, min_constraint, max_constraint, output);</span><br><span class="line">    output-&gt;gain *= meta_-&gt;penalty;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>同样的，对于数值型和类型特征，处理方法find_best_threshold_fun_是不一样的。</p>
<p>在讲具体做法前，首先讲一个Trick，可以加速直方图计算过程，即<strong>直方图做差加速</strong>。</p>
<h4 id="直方图做差加速">直方图做差加速</h4>
<p>直方图算法还可以进一步加速：<strong>一个叶子节点的直方图可以由它的父亲节点的直方图与其兄弟的直方图做差得到</strong>。</p>
<figure>
<img src="../images/machine-learning-lightgbm/LightGBM-histogram-subtraction.png" alt="LightGBM-histogram-subtraction" /><figcaption>LightGBM-histogram-subtraction</figcaption>
</figure>
<p>原来构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的#bin个桶。使用这个方法，构建完一个叶子的直方图后，可以用非常微小的代价得到它兄弟的直方图，相当于速度提升了一倍。</p>
<h4 id="数值型特征-1">数值型特征</h4>
<p>对于数值型特征，find_best_threshold_fun_函数如下（我去除了一些if条件，这里只是为了说明计算过程）:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">FindBestThresholdNumerical</span><span class="params">(<span class="keyword">double</span> sum_gradient, <span class="keyword">double</span> sum_hessian, <span class="keyword">data_size_t</span> num_data, <span class="keyword">double</span> min_constraint, <span class="keyword">double</span> max_constraint, SplitInfo* output)</span> </span>&#123;</span><br><span class="line">    is_splittable_ = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">double</span> gain_shift = GetLeafSplitGain(sum_gradient, sum_hessian,</span><br><span class="line">                                         meta_-&gt;config-&gt;lambda_l1, meta_-&gt;config-&gt;lambda_l2, meta_-&gt;config-&gt;max_delta_step);</span><br><span class="line">    <span class="keyword">double</span> min_gain_shift = gain_shift + meta_-&gt;config-&gt;min_gain_to_split;</span><br><span class="line">    FindBestThresholdSequence(sum_gradient, sum_hessian, num_data, min_constraint, max_constraint, min_gain_shift, output, <span class="number">-1</span>, <span class="literal">true</span>, <span class="literal">false</span>);</span><br><span class="line">    FindBestThresholdSequence(sum_gradient, sum_hessian, num_data, min_constraint, max_constraint, min_gain_shift, output, <span class="number">1</span>, <span class="literal">true</span>, <span class="literal">false</span>);</span><br><span class="line">    output-&gt;gain -= min_gain_shift;</span><br><span class="line">    output-&gt;monotone_type = meta_-&gt;monotone_type;</span><br><span class="line">    output-&gt;min_constraint = min_constraint;</span><br><span class="line">    output-&gt;max_constraint = max_constraint;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>可以看到FindBestThresholdSequence被调用了两次，分别是从左到右和从右向左，类似XGBoost的缺失值自动寻找划分方向。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">FindBestThresholdSequence</span><span class="params">(<span class="keyword">double</span> sum_gradient, <span class="keyword">double</span> sum_hessian, <span class="keyword">data_size_t</span> num_data, <span class="keyword">double</span> min_constraint, <span class="keyword">double</span> max_constraint, <span class="keyword">double</span> min_gain_shift, SplitInfo* output, <span class="keyword">int</span> dir, <span class="keyword">bool</span> skip_default_bin, <span class="keyword">bool</span> use_na_as_missing)</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int8_t</span> bias = meta_-&gt;bias;</span><br><span class="line">    <span class="keyword">double</span> best_sum_left_gradient = NAN;</span><br><span class="line">    <span class="keyword">double</span> best_sum_left_hessian = NAN;</span><br><span class="line">    <span class="keyword">double</span> best_gain = kMinScore;</span><br><span class="line">    <span class="keyword">data_size_t</span> best_left_count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">uint32_t</span> best_threshold = <span class="keyword">static_cast</span>&lt;<span class="keyword">uint32_t</span>&gt;(meta_-&gt;num_bin);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (dir == <span class="number">-1</span>) &#123; <span class="comment">// from right to left</span></span><br><span class="line">		.......</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123; <span class="comment">// from left to right</span></span><br><span class="line">      <span class="keyword">double</span> sum_left_gradient = <span class="number">0.0f</span>;</span><br><span class="line">      <span class="keyword">double</span> sum_left_hessian = kEpsilon;</span><br><span class="line">      <span class="keyword">data_size_t</span> left_count = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">int</span> t = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">const</span> <span class="keyword">int</span> t_end = meta_-&gt;num_bin - <span class="number">2</span> - bias;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">for</span> (; t &lt;= t_end; ++t) &#123;</span><br><span class="line"></span><br><span class="line">	    sum_left_gradient += data_[t].sum_gradients;</span><br><span class="line">	    sum_left_hessian += data_[t].sum_hessians;</span><br><span class="line">	    left_count += data_[t].cnt;</span><br><span class="line">       </span><br><span class="line">        <span class="keyword">data_size_t</span> right_count = num_data - left_count;</span><br><span class="line">        <span class="keyword">double</span> sum_right_hessian = sum_hessian - sum_left_hessian;</span><br><span class="line">        <span class="keyword">double</span> sum_right_gradient = sum_gradient - sum_left_gradient;</span><br><span class="line">        <span class="comment">// current split gain</span></span><br><span class="line">        <span class="keyword">double</span> current_gain = GetSplitGains(sum_left_gradient, sum_left_hessian, sum_right_gradient, sum_right_hessian,</span><br><span class="line">                                            meta_-&gt;config-&gt;lambda_l1, meta_-&gt;config-&gt;lambda_l2, meta_-&gt;config-&gt;max_delta_step,</span><br><span class="line">                                            min_constraint, max_constraint, meta_-&gt;monotone_type);</span><br><span class="line">        <span class="comment">// gain with split is worse than without split</span></span><br><span class="line">        <span class="keyword">if</span> (current_gain &lt;= min_gain_shift) <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// mark to is splittable</span></span><br><span class="line">        is_splittable_ = <span class="literal">true</span>;</span><br><span class="line">        <span class="comment">// better split point</span></span><br><span class="line">        <span class="keyword">if</span> (current_gain &gt; best_gain) &#123;</span><br><span class="line">          best_left_count = left_count;</span><br><span class="line">          best_sum_left_gradient = sum_left_gradient;</span><br><span class="line">          best_sum_left_hessian = sum_left_hessian;</span><br><span class="line">          best_threshold = <span class="keyword">static_cast</span>&lt;<span class="keyword">uint32_t</span>&gt;(t + bias);</span><br><span class="line">          best_gain = current_gain;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (is_splittable_ &amp;&amp; best_gain &gt; output-&gt;gain) &#123;</span><br><span class="line">      <span class="comment">// update split output information</span></span><br><span class="line">      .....</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>而这个关键的GetSplitGains是什么呢？</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">double</span> <span class="title">GetSplitGains</span><span class="params">(<span class="keyword">double</span> sum_left_gradients, <span class="keyword">double</span> sum_left_hessians,</span></span></span><br><span class="line"><span class="function"><span class="params">                              <span class="keyword">double</span> sum_right_gradients, <span class="keyword">double</span> sum_right_hessians,</span></span></span><br><span class="line"><span class="function"><span class="params">                              <span class="keyword">double</span> l1, <span class="keyword">double</span> l2, <span class="keyword">double</span> max_delta_step,</span></span></span><br><span class="line"><span class="function"><span class="params">                              <span class="keyword">double</span> min_constraint, <span class="keyword">double</span> max_constraint, <span class="keyword">int8_t</span> monotone_constraint)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">double</span> left_output = CalculateSplittedLeafOutput(sum_left_gradients, sum_left_hessians, l1, l2, max_delta_step, min_constraint, max_constraint);</span><br><span class="line">    <span class="keyword">double</span> right_output = CalculateSplittedLeafOutput(sum_right_gradients, sum_right_hessians, l1, l2, max_delta_step, min_constraint, max_constraint);</span><br><span class="line">    <span class="keyword">if</span> (((monotone_constraint &gt; <span class="number">0</span>) &amp;&amp; (left_output &gt; right_output)) ||</span><br><span class="line">      ((monotone_constraint &lt; <span class="number">0</span>) &amp;&amp; (left_output &lt; right_output))) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> GetLeafSplitGainGivenOutput(sum_left_gradients, sum_left_hessians, l1, l2, left_output)</span><br><span class="line">      + GetLeafSplitGainGivenOutput(sum_right_gradients, sum_right_hessians, l1, l2, right_output);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>而第一个调用的函数如下（其实就是类似XGBoost的最优叶子节点输出：<span class="math inline">\(w_j = – \frac{G_j}{H_j+\lambda}\)</span>）：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">double</span> <span class="title">CalculateSplittedLeafOutput</span><span class="params">(<span class="keyword">double</span> sum_gradients, <span class="keyword">double</span> sum_hessians, <span class="keyword">double</span> l1, <span class="keyword">double</span> l2, <span class="keyword">double</span> max_delta_step,</span></span></span><br><span class="line"><span class="function"><span class="params">                                            <span class="keyword">double</span> min_constraint, <span class="keyword">double</span> max_constraint)</span>  </span>=&gt; 调用：</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">double</span> <span class="title">CalculateSplittedLeafOutput</span><span class="params">(<span class="keyword">double</span> sum_gradients, <span class="keyword">double</span> sum_hessians, <span class="keyword">double</span> l1, <span class="keyword">double</span> l2, <span class="keyword">double</span> max_delta_step)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">double</span> ret = -ThresholdL1(sum_gradients -  l1) / (sum_hessians + l2);</span><br><span class="line">    <span class="keyword">if</span> (max_delta_step &lt;= <span class="number">0.0f</span> || <span class="built_in">std</span>::<span class="built_in">fabs</span>(ret) &lt;= max_delta_step) &#123;</span><br><span class="line">      <span class="keyword">return</span> ret;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> Common::Sign(ret) * max_delta_step;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>得到了叶子结点输出后，GetLeafSplitGainGivenOutput其实就是左右子树累加起来，<span class="math inline">\(G_jw_j + \frac{1}{2} (H_j + \lambda) w_j^2\)</span>：和XGBoost一样。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">double</span> <span class="title">GetLeafSplitGainGivenOutput</span><span class="params">(<span class="keyword">double</span> sum_gradients, <span class="keyword">double</span> sum_hessians, <span class="keyword">double</span> l1, <span class="keyword">double</span> l2, <span class="keyword">double</span> output)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">double</span> sg_l1 = ThresholdL1(sum_gradients, l1);</span><br><span class="line">    <span class="keyword">return</span> -(<span class="number">2.0</span> * sg_l1 * output + (sum_hessians + l2) * output * output);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>注意到上面的增益计算出来是左子树+右子树，然后和min_gain_shift比较，而XGBoost是如下形式: <span class="math display">\[
Gain = \frac{1}{2}[\underbrace{\frac{G_L^2}{H_L+\lambda}}_{左子树分数} + \underbrace{\frac{G_R^2}{H_R+\lambda}}_{右子树分数} – \underbrace{\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}}_{分裂前分数}] – \underbrace{\gamma}_{新叶节点复杂度}
\]</span> 难道不一样？其实是一样的，min_gain_shift计算方式一开始的代码就给出了，即：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">double</span> gain_shift = GetLeafSplitGain(sum_gradient, sum_hessian,</span><br><span class="line">                                         meta_-&gt;config-&gt;lambda_l1, meta_-&gt;config-&gt;lambda_l2, meta_-&gt;config-&gt;max_delta_step);</span><br><span class="line"><span class="keyword">double</span> min_gain_shift = gain_shift + meta_-&gt;config-&gt;min_gain_to_split;</span><br></pre></td></tr></table></figure>
<p>就是分裂前的分数！</p>
<p>因此，是和XGBoost一样的。</p>
<h3 id="直方图算法小结">直方图算法小结</h3>
<p>可以看出，直方图算法的有点有：</p>
<ul>
<li>可以减少内存占用，比如离散为256个Bin时，只需要用8位整形就可以保存一个样本被映射为哪个Bin(这个bin可以说就是转换后的特征)，对比预排序的Exact greedy算法来说（用int_32来存储索引+ 用float_32保存特征值），可以节省7/8的空间。</li>
<li>计算效率也得到提高，预排序的Exact greedy对每个特征都需要遍历一遍数据，并计算增益，复杂度为<span class="math inline">\(O(\#feature \times \#data)\)</span>。而直方图算法在建立完直方图后，只需要对每个特征遍历直方图即可，复杂度为<span class="math inline">\(O(\#feature \times \#bins)\)</span>。</li>
<li>提高缓存命中率，因为它访问梯度是连续的（直方图）。</li>
<li>此外，在数据并行的时候，直方图算法可以<strong>大幅降低通信代价。</strong>（数据并行、特征并行在本文后面讲解）</li>
</ul>
<p>当然也有不够精确的缺点：</p>
<blockquote>
<p>当然，Histogram算法并不是完美的。由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。但在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；较粗的分割点<strong>也有正则化的效果</strong>，可以有效地防止过拟合；即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升（Gradient Boosting）的框架下没有太大的影响。</p>
</blockquote>
<h2 id="直方图算法改进">直方图算法改进</h2>
<p>直方图算法仍有优化的空间，建立直方图的复杂度为<span class="math inline">\(O(\#feature \times \#data)\)</span>，如果能<strong>降低特征数</strong>或者<strong>降低样本数</strong>，训练的时间会大大减少。以往的降低样本数的方法中，要么不能直接用在GBDT上，要么会损失精度。而降低特征数的直接想法是去除弱的特征（通常用PCA完成），然而，这些方法往往都假设特征是有冗余的，然而通常特征是精心设计的，去除它们中的任何一个可能会影响训练精度。因此LightGBM提出了GOSS算法和EFB算法。</p>
<h3 id="gradient-based-one-side-samplinggoss">Gradient-based One-Side Sampling（GOSS）</h3>
<p>在AdaBoost中，权重向量w很好的反应了样本的重要性。而在GBDT中，则没有这样的直接权重来反应样本的重要程度。但是梯度是一个很好的指标，<strong>如果一个样本的梯度很小，说明该样本的训练误差很小</strong>，或者说该<strong>样本已经得到了很好的训练(well-trained)</strong>。</p>
<p>要减少样本数，一个直接的想法是抛弃那些梯度很小的样本，但是这样训练集的分布会被改变，可能会使得模型准确率下降。LightGBM提出 Gradient-based One-Side Sampling (GOSS)来解决这个问题。</p>
<p>GOSS的做法伪代码描述如下：</p>
<figure>
<img src="../images/machine-learning-lightgbm/LightGBM-Gradient-based-One-Side-Sampling.png" alt="LightGBM-Gradient-based-One-Side-Sampling" /><figcaption>LightGBM-Gradient-based-One-Side-Sampling</figcaption>
</figure>
<p>即：</p>
<ol type="1">
<li>根据<strong>梯度的绝对值</strong>将样本进行<strong>降序</strong>排序</li>
<li>选择前<span class="math inline">\(a \times100\%\)</span>的样本，这些样本称为A</li>
<li>剩下的数据<span class="math inline">\((1-a) \times100\%\)</span> 的数据中，随机抽取<span class="math inline">\(b \times100\%\)</span>的数据，这些样本称为B</li>
<li>在计算增益的时候，放大样本B中的梯度<span class="math inline">\((1-a) / b\)</span> 倍</li>
<li>关于g，在具体的实现中是一阶梯度和二阶梯度的乘积，见Github的实现（ LightGBM/src/boosting/goss.hpp）</li>
</ol>
<p>使用GOSS进行采样，使得训练算法更加的关注没有充分训练(under-trained)的样本，并且只会稍微的改变原有的数据分布。</p>
<p>原有的在特征j值为d处分数据带来的增益可以定义为： <span class="math display">\[
V_{j|O}(d) = \frac{1}{n_O}\left(\frac{(\sum_{x_i\in O:x_{ij} \le d}g_i)^2}{n_{l|O}^j(d)}  + \frac{(\sum_{x_i\in O:x_{ij} \gt d}g_i)^2}{n_{r|O}^j(d)}  \right)
\]</span> 其中：</p>
<ul>
<li>O为在决策树待分裂节点的训练集</li>
<li><span class="math inline">\(n_o = \sum I(x_i \in O)\)</span></li>
<li><span class="math inline">\(n_{l|O}^j(d) = \sum I[x_i \in O: x_{ij} \le d]\ and\ n_{r|O}^j(d) = \sum I[x_i \in O: x_{ij} \gt d]\)</span></li>
</ul>
<p>而使用GOSS后，增益定义为： <span class="math display">\[
V_{j|O}(d) = \frac{1}{n_O}\left(\frac{(\sum_{x_i\in A_l} g_i + \frac{1-a}{b} \sum_{x_i\in B_l} g_i)^2 }{n_{l}^j(d)}  + \frac{(\sum_{x_i\in A_r} g_i + \frac{1-a}{b} \sum_{x_i\in B_l} g_r)^2 }{n_{r}^j(d)}  \right)
\]</span> 其中:</p>
<ul>
<li><span class="math inline">\(A_l = \{x_i \in A: x_{ij} \le d\}, A_r = \{x_i \in A: x_{ij} \gt d\}\)</span></li>
<li><span class="math inline">\(B_l = \{x_i \in B: x_{ij} \le d\}, B_r = \{x_i \in B: x_{ij} \gt d\}\)</span></li>
</ul>
<h3 id="exclusive-feature-bundlingefb">Exclusive Feature Bundling（EFB）</h3>
<p>一个有高维特征空间的数据往往是稀疏的，而稀疏的特征空间中，许多特征是互斥的。所谓互斥就是他们从来不会同时具有非0值（一个典型的例子是进行One-hot编码后的类别特征）。</p>
<p>LightGBM利用这一点提出Exclusive Feature Bundling（EFB）算法<strong>来进行互斥特征的合并，从而减少特征的数目</strong>。做法是先确定哪些互斥的特征可以合并（可以合并的特征放在一起，称为bundle），然后将各个bundle合并为一个特征。</p>
<p>这样建立直方图的时间将从<span class="math inline">\(O(\#feature \times \#data)\)</span>变为<span class="math inline">\(O(\#bundle \times \#data)\)</span>，而<span class="math inline">\(\#bundle &lt;&lt; \#feature\)</span>，这样GBDT能在精度不损失的情况下进一步提高训练速度。</p>
<p>那么，问题来了：</p>
<ol type="1">
<li>如何判断哪里特征应该放在一个Bundle中？</li>
<li>如何将bundle中的特征合并为一个新的特征？</li>
</ol>
<h4 id="greedy-bundle">Greedy bundle</h4>
<p>对于第1个问题，将特征划分为最少数量的互斥的bundle是NP问题（可以根据图着色问题来证明）。</p>
<p>因此，同样采用近似算法。我们可以构建一张图，图上的顶点代表特征，若两个特征<strong>不互斥</strong>，则在他们之间连一条边。</p>
<p>更进一步的，通常有少量的特征，它们之间并非完全的独立，但是绝大多数情况下，并不会同时取非0值。若构建Bundle的算法允许小的冲突，就能得到更少数的bundle，进一步提高效率。可以证明，随机的污染一部分特征则最多影响精度<span class="math inline">\(O([1-\gamma]n)^{-2/3}\)</span>, <span class="math inline">\(\gamma\)</span>为最大的特征冲突率，也是在速度和精度之间达到平衡的有效手段。</p>
<p>因此，LightGBM的构建bundle算法描述如下（算法3）：</p>
<figure>
<img src="../images/machine-learning-lightgbm/LightGBM-greedy-bundling.png" alt="LightGBM-greedy-bundling" /><figcaption>LightGBM-greedy-bundling</figcaption>
</figure>
<p>即：</p>
<ol type="1">
<li>构造带权图G，边的权重代表两个feature之间冲突的数量</li>
<li>对特征按度降序排序</li>
<li>按顺序对排好序的特征进行遍历，对于当前特征i，查看是否能加入已有的bundle（冲突要小），若不行，则新建一个bundle</li>
</ol>
<p>上述的算法复杂度为<span class="math inline">\(O(\#feature^2)\)</span>，当特征数很大的时候，仍然效率不高。</p>
<p>算法3可以进一步优化：不建立图，<strong>直接按特征的非0值的个数进行排序</strong>。（这也是一种贪心，非0值越多，越可能冲突）。</p>
<h4 id="merge-exclusive-features">Merge Exclusive Features</h4>
<p>现在来回答第2个问题，我们已经有了一个个的bundle，如何将bundle中的特征合并为一个新的特征呢？</p>
<p>回想起在直方图算法中，我们将连续的特征变为一个个离散的bins值，这是以特征为粒度的，即一个特征一张直方图。而合并后，一个很关键的点是<strong>合并后原本不同特征的值要有所体现</strong>，这样在新的特征中遍历直方图才能相当于遍历原来好几个直方图，从而找到切分点。</p>
<p>这可以通过<strong>对原始特征的值添加偏移来实现</strong>，从而将互斥的特征放在不同的bins中。例如，一个Bundle中有两个特征A和B，<span class="math inline">\(A \in [0,10),\ B \in [0,20)\)</span>，可以给特征B添加偏移量10，使得B的值域范围变为<span class="math inline">\(B \in [10,30)\)</span>，然后，A和B就可以合并成值域为<span class="math inline">\([0,30]\)</span>新特征。这就是Merge Exclusive Features（MEF）算法。</p>
<p>伪代码描述如下：</p>
<figure>
<img src="../images/machine-learning-lightgbm/LightGBM-merge-exclusive-features.png" alt="LightGBM-merge-exclusive-features" /><figcaption>LightGBM-merge-exclusive-features</figcaption>
</figure>
<p>通过MEF算法，将许多互斥的稀疏特征转化为稠密的特征，降低了特征的数量，提高了建直方图的效率。</p>
<h2 id="树的生长策略">树的生长策略</h2>
<p>在XGBoost中，树是按层生长的，称为<strong>Level</strong>-wise tree growth，同一层的所有节点都做分裂，最后剪枝，如下图所示：</p>
<figure>
<img src="../images/machine-learning-lightgbm/LightGBM-level-wise-tree-growth.png" alt="LightGBM-level-wise-tree-growth" /><figcaption>LightGBM-level-wise-tree-growth</figcaption>
</figure>
<blockquote>
<p>Level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。</p>
</blockquote>
<p>而LightGBM采用的是<strong>Leaf</strong>-wise tree growth：</p>
<blockquote>
<p>Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。</p>
</blockquote>
<figure>
<img src="../images/machine-learning-lightgbm/LightGBM-leaf-wise-tree-growth.png" alt="LightGBM-leaf-wise-tree-growth" /><figcaption>LightGBM-leaf-wise-tree-growth</figcaption>
</figure>
<h2 id="并行计算">并行计算</h2>
<p>本小节主要根据<a target="_blank" rel="noopener" href="https://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-network-communication">LightGBM的官方文档</a>中提到的并行计算优化进行讲解。</p>
<p>在本小节中，<strong>工作的节点称为worker</strong></p>
<h3 id="特征并行">特征并行</h3>
<p>特征并行主要是并行化决策树中寻找最优划分点(“Find Best Split”)的过程，因为这部分最为耗时。</p>
<h4 id="传统算法">传统算法</h4>
<p>传统算法的做法如下：</p>
<ol type="1">
<li>垂直划分数据（<strong>对特征划分</strong>），不同的worker有<strong>不同的特征集</strong></li>
<li>每个workers找到局部最佳的切分点{feature, threshold}</li>
<li>workers使用点对点通信，找到全局最佳切分点</li>
<li>具有全局最佳切分点的worker进行节点分裂，然后广播切分后的结果（左右子树的instance indices）</li>
<li>其它worker根据收到的instance indices也进行划分</li>
</ol>
<figure>
<img src="../images/machine-learning-lightgbm/LightGBM-feature-parallelization.png" alt="LightGBM-feature-parallelization" /><figcaption>LightGBM-feature-parallelization</figcaption>
</figure>
<p>传统算法的缺点是：</p>
<ol type="1">
<li>无法加速split的过程，该过程复杂度为<span class="math inline">\(O(\#data)\)</span>，当数据量大的时候效率不高</li>
<li>需要广播划分的结果（左右子树的instance indices），1条数据1bit的话，大约需要花费<span class="math inline">\(O(\#data / 8)\)</span></li>
</ol>
<h4 id="lightgbm中的特征并行">LightGBM中的特征并行</h4>
<p>每个worker<strong>保存所有的数据集</strong>，这样找到全局最佳切分点后各个worker都可以自行划分，就不用进行广播划分结果，减小了网络通信量。过程如下：</p>
<ol type="1">
<li>每个workers找到局部最佳的切分点{feature, threshold}</li>
<li>workers使用点对点通信，找到全局最佳切分点</li>
<li>每个worker根据全局全局最佳切分点进行节点分裂</li>
</ol>
<p>但是这样仍然有缺点：</p>
<ol type="1">
<li>split过程的复杂度仍是<span class="math inline">\(O(\#data)\)</span>，当数据量大的时候效率不高</li>
<li>每个worker保存所有数据，存储代价高</li>
</ol>
<h3 id="数据并行">数据并行</h3>
<h4 id="传统算法-1">传统算法</h4>
<p>数据并行目标是并行化整个决策学习的过程：</p>
<ol type="1">
<li>水平切分数据，不同的worker拥有部分数据</li>
<li>每个worker根据本地数据构建局部直方图</li>
<li>合并所有的局部直方图得到全部直方图</li>
<li>根据全局直方图找到最优切分点并进行分裂</li>
</ol>
<figure>
<img src="../images/machine-learning-lightgbm/LightGBM-data-parallelization.png" alt="LightGBM-data-parallelization" /><figcaption>LightGBM-data-parallelization</figcaption>
</figure>
<p>在第3步中，有两种合并的方式：</p>
<ul>
<li>采用点对点方式(point-to-point communication algorithm)进行通讯，每个worker通讯量为<span class="math inline">\(O(\#machine * \#feature * \#bin)\)</span></li>
<li>采用collective communication algorithm(如“<a target="_blank" rel="noopener" href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html">All Reduce</a>”)进行通讯（相当于有一个中心节点，通讯后在返回结果），每个worker的通讯量为<span class="math inline">\(O(2 * \#feature * \#bin)\)</span></li>
</ul>
<p>可以看出通信的代价是很高的，这也是数据并行的缺点。</p>
<h4 id="lightgbm中的数据并行">LightGBM中的数据并行</h4>
<ol type="1">
<li>使用“Reduce Scatter”将不同worker的不同特征的直方图合并，然后workers在局部合并的直方图中找到局部最优划分，最后同步全局最优划分。</li>
<li>前面提到过，可以通过直方图作差法得到兄弟节点的直方图，因此只需要通信一个节点的直方图。</li>
</ol>
<p>通过上述两点做法，通信开销降为<span class="math inline">\(O(0.5 * \#feature * \#bin)\)</span></p>
<h3 id="voting-parallel">Voting Parallel</h3>
<p>LightGBM采用一种称为<strong>PV-Tree</strong>的算法进行投票并行(Voting Parallel)，其实这本质上也是一种<strong>数据并行</strong>。</p>
<p>PV-Tree和普通的决策树差不多，只是在寻找最优切分点上有所不同。</p>
<p>其算法伪代码描述如下：</p>
<figure>
<img src="../images/machine-learning-lightgbm/LightGBM-pv-tree.png" alt="LightGBM-pv-tree" /><figcaption>LightGBM-pv-tree</figcaption>
</figure>
<ol type="1">
<li>水平切分数据，不同的worker拥有部分数据。</li>
<li>Local voting: 每个worker构建直方图，找到top-k个最优的本地划分特征</li>
<li>Global voting: 中心节点聚合得到最优的top-2k个全局划分特征（top-2k是看对各个worker选择特征的个数进行计数，取最多的2k个）</li>
<li>Best Attribute Identification： 中心节点向worker收集这top-2k个特征的直方图，并进行合并，然后计算得到全局的最优划分</li>
<li>中心节点将全局最优划分广播给所有的worker，worker进行本地划分。</li>
</ol>
<figure>
<img src="../images/machine-learning-lightgbm/LightGBM-voting-parallelization.png" alt="LightGBM-voting-parallelization" /><figcaption>LightGBM-voting-parallelization</figcaption>
</figure>
<p>可以看出，PV-tree将原本需要<span class="math inline">\(\#feature \times \#bin\)</span> 变为了<span class="math inline">\(2k \times \#bin\)</span>，通信开销得到降低。此外，可以证明，当每个worker的数据足够多的时候，top-2k个中包含全局最佳切分点的概率非常高。</p>
<h2 id="小结">小结</h2>
<p>LightGBM 和 XGBoost对比如下：</p>
<figure>
<img src="../images/machine-learning-lightgbm/LightGBM-VS-XGBoost.png" alt="LightGBM-VS-XGBoost" /><figcaption>LightGBM-VS-XGBoost</figcaption>
</figure>
<h2 id="参考资料">参考资料</h2>
<ul>
<li>Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. &quot;<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree">LightGBM: A Highly Efficient Gradient Boosting Decision Tree</a>&quot;. In Advances in Neural Information Processing Systems (NIPS), pp. 3149-3157. 2017.</li>
<li>Qi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, Tieyan Liu. &quot;<a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/6380-a-communication-efficient-parallel-algorithm-for-decision-tree">A Communication-Efficient Parallel Algorithm for Decision Tree</a>&quot;. Advances in Neural Information Processing Systems 29 (NIPS 2016).</li>
<li>GBDT算法原理与系统设计简介 - weapon</li>
<li>GBDT详解 - 火光摇曳</li>
<li><p><a target="_blank" rel="noopener" href="https://www.msra.cn/zh-cn/news/features/lightgbm-20170105">开源 | LightGBM：三天内收获GitHub 1000 星</a></p></li>
<li><a target="_blank" rel="noopener" href="https://v.qq.com/x/page/k0362z6lqix.html">如何玩转LightGBM</a></li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/Microsoft/LightGBM/">LightGBM github地址</a></p></li>
</ul>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>hrwhisper
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://www.hrwhisper.me/machine-learning-lightgbm/" title="『我爱机器学习』集成学习（四）LightGBM">https://www.hrwhisper.me/machine-learning-lightgbm/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        <div class="reward-container">
  <div>请我喝杯咖啡吧~</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/donate/wechat_pay.png" alt="hrwhisper 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/donate/alipay.jpg" alt="hrwhisper 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
              <a href="/tags/Machine-Learning-model/" rel="tag"># Machine Learning model</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/machine-learning-xgboost/" rel="prev" title="『我爱机器学习』集成学习（三）XGBoost">
      <i class="fa fa-chevron-left"></i> 『我爱机器学习』集成学习（三）XGBoost
    </a></div>
      <div class="post-nav-item">
    <a href="/machine-learning-fm-ffm-deepfm-deepffm/" rel="next" title="『我爱机器学习』FM、FFM与DeepFM">
      『我爱机器学习』FM、FFM与DeepFM <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%B4%E6%96%B9%E5%9B%BE%E7%AE%97%E6%B3%95"><span class="nav-number">1.</span> <span class="nav-text">直方图算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B4%E6%96%B9%E5%9B%BE%E7%AE%97%E6%B3%95-1"><span class="nav-number">1.1.</span> <span class="nav-text">直方图算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%88%86%E6%A1%B6%E5%91%A2"><span class="nav-number">1.2.</span> <span class="nav-text">如何分桶呢？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E5%80%BC%E5%9E%8B%E7%89%B9%E5%BE%81"><span class="nav-number">1.2.1.</span> <span class="nav-text">数值型特征</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%B1%BB%E5%88%AB%E7%89%B9%E5%BE%81"><span class="nav-number">1.2.2.</span> <span class="nav-text">类别特征</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA%E7%9B%B4%E6%96%B9%E5%9B%BE"><span class="nav-number">1.3.</span> <span class="nav-text">构建直方图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%BB%E6%89%BE%E6%9C%80%E4%BC%98%E5%88%87%E5%88%86%E7%82%B9"><span class="nav-number">1.4.</span> <span class="nav-text">寻找最优切分点</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%81%9A%E5%B7%AE%E5%8A%A0%E9%80%9F"><span class="nav-number">1.4.1.</span> <span class="nav-text">直方图做差加速</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E5%80%BC%E5%9E%8B%E7%89%B9%E5%BE%81-1"><span class="nav-number">1.4.2.</span> <span class="nav-text">数值型特征</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B4%E6%96%B9%E5%9B%BE%E7%AE%97%E6%B3%95%E5%B0%8F%E7%BB%93"><span class="nav-number">1.5.</span> <span class="nav-text">直方图算法小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%B4%E6%96%B9%E5%9B%BE%E7%AE%97%E6%B3%95%E6%94%B9%E8%BF%9B"><span class="nav-number">2.</span> <span class="nav-text">直方图算法改进</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#gradient-based-one-side-samplinggoss"><span class="nav-number">2.1.</span> <span class="nav-text">Gradient-based One-Side Sampling（GOSS）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#exclusive-feature-bundlingefb"><span class="nav-number">2.2.</span> <span class="nav-text">Exclusive Feature Bundling（EFB）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#greedy-bundle"><span class="nav-number">2.2.1.</span> <span class="nav-text">Greedy bundle</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#merge-exclusive-features"><span class="nav-number">2.2.2.</span> <span class="nav-text">Merge Exclusive Features</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%91%E7%9A%84%E7%94%9F%E9%95%BF%E7%AD%96%E7%95%A5"><span class="nav-number">3.</span> <span class="nav-text">树的生长策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97"><span class="nav-number">4.</span> <span class="nav-text">并行计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%B9%B6%E8%A1%8C"><span class="nav-number">4.1.</span> <span class="nav-text">特征并行</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95"><span class="nav-number">4.1.1.</span> <span class="nav-text">传统算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lightgbm%E4%B8%AD%E7%9A%84%E7%89%B9%E5%BE%81%E5%B9%B6%E8%A1%8C"><span class="nav-number">4.1.2.</span> <span class="nav-text">LightGBM中的特征并行</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C"><span class="nav-number">4.2.</span> <span class="nav-text">数据并行</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95-1"><span class="nav-number">4.2.1.</span> <span class="nav-text">传统算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lightgbm%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C"><span class="nav-number">4.2.2.</span> <span class="nav-text">LightGBM中的数据并行</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#voting-parallel"><span class="nav-number">4.3.</span> <span class="nav-text">Voting Parallel</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93"><span class="nav-number">5.</span> <span class="nav-text">小结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">6.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="hrwhisper"
      src="/images/site/avatar.jpg">
  <p class="site-author-name" itemprop="name">hrwhisper</p>
  <div class="site-description" itemprop="description">一个分享机器学习、算法与数据结构，个人学习心得、读书笔记、生活的博客。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">228</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">51</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hrwhisper" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hrwhisper" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/murmured" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;murmured" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2013 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hrwhisper</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"fVcjWMD8aI6F0qEfKdUaHa4f-gzGzoHsz","app_key":"b26lBsbwmVyxTSnNrsBrnv3U","server_url":null,"security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      // script.setAttribute("data-pjax", "");
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'fVcjWMD8aI6F0qEfKdUaHa4f-gzGzoHsz',
      appKey     : 'b26lBsbwmVyxTSnNrsBrnv3U',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
