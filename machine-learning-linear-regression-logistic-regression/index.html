<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/site/avatar.jpg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/site/avatar.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/site/avatar.jpg">
  <link rel="mask-icon" href="/images/site/avatar.jpg" color="#222">
  <meta name="google-site-verification" content="fMKqXfnCsLFKKj0NjoZZApB_BuqLVUiJxtRkj-rznU4">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.hrwhisper.me","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="本文介绍：  线性回归 逻辑回归 Softmax 并行化逻辑回归">
<meta property="og:type" content="article">
<meta property="og:title" content="『我爱机器学习』线性回归、逻辑回归与Softmax">
<meta property="og:url" content="https://www.hrwhisper.me/machine-learning-linear-regression-logistic-regression/index.html">
<meta property="og:site_name" content="细语呢喃">
<meta property="og:description" content="本文介绍：  线性回归 逻辑回归 Softmax 并行化逻辑回归">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-linear-regression-logistic-regression/log-linear-regression.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-linear-regression-logistic-regression/Logistic-curve.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-linear-regression-logistic-regression/lr-data-feature-parallel.png">
<meta property="og:image" content="https://www.hrwhisper.me/images/machine-learning-linear-regression-logistic-regression/Google-DistBelief.png">
<meta property="article:published_time" content="2018-03-05T09:08:00.000Z">
<meta property="article:modified_time" content="2020-10-21T15:38:36.927Z">
<meta property="article:author" content="hrwhisper">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="Machine Learning model">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.hrwhisper.me/images/machine-learning-linear-regression-logistic-regression/log-linear-regression.png">

<link rel="canonical" href="https://www.hrwhisper.me/machine-learning-linear-regression-logistic-regression/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>『我爱机器学习』线性回归、逻辑回归与Softmax | 细语呢喃</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-69270533-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-69270533-1');
      }
    </script>


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d6a8cb42bd9ae728375b6726daa75e95";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">细语呢喃</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">技术改变生活</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">25</span></a>

  </li>
        <li class="menu-item menu-item-leetcode">

    <a href="/leetcode-algorithm-solution/" rel="section"><i class="fa fa-archive fa-fw"></i>leetcode</a>

  </li>
        <li class="menu-item menu-item-friends">

    <a href="/friend-link/" rel="section"><i class="fa fa-link fa-fw"></i>friends</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about-me/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.hrwhisper.me/machine-learning-linear-regression-logistic-regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/site/avatar.jpg">
      <meta itemprop="name" content="hrwhisper">
      <meta itemprop="description" content="一个分享机器学习、算法与数据结构，个人学习心得、读书笔记、生活的博客。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="细语呢喃">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          『我爱机器学习』线性回归、逻辑回归与Softmax
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-03-05 17:08:00" itemprop="dateCreated datePublished" datetime="2018-03-05T17:08:00+08:00">2018-03-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/study/" itemprop="url" rel="index"><span itemprop="name">study</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/study/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span id="/machine-learning-linear-regression-logistic-regression/" class="post-meta-item leancloud_visitors" data-flag-title="『我爱机器学习』线性回归、逻辑回归与Softmax" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">评论数：</span>
    
    <a title="valine" href="/machine-learning-linear-regression-logistic-regression/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/machine-learning-linear-regression-logistic-regression/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本文介绍：</p>
<ul>
<li>线性回归</li>
<li>逻辑回归</li>
<li>Softmax</li>
<li>并行化逻辑回归 <a id="more"></a></li>
</ul>
<h2 id="线性回归">线性回归</h2>
<p>回归问题举个简单的例子，如预测股票的价格，输出空间y不再是一个标签，而是一个实数集。</p>
<p>对此，线性回归问题的假设是： <span class="math display">\[
f({\bf x}) = {\bf w^T x} + b
\]</span></p>
<p>可以看出和感知机的模型很像，只不过不用取sign，因为最后结果就是个连续的值。</p>
<p>有时为了方便表示，将b吸收进<strong>w</strong>，而把所有的样本<strong>x</strong>添加一列为1，成为一个矩阵X： <span class="math display">\[
{\bf w} = ({\bf w_{old} },b)
\]</span></p>
<p><span class="math display">\[
\rm X = \begin{bmatrix}
x_{11}   &amp; x_{12}     &amp; \cdots &amp; x_{1d} &amp; 1    \\
x_{21}   &amp; x_{22}     &amp; \cdots &amp; x_{2d} &amp; 1    \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
x_{n1}   &amp; x_{n2}     &amp; \cdots &amp; x_{nd} &amp; 1    \\
\end{bmatrix}
\]</span></p>
<p>于是得到： <span class="math display">\[
f({\bf x}) = \rm X{\bf w}
\]</span></p>
<p>设样本数为n，特征数为d，则维度为：</p>
<ul>
<li><span class="math inline">\(\rm X\)</span>: n * (d + 1)</li>
<li><span class="math inline">\({\bf y}\)</span>: n * 1</li>
<li><span class="math inline">\({\bf w}\)</span>: (d + 1) * 1</li>
</ul>
<h3 id="优化目标">优化目标</h3>
<p>在回归问题中，<strong>均方误差</strong>是常见的性能度量，公式如下： <span class="math display">\[
{\rm err}(f({\bf x}),  y) = (f({\bf x}) - y)^2
\]</span> 均方误差对应了常见的欧几里得距离，基于均方误差最小化来进行模型求解的方法称为<strong>最小二乘法</strong>。</p>
<p>线性回归便是用均方误差作为损失函数， <span class="math display">\[
\begin{align*}  L({\bf w}) 
&amp; = (f({\bf x}) - {\bf y})^2 \\
&amp; = (\rm X{\bf w} - {\bf y})^T (\rm X{\bf w} - {\bf y})
\end{align*}
\]</span> 其实也就是找一个<strong>w</strong>使得损失函数最小：</p>
<p><span class="math display">\[
{\bf w}^* = \rm{arg min_{\bf w} }  (X{\bf w} - {\bf y})^T (X{\bf w} - {\bf y})
\]</span></p>
<h3 id="求解">求解</h3>
<p>对<strong>w</strong>求导得： <span class="math display">\[
\frac{\partial L({\bf w})}{\partial {\bf w} } = 2\rm X^T(\rm X{\bf w} - {\bf y})
\]</span> 若<span class="math inline">\(\rm X^TX\)</span>是可逆的，令导数为0便得到： <span class="math display">\[
{\bf w}^* = ({\rm X^TX})^{-1}{\rm X^T}{\bf y}
\]</span></p>
<p>通常有特征数d &lt;&lt; 样本数m，使得<span class="math inline">\(\rm X^TX\)</span>满秩，有唯一解。</p>
<p>但是也有许多情况m &lt; d,这样<span class="math inline">\(\rm X^TX\)</span>不是满秩的，会有多组解。此时可以解出多个<strong>w</strong>，都能使均方误差最小化。选择哪一解作为输出将由学习算法归纳偏好决定。</p>
<p>实际求解过程中，一般都直接求解<span class="math inline">\(\rm (X^TX)^{−1}X^T\)</span> ，<span class="math inline">\(\rm (X^TX)^{−1}X^T\)</span>称为<span class="math inline">\(\rm X\)</span>的伪逆，记作<span class="math inline">\({\rm X}^\dagger\)</span>。</p>
<p>求解出伪逆后，线性回归模型为： <span class="math display">\[
f({\bf x}) = \rm X (X^\dagger {\bf y})
\]</span></p>
<h3 id="空间变换">空间变换</h3>
<p>线性回归可以进行输出空间的变换。</p>
<p>比如当我们觉得样本的是在指数尺度上变化，我们可以加个指数的变换使得<span class="math inline">\(f({\bf x})\)</span>接近y <span class="math display">\[
f({\bf x}) = e^{ {\bf w^Tx}+b}
\]</span> 也就是求解： <span class="math display">\[
\ln y = {\bf w^T x }+b
\]</span> 便得到了对数线性回归，如图（图来自周志华的《机器学习》）：</p>
<figure>
<img src="../images/machine-learning-linear-regression-logistic-regression/log-linear-regression.png" alt="Logistic-curve" /><figcaption>Logistic-curve</figcaption>
</figure>
<p>这里对数函数起到了将线性回归模型的预测值与真实值联系起来的作用。</p>
<p>更一般的，考虑单调可谓函数g，令： <span class="math display">\[
y = g^{-1}({\bf w^Tx}+b)
\]</span> 这样的模型称为”<strong>广义线性模型</strong>“。g称为联系函数，显然对数线性回归是g = ln时的特例。</p>
<h2 id="逻辑回归">逻辑回归</h2>
<p>现在来讲讲逻辑回归（Logistic Regression），虽然它的名字叫“回归”，但其实它用于<strong>分类</strong>任务。</p>
<p>在二分类问题中，我们的训练数据集是 <span class="math inline">\(T = \{(x_1,y_1), \cdots,(x_n,y_n)\}\)</span> ，其中<span class="math inline">\(y_i \in \{-1, +1\}\)</span>。训练数据的标签是-1和+1，那么我们预测也是输出+1或者-1标签。但是有时候我们想知道的是概率，即为正例或负例的概率是多少。比如用于医院的任务中，给定一个病人，预测其是否得某种病a。不仅是关心ta是否得病，还关心得病的概率是多少。这个问题与原来的二分类问题有所差别，被称作“<strong>软二元分类</strong>”问题。求解此问题得到的值越高，说明越有可能是得病a的，否则越有可能是不得病a的。此时目标函数会变化成<span class="math inline">\(f({\bf x})=P(+1|{\bf x})∈[0,1]\)</span>的形式</p>
<p>和之前一样，我们仍可以假设计算加权分数，即 <span class="math display">\[
z = \sum_{j=0}^d w_jx_j = {\bf w^Tx}
\]</span> 然后，将该分数缩放到[0, 1]这个区间。而缩放操作通常使用<strong>Logistic函数</strong>来完成，因此这个问题称为Logistic回归问题，假设也被称为Logistic假设。</p>
<h3 id="logistic函数与逻辑回归模型">Logistic函数与逻辑回归模型</h3>
<p>现在介绍神奇的Logistic函数 <span class="math display">\[
\sigma(z)= \frac{e^{\bf z} }{1+e^{\bf z} } = \frac{1}{1+e^{-{\bf z} }}
\]</span> 画出二维的图：</p>
<figure>
<img src="../images/machine-learning-linear-regression-logistic-regression/Logistic-curve.png" alt="Logistic-curve" /><figcaption>Logistic-curve</figcaption>
</figure>
<p>从上图可以看出，对数几率函数是一种<strong>Sigmoid函数</strong>(即形似s的函数)，它将z值转化为接近0或1的y值。</p>
<p>将上面的假设带入Logistic函数，便得到我们的<strong>逻辑回归模型</strong>，这里设为<span class="math inline">\(h({\bf x})\)</span> : <span class="math display">\[
h({\bf x}) = \sigma({\bf w^Tx}) = \frac{1}{1+e^{-({\bf w^Tx})} } \tag{2-1}
\]</span> 按照上面的说法，我们进行了”映射“，这就是输出为正例的概率，因此 <span class="math display">\[
\begin{align}
p(Y = 1 | {\bf x}) &amp;= h({\bf x}) = \frac{1}{1+e^{-{\bf w^Tx} }} \\
p(Y = 0 | {\bf x}) &amp;= 1-P(Y = 1|{\bf x}) \\
                    &amp;= \frac{1}{1+e^{ {\bf w^Tx} }} \\
\end{align}
\]</span> 现在可以讨论Logistic函数的特点了，一个事件的<strong>几率</strong>(odds) 是该事件发生的概率与该事件不发生概率的比值，如果事件发生的概率为p，那么该事件的几率是<span class="math inline">\(\frac{p}{1-p}\)</span>，该事件的<strong>对数几率</strong>（就是取对数）是： <span class="math display">\[
\ln \frac{p}{1-p}
\]</span> 对于逻辑回归而言： <span class="math display">\[
\ln \frac{P(Y = 1|{\bf x}) }{1-P(Y = 1|{\bf x}) } = \ln \frac{\frac{e^{\bf w^Tx} }{1+e^{\bf w^Tx} }}{\frac{1}{1+e^{\bf w^Tx} }} =  {\bf w^Tx}
\]</span> 因此可以看出，实际上我们的逻辑回归模型是<strong>用线性回归模型的预测结果去逼近真实标记的对数几率。</strong></p>
<p>因此，我们用Logistic函数，并使用了线性函数<span class="math inline">\(\bf w^Tx\)</span>完成了实数域到输出区间[0,1]（概率值）的转化。</p>
<h3 id="参数估计与损失函数">参数估计与损失函数</h3>
<p>那么，如何确定逻辑回归模型中的<strong>w</strong>呢？我们可以使用<strong>极大似然法（maximum likelihood method）</strong>估计模型参数<strong>w</strong>。</p>
<p>写出似然函数为： <span class="math display">\[
\prod_{i=1}^N h({\bf x_i})^{y_i} (1- h({\bf x_i}))^{1-y_i}\\
\]</span> 取对数似然为： <span class="math display">\[
\sum_{i=1}^N [y_i\ln h({\bf x_i}) + (1-y_i)\ln(1- h({\bf x_i}))]
\]</span></p>
<p>求上式的最大值即可得到<strong>w</strong>的估计值。但实际中，我们一般喜欢优化极小值，因此取个负号，得到我们逻辑回归的损失函数： <span class="math display">\[
L(w) = -\sum_{i=1}^N [y_i\ln h({\bf x_i}) + (1-y_i)\ln(1- h({\bf x_i}))]\tag{2-2}
\]</span></p>
<h3 id="梯度下降求解">梯度下降求解</h3>
<p>式2-2给出了损失函数L(<strong>w</strong>)，该函数是关于<strong>w</strong>的高阶可导连续凸函数，可以用梯度下降法、牛顿法求解。</p>
<p>下面使用最常用的梯度下降法。</p>
<p>为了方便的求解偏导，首先计算sigmoid函数的导数<span class="math inline">\(\sigma(z) = \frac{1}{1+e^{-z} }\)</span>的偏导： <span class="math display">\[
\begin{align*}
\frac{\partial \sigma}{\partial z} &amp;= -\left(\frac{1}{1+e^{-z} }\right)^2 \frac{\partial e^{-z} }{\partial z}\\
&amp;= -\left(\frac{1}{1+e^{-z} }\right)^2  e^{-z} (-1)\\
&amp;= \sigma(z) \left(\frac{ e^{-z} }{1+e^{-z} }\right) \\
&amp;=\sigma(z)(1 - \sigma(z))
\end{align*}
\]</span> 对于<span class="math inline">\(h({\bf x_i})\)</span>对<span class="math inline">\(w_j\)</span>的偏导则为： <span class="math display">\[
\begin{align*}
\frac{\partial h({\bf x_i})}{\partial w_j} &amp;= -\left(\frac{1}{1+e^{-z} }\right)^2 \frac{\partial e^{-z} }{\partial z}\frac{\partial z}{\partial w_j}\\
&amp;=\sigma(z)(1 - \sigma(z)) x_{ij} \\
&amp;= h({\bf x_i})(1-h({\bf x_i}))x_{ij}
\end{align*}
\]</span> 求完前面，现在可以容易求L(<strong>w</strong>)对<span class="math inline">\(w_j\)</span>求偏导啦： <span class="math display">\[
\begin{align*} \frac{\partial L(\bf w)}{\partial w_j} 
    &amp;= -\sum_{i=1}^N [y_i(1-h({\bf x_i}))x_{ij} - (1-y_i)h({\bf x_i})x_{ij}]\\
    &amp;= \sum_{i=1}^N[-y_ix_{ij} + h({\bf x_i})x_{ij}] \\
     &amp;= \sum_{i=1}^N \left(h({\bf x_i}) -y_i \right)x_{ij}
\end{align*}
\]</span> 于是，<strong>w</strong>的更新公式为： <span class="math display">\[
w_j = w_j - \eta\sum_{i=1}^N(h({\bf x_i}) -y_i )x_{ij}\tag{2-3}
\]</span></p>
<p>注意上述的更新公式有连加符号，在python的实现中，如果用for是非常慢的，我们希望能向量化上述式子，从而使用Numpy来加速过程。</p>
<p>查了半天资料，基本没个详细的，自己推吧。</p>
<p>那么，怎么加速呢？</p>
<p>分析公式2-3，我们发现<span class="math inline">\(w_j\)</span>更新是<span class="math inline">\(w_j\)</span>减去步长 乘上 每个训练数据的<span class="math inline">\(h({\bf x_i})-y_i\)</span>并乘上对应的<span class="math inline">\(\bf x_i\)</span>的j维。</p>
<p>如何消去累加? 联想到向量乘法或者矩阵乘法就是有累加的过程！</p>
<p>本文最开始的时候讲过：</p>
<blockquote>
<p><span class="math display">\[
\rm X = \begin{bmatrix}
x_{11}   &amp; x_{12}     &amp; \cdots &amp; x_{1d} &amp; 1    \\
x_{21}   &amp; x_{22}     &amp; \cdots &amp; x_{2d} &amp; 1    \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
x_{n1}   &amp; x_{n2}     &amp; \cdots &amp; x_{nd} &amp; 1    \\
\end{bmatrix}
\]</span></p>
<p>维度如下：</p>
<ul>
<li>X: N * (d + 1)</li>
<li><span class="math inline">\({\bf y}\)</span>: N * 1</li>
<li><span class="math inline">\({\bf w}\)</span>: (d + 1) * 1</li>
</ul>
</blockquote>
<p>我们可以构造矩阵 <span class="math display">\[
\begin{align}
X^T(h(x) - y) 
&amp;= \begin{bmatrix} 
            x_{11}   &amp; x_{12}     &amp; \cdots &amp; x_{1d} &amp; 1    \\
            x_{21}   &amp; x_{22}     &amp; \cdots &amp; x_{2d} &amp; 1    \\
            \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
            x_{n1}   &amp; x_{n2}     &amp; \cdots &amp; x_{nd} &amp; 1    \\
    \end{bmatrix} ^T 
    \begin{bmatrix}h(x_1) - y_1  \\h(x_2) - y_2    \\ \vdots  \\h(x_n) - y_n   \\\end{bmatrix}
\\&amp; = \begin{bmatrix} 
                x_{11}   &amp; x_{21}     &amp; \cdots &amp; x_{n1}     \\
                x_{12}   &amp; x_{22}     &amp; \cdots &amp; x_{n2}     \\
                \vdots &amp; \vdots &amp; \ddots &amp; \vdots  \\
                x_{1d}   &amp; x_{2d}     &amp; \cdots &amp; x_{nd}    \\
                1   &amp; 1     &amp; \cdots &amp;1   \\
         \end{bmatrix} 
    \begin{bmatrix}h(x_1) - y_1  \\h(x_2) - y_2    \\ \vdots  \\h(x_n) - y_n   \\\end{bmatrix}
\\&amp;= \begin{bmatrix} 
                x_{11}   &amp; x_{21}     &amp; \cdots &amp; x_{n1}     \\
                x_{12}   &amp; x_{22}     &amp; \cdots &amp; x_{n2}     \\
                \vdots &amp; \vdots &amp; \ddots &amp; \vdots  \\
                x_{1d}   &amp; x_{2d}     &amp; \cdots &amp; x_{nd}    \\
                x_{1(d+1)}   &amp; x_{2(d+1)}     &amp; \cdots &amp; x_{n(d+1)}    \\
         \end{bmatrix} 
         \begin{bmatrix}h(x_1) - y_1  \\h(x_2) - y_2    \\ \vdots  \\h(x_n) - y_n  \\\end{bmatrix}
\\&amp;= \begin{bmatrix} 
                \sum_{i=1}^n(h(x_i)-y_i) x_{i1}    \\
                \sum_{i=1}^n(h(x_i)-y_i) x_{i2}   \\
                \vdots   \\
                 \sum_{i=1}^n(h(x_i)-y_i) x_{id}   \\
                \sum_{i=1}^n(h(x_i)-y_i) x_{i(d+1)}  \\
         \end{bmatrix}
\end{align}
\]</span> 推导到这很明显了吧？</p>
<p>还不明显，那么继续： <span class="math display">\[
\begin{align}
{\bf w} &amp;= {\bf w}- \eta {\rm X^T}(h({\bf x}) - {\bf y}) 
\\&amp; =  \begin{bmatrix} 
                w_1 \\
                w_2 \\
                \vdots\\
                w_d\\
                w_{d+1}
         \end{bmatrix}-\eta
     \begin{bmatrix} 
                \sum_{i=1}^n(h(x_i)-y_i) x_{i1}    \\
                \sum_{i=1}^n(h(x_i)-y_i) x_{i2}   \\
                \vdots   \\
                 \sum_{i=1}^n(h(x_i)-y_i) x_{id}   \\
                \sum_{i=1}^n(h(x_i)-y_i) x_{i(d+1)}  \\
         \end{bmatrix}
  \\&amp; = \begin{bmatrix} 
                w_1 - \eta\sum_{i=1}^n(h(x_i)-y_i) x_{i1}    \\
                w_2- \eta\sum_{i=1}^n(h(x_i)-y_i) x_{i2}   \\
                \vdots   \\
                 w_d - \eta\sum_{i=1}^n(h(x_i)-y_i) x_{id}   \\
               w_{d+1} - \eta \sum_{i=1}^n(h(x_i)-y_i) x_{i(d+1)}  \\
         \end{bmatrix} \tag{2-4}
\end{align}
\]</span> 看到这里就懂了吧？ 式2-4是2-3等价的矩阵表示。</p>
<p>因此我们可以用式2-4来写程序~</p>
<p>这也是《机器学习实战》中倒数第二行式子的由来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span>(<span class="params">dataMatIn, classLabels</span>):</span></span><br><span class="line">	dataMatrix = np.mat(dataMatIn)</span><br><span class="line">	labelMat = np.mat(classLabels).transpose()</span><br><span class="line">	m, n = np.shape(dataMatrix)</span><br><span class="line">	alpha = <span class="number">0.001</span></span><br><span class="line">	maxCycles = <span class="number">500</span>	</span><br><span class="line">	weights = np.ones((n,<span class="number">1</span>))</span><br><span class="line">	<span class="keyword">for</span> k <span class="keyword">in</span> range(maxCycles):</span><br><span class="line">		h = sigmoid(dataMatrix * weights)</span><br><span class="line">		error = labelMat - h</span><br><span class="line">		weights = weights + alpha * dataMatrix.transpose() * error</span><br><span class="line">	<span class="keyword">return</span> weights.getA()	</span><br></pre></td></tr></table></figure>
<h3 id="另一种似然函数">另一种似然函数</h3>
<p>在上面推导损失函数2-2的时候，我们y的标签为{0，1}，如果我们我们假设标签为{-1, +1}，那么同样用极大似然估计，会得到很多材料上的另一种类似的似然函数。</p>
<p>推导如下： <span class="math display">\[
\begin{align}
p(Y = +1 | {\bf x}) &amp;= h({\bf x}) = \frac{1}{1+e^{-{\bf w^Tx} }} \\
p(Y = -1 | {\bf x}) &amp;= 1-P(Y = 1|{\bf x}) \\
                    &amp;= \frac{1}{1+e^{ {\bf w^Tx} }}
\end{align}
\]</span> 由于sigmoid有性质<span class="math inline">\(h(-x) = 1 - h(x)\)</span>，因此上面两式可以合并为： <span class="math display">\[
p(y |{\bf x}) = h(y{\bf x}) = \frac{1}{1+e^{-y{\bf w^Tx} }}
\]</span> 写出似然函数为： <span class="math display">\[
\begin{align*}
\prod_{i=1}^n p(y_i |{\bf x_i})  &amp;= \prod_{i=1}^n h(y_i{\bf x_i}) 
\\ &amp;=\prod_{i=1}^n\frac{1}{1+e^{-y_i{\bf w^Tx_i} }}
\end{align*}
\]</span> 取对数似然有： <span class="math display">\[
\sum_{i=1}^n \ln\frac{1}{1+e^{-y_i{\bf w^Tx_i} }} = -\sum_{i=1}^n \ln(1+e^{-y_i{\bf w^Tx_i} })
\]</span> 取负值就得到另一种损失函数 <span class="math display">\[
L(w) =\sum_{i=1}^n\ln(1+e^{-y_i{\bf w^Tx_i} })\tag{2-5}
\]</span></p>
<h2 id="softmax">Softmax</h2>
<p>逻辑回归是处理二分类的问题，我们可以将它推广到多分类的情况，就是Softmax： <span class="math display">\[
h({\bf x}) = \left[ \begin{aligned}z_1 \\z_2 \\ ...\\z_K \end{aligned}\right]=\frac{1}{\sum_{j=1}^Ke^{\bf w_j^Tx} }\left[ \begin{aligned}e^{\bf w_1^Tx} \\e^{\bf w_2^Tx} \\ ...\\e^{\bf w_K^Tx} \end{aligned}\right]\tag{3-1}
\]</span> 看上去仿佛很复杂，因为它输出的是一个向量，向量中每一项分别表示对应类别的概率。而对于每个类别都有一个<span class="math inline">\(w_k\)</span></p>
<p>假如说求解第k个类别的概率，那么输出就是一个实数： <span class="math display">\[
P(y=k|\ {\bf x,w}) = h_k({\bf x}) =\frac{e^{\bf w_k^Tx} }{\sum_{j=1}^Ke^{\bf w_j^Tx} }\tag{3-2}
\]</span></p>
<p>这里的数据集形式为：<span class="math inline">\(\{ {(\bf x_1, y_1}), ({\bf x_2, y_2}), \cdots, ({\bf x_N, y_N}) \}\)</span></p>
<p>这里<span class="math inline">\(\bf x\)</span>是一个D * 1的向量（D为特征数）。而<span class="math inline">\({\bf y_i}\)</span>则是一个只有一个数字为1，其余都是0的K * 1维向量，用来表示这个样本属于哪个类别。</p>
<h3 id="softmax学习">Softmax学习</h3>
<p>和逻辑回归一样，采用极大似然估计。对于单个服从多项分布的<strong>单个</strong>样本，其似然函数为： <span class="math display">\[
\prod_{k=1}^K h_k^{y_k},\\
 其中，h_k = h_k({\bf x})=\frac{e^{\bf w_k^Tx} }{\sum_{j=1}^Ke^{\bf w_j^Tx} }
\]</span> 老样子，我们采用对数似然，并取相反数得：</p>
<p><span class="math display">\[
L({\bf W}) = -\sum_{k=1}^K y_k\ln h_k\tag{3-3}
\]</span> 3-3也被称为<strong>交叉熵损失</strong>（cross entropy loss）。注意这里的符号是大写加粗的<span class="math inline">\(\bf W\)</span>，表示为一个矩阵，形式为<span class="math inline">\(\bf [w_1, w_2,\cdots,w_K]\)</span>,每个类别都有一个d维的参数<span class="math inline">\(w_k\)</span>，合起来就是d * K。</p>
<p>要优化3-2，可以用梯度下降的方法进行求导。</p>
<p>为了简单起见，我们先让<span class="math inline">\(h_k\)</span>对<span class="math inline">\(\bf w_j\)</span>求偏导,求导需要注意有<span class="math inline">\(j=k\)</span>和<span class="math inline">\(j \ne k\)</span>两种情况，需要分开计算。</p>
<p>对于<span class="math inline">\(j=k\)</span>的情况，有: <span class="math display">\[
\begin{align*}
\frac{\partial h_k}{\partial \bf w_j} &amp;=  \frac{e^{\bf w_k^Tx} }{\sum_{a=1}^Ke^{\bf w_a^Tx} } {\bf x} - \frac{e^{\bf w_k^Tx} }{(\sum_{a=1}^Ke^{\bf w_a^Tx})^2} e^{\bf w_j^Tx} {\bf x}\\
&amp;=h_k (1 - h_j){\bf x} \\
&amp;=h_j (1 - h_j){\bf x} \tag{3-4}
\end{align*}
\]</span> 对于<span class="math inline">\(j \ne k\)</span>的求导，有: <span class="math display">\[
\begin{align*}
\frac{\partial h_k}{\partial \bf w_j} &amp;=  - \frac{e^{\bf w_k^Tx} }{(\sum_{a=1}^Ke^{\bf w_a^Tx})^2} e^{\bf w_j^Tx} {\bf x}\\
&amp;=-h_k h_j{\bf x}\tag{3-5}
\end{align*}
\]</span> 于是，我们现在就可以求3-3 <strong>单个样本</strong>的交叉熵的导数了： <span class="math display">\[
\begin{align*}
\frac{\partial L({\bf W})}{\partial \bf w_j} &amp; = -\sum_{k} y_k \frac{\partial \log h_k}{\partial \bf w_j}\\
&amp;= -\sum_{k} y_k \frac{1}{h_k}\frac{\partial h_k}{\partial \bf w_j}\\
&amp;= -y_j  \frac{1}{h_j}h_j (1 - h_j){\bf x} + \sum_{k\ne j}y_k \frac{1}{h_k}h_kh_j{\bf x}\\
&amp;=- y_j (1 - h_j){\bf x} + \sum_{k\ne j}y_k h_j{\bf x}\\
&amp;= \left(-y_j +y_jh_j+ \sum_{k\ne j}y_kh_j\right) {\bf x}\\
&amp;= \left(-y_j +\sum_{k}y_kh_j\right) {\bf x} \\
&amp;= \left(-y_j +h_j\sum_{k}y_k\right) {\bf x} \\
&amp;= \left(h_j -y_j\right) {\bf x} \\
\end{align*}
\]</span> 进一步的，如果要计算整个训练集的损失，可以求： <span class="math display">\[
L({\bf W}) = -\sum_{i=1}^N\sum_{i=1}^K y_k\ln h_k({\bf x_i})\tag{3-4}
\]</span></p>
<h3 id="参数的冗余性">参数的冗余性</h3>
<p>Softmax 回归有一个不寻常的特点：它有一个“冗余”的参数集。 比如说，我们对每个类别参数<span class="math inline">\(\bf w_k\)</span>都减去一个向量<span class="math inline">\(\theta\)</span>，得： <span class="math display">\[
\begin{align*}
P(y=k|\ {\bf x,w}) = h_k({\bf x}) &amp;=\frac{e^{\bf (w_k - \theta)^Tx} }{\sum_{j=1}^Ke^{\bf (w_j- \theta)^Tx} }\\
&amp;=\frac{e^{\bf w_k ^Tx} e^{\bf - \theta^Tx} }{\sum_{j=1}^Ke^{\bf w_j^Tx}e^{\bf - \theta^Tx} }\\
&amp;=\frac{e^{\bf w_k^Tx} }{\sum_{j=1}^Ke^{\bf w_j^Tx} }
\end{align*}
\]</span> 可以看到减去一个向量完全不影响预测结果！这说明如果参数<span class="math inline">\(\bf (w_1, w_2,\cdots , w_k)\)</span>是已经根据损失函数求解后的极小值点，那么<span class="math inline">\(\bf (w_1 - \theta, w_2 - \theta,\cdots , w_k - \theta)\)</span>同样也是极小值点。因此解不唯一。</p>
<p>解决这个问题可以加入权重衰减（weight decay），说白了就是<strong>加入L2正则项</strong>。</p>
<p>3-4就变为： <span class="math display">\[
L({\bf W}) = -\sum_{i=1}^N\sum_{k=1}^K y_k\ln h_k({\bf x_i}) + \sum_{i=1}^d \sum_{k=1}^Kw_{ij}^2 \tag{3-4}
\]</span></p>
<h3 id="和逻辑回归的关系">和逻辑回归的关系</h3>
<p>当k = 2时，Softmax表达式为： <span class="math display">\[
h({\bf x}) = \frac{1}{e^{\bf w_1^Tx}+e^{\bf w_2^Tx} }\left[ \begin{aligned}e^{\bf w_1^Tx} \\e^{\bf w_2^Tx}  \end{aligned}\right]
\]</span> 利用冗余性的特点，参数都减去<span class="math inline">\(w_1\)</span>，得： <span class="math display">\[
\begin{align*}
h({\bf x}) &amp;= \frac{1}{e^{\bf (w_1 - w_1)^Tx}+e^{\bf (w_2 - w_1)^Tx} }\left[ \begin{aligned}e^{\bf (w_1 - w_1)^Tx} \\e^{\bf (w_2 - w_1)^Tx}  \end{aligned}\right]\\
&amp;=  \frac{1}{1+e^{\bf (w_2 - w_1)^Tx} }\left[ \begin{aligned} 1 \\e^{\bf (w_2 - w_1)^Tx}  \end{aligned}\right] \\
&amp;= \left[ \begin{aligned}  \frac{1}{1+e^{\bf (w_2 - w_1)^Tx} } \\  \frac{e^{\bf (w_2 - w_1)^Tx} }{1+e^{\bf (w_2 - w_1)^Tx} } \end{aligned}\right] \\
&amp;= \left[ \begin{aligned}  \frac{1}{1+e^{\bf (w_2 - w_1)^Tx} } \\  1 -\frac{1}{1+e^{\bf (w_2 - w_1)^Tx} } \end{aligned}\right] \\
\end{align*}
\]</span> 令<span class="math inline">\(\bf w = w_2 - w_1\)</span>，得一个类别的概率为<span class="math inline">\(\frac{1}{1+e^{\bf w^Tx} }\)</span>，另一个类别的概率为<span class="math inline">\(1 - \frac{1}{1+e^{\bf w^Tx} }\)</span>，这和逻辑回归是一样的。</p>
<p>因此，可以说Softmax是Logistic 回归的一般形式，当K=2时，Softmax退化为logistic回归。</p>
<h3 id="使用softmax-还是k个逻辑回归">使用Softmax 还是K个逻辑回归</h3>
<p>如果你在开发一个音乐分类的应用，需要对k种类型的音乐进行识别，那么是选择使用 Softmax 分类器呢，还是使用 logistic 回归算法建立 k 个独立的二元分类器呢？</p>
<p><strong>如果你的类别是互斥的，那么用Softmax，否则用K个逻辑回归</strong>：</p>
<blockquote>
<p>这一选择取决于你的类别之间是否互斥，例如，如果你有四个类别的音乐，分别为：古典音乐、乡村音乐、摇滚乐和爵士乐，那么你可以假设每个训练样本只会被打上一个标签（即：一首歌只能属于这四种音乐类型的其中一种），此时你应该使用类别数 k = 4 的Softmax回归。（如果在你的数据集中，有的歌曲不属于以上四类的其中任何一类，那么你可以添加一个“其他类”，并将类别数 <em>k</em> 设为5。）</p>
<p>如果你的四个类别如下：人声音乐、舞曲、影视原声、流行歌曲，那么这些类别之间并不是互斥的。例如：一首歌曲可以来源于影视原声，同时也包含人声 。这种情况下，使用4个二分类的 logistic 回归分类器更为合适。这样，对于每个新的音乐作品，我们的算法可以分别判断它是否属于各个类别。</p>
<p>现在我们来看一个计算视觉领域的例子，你的任务是将图像分到三个不同类别中。</p>
<ol type="1">
<li>假设这三个类别分别是：室内场景、户外城区场景、户外荒野场景。你会使用Softmax回归还是 3个logistic 回归分类器呢？</li>
<li>现在假设这三个类别分别是室内场景、黑白图片、包含人物的图片，你又会选择 Softmax回归还是多个 logistic 回归分类器呢？</li>
</ol>
<p>在第一个例子中，三个类别是互斥的，因此更适于选择Softmax回归分类器 。而在第二个例子中，建立三个独立的 logistic回归分类器更加合适。</p>
</blockquote>
<h2 id="并行化逻辑回归">并行化逻辑回归</h2>
<p>逻辑回归虽然简单，但是在真实环境中，数据量大，也往往对其做并行化处理。</p>
<p>这里，我们首先要复习一下逻辑回归的更新公式2-3： <span class="math display">\[
w_j = w_j - \eta\sum_{i=1}^N(h({\bf x_i}) -y_i )x_{ij}\\
其中，h({\bf x_i}) = \frac{1}{1+e^{-({\bf w^Tx_i})} }
\]</span> 注意到<span class="math inline">\(\bf w\)</span>的维度为d，需要对数据的每一维都使用上式更新。可以看到，一次更新的开销数据量N以及维度d有关。</p>
<h3 id="数据并行">数据并行</h3>
<p>这是最简单也是最容易想到的数据并行方式了，假设有a台机器，则把数据随机分到a台机器上，每台机器数据不重复，这样每台机器有N/a条样本，每个样本有d个特征。每台机器分别对其样本计算<span class="math inline">\((h({\bf x_i}) -y_i )x_{ij}\)</span>，最后求和合并即可。</p>
<p>这个方式解决了数据量大的问题，但是实际中特征数量可能很高。</p>
<h3 id="特征并行">特征并行</h3>
<p>特征并行就是对特征进行划分，假设有b台机器，那么，每台机器的有<span class="math inline">\(\frac{d}{b}\)</span>个特征，N个样本，每台机器对其拥有的特征j分别计算<span class="math inline">\((h({\bf x_i}) -y_i )x_{ij}\)</span>，然后和其它机器同步更新后的参数即可。</p>
<h3 id="数据特征并行">数据+特征并行</h3>
<p>数据+特征并行就是上面两种的结合，如下图所示，将数据分为a * b块，其中，即将数据按水平划分，又在特征上垂直划分。</p>
<figure>
<img src="../images/machine-learning-linear-regression-logistic-regression/lr-data-feature-parallel.png" alt="lr-data-feature-parallel" /><figcaption>lr-data-feature-parallel</figcaption>
</figure>
<p>看看公式2-3，我们需要计算<span class="math inline">\(\bf w^Tx\)</span>，因此，我们可以对所有机器分别计算<span class="math inline">\(\sum_{j} w_jx_{ij}\)</span>，然后按照行号相同的进行归并，就得到了<span class="math inline">\(\bf w^Tx_i\)</span>。</p>
<p>此外，我们需要计算<span class="math inline">\(\sum_{i=1}^N(h({\bf x_i}) -y_i )x_{ij}\)</span>，这就是要让列号相同的结点进行归并。</p>
<h3 id="google-distbelief">Google DistBelief</h3>
<p>Google实现了一个名为DistBelief的框架，采用parameter server来同步参数，如下图：</p>
<figure>
<img src="../images/machine-learning-linear-regression-logistic-regression/Google-DistBelief.png" alt="Google-DistBelief" /><figcaption>Google-DistBelief</figcaption>
</figure>
<p>将训练数据划分为若干个子集，并<strong>在每一个子集上运行一个单独的模型副本</strong>，各模型副本通过参数服务器来交换梯度信息，参数服务器用来维护模型参数当前的状态，它也分布在多台机器上（假如有10台机器，那么每台机器将负责存储和更新1/10的模型参数，因此，每个模型副本只需要和parameters中和该结点有关的模型参数那部分结点进行通信）。</p>
<p>可以看出，这里面有两种异步性：</p>
<ol type="1">
<li>模型副本之间的运行是独立的</li>
<li>参数服务器之间的机器也是独立的</li>
</ol>
<p>如果每个模型副本处理完一个样本就和参数服务器进行通信，那么通信量会很大，为了减少通信的开销，设置了两个参数：<span class="math inline">\(n_{fetch}\)</span>和<span class="math inline">\(n_{push}\)</span>，每<span class="math inline">\(n_{fetch}\)</span>向服务器取一次参数，每<span class="math inline">\(n_{push}\)</span>次向服务器推送结果，<span class="math inline">\(n_{fetch}\)</span>不一定等于<span class="math inline">\(n_{push}\)</span></p>
<p>此外，</p>
<ul>
<li>一个Model replicas失效并不影响其它的Model replicas，因此鲁棒性很强</li>
<li>Model replicas是异步执行的，省去了同步时间</li>
<li>每个 Model Replicas 中的 fetch compute push 操作是通过三个线程完成的，类似于流水线机制，大大加快了速度</li>
<li>参数服务器的每个节点更新参数的次数不一定相同，有一定的随机性</li>
</ul>
<p>然而，这样的异步处理方式使得模型具有很大的随机性，主要体现在一个模型计算样本的梯度的时候，采用的是一组稍微过时的参数，因为它在计算梯度时，其它副本很可能已经对参数服务器中的参数进行了更新；并且某一个时刻，各个机器上的参数更新次数和更新顺序不一定相同。这样的做法缺乏理论基础，但最后得到的模型效果还是不错的。</p>
<h2 id="参考资料">参考资料</h2>
<ul>
<li>《机器学习》 周志华</li>
<li>《统计学习方法》 - 李航</li>
<li>机器学习基石 - 林轩田</li>
<li>机器学习--Logistic回归计算过程的推导
<ul>
<li>http://blog.csdn.net/ligang_csdn/article/details/53838743</li>
<li>http://blog.csdn.net/jediael_lu/article/details/77852060</li>
</ul></li>
<li>Softmax
<ul>
<li><a target="_blank" rel="noopener" href="http://deeplearning.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92">Softmax回归</a></li>
<li><p><a target="_blank" rel="noopener" href="http://www.cnblogs.com/ooon/p/5690848.html">Logistic 与 softmax</a></p></li>
<li><p>https://deepnotes.io/softmax-crossentropy</p></li>
</ul></li>
<li>并行化逻辑回归
<ul>
<li><a target="_blank" rel="noopener" href="http://blog.sina.com.cn/s/blog_6cb8e53d0101oetv.html">并行逻辑回归</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/itplus/article/details/31831661">DistBelief 框架下的并行随机梯度下降法 - Downpour SGD</a></li>
</ul></li>
</ul>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>hrwhisper
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://www.hrwhisper.me/machine-learning-linear-regression-logistic-regression/" title="『我爱机器学习』线性回归、逻辑回归与Softmax">https://www.hrwhisper.me/machine-learning-linear-regression-logistic-regression/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        <div class="reward-container">
  <div>请我喝杯咖啡吧~</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/donate/wechat_pay.png" alt="hrwhisper 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/donate/alipay.jpg" alt="hrwhisper 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
              <a href="/tags/Machine-Learning-model/" rel="tag"># Machine Learning model</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/machine-learning-perceptron/" rel="prev" title="『我爱机器学习』感知机">
      <i class="fa fa-chevron-left"></i> 『我爱机器学习』感知机
    </a></div>
      <div class="post-nav-item">
    <a href="/diary-2018-03-04/" rel="next" title="2018.03~04">
      2018.03~04 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-text">线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="nav-text">优化目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%82%E8%A7%A3"><span class="nav-text">求解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A9%BA%E9%97%B4%E5%8F%98%E6%8D%A2"><span class="nav-text">空间变换</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="nav-text">逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#logistic%E5%87%BD%E6%95%B0%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="nav-text">Logistic函数与逻辑回归模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%E4%B8%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">参数估计与损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B1%82%E8%A7%A3"><span class="nav-text">梯度下降求解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%A6%E4%B8%80%E7%A7%8D%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0"><span class="nav-text">另一种似然函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#softmax"><span class="nav-text">Softmax</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#softmax%E5%AD%A6%E4%B9%A0"><span class="nav-text">Softmax学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E7%9A%84%E5%86%97%E4%BD%99%E6%80%A7"><span class="nav-text">参数的冗余性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%92%8C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-text">和逻辑回归的关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8softmax-%E8%BF%98%E6%98%AFk%E4%B8%AA%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="nav-text">使用Softmax 还是K个逻辑回归</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B9%B6%E8%A1%8C%E5%8C%96%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="nav-text">并行化逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C"><span class="nav-text">数据并行</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%B9%B6%E8%A1%8C"><span class="nav-text">特征并行</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%89%B9%E5%BE%81%E5%B9%B6%E8%A1%8C"><span class="nav-text">数据+特征并行</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#google-distbelief"><span class="nav-text">Google DistBelief</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-text">参考资料</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="hrwhisper"
      src="/images/site/avatar.jpg">
  <p class="site-author-name" itemprop="name">hrwhisper</p>
  <div class="site-description" itemprop="description">一个分享机器学习、算法与数据结构，个人学习心得、读书笔记、生活的博客。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">251</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">52</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hrwhisper" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hrwhisper" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/murmured" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;murmured" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
      <script data-ad-client="ca-pub-1580254183546533" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2013 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hrwhisper</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"fVcjWMD8aI6F0qEfKdUaHa4f-gzGzoHsz","app_key":"b26lBsbwmVyxTSnNrsBrnv3U","server_url":null,"security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      // script.setAttribute("data-pjax", "");
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'fVcjWMD8aI6F0qEfKdUaHa4f-gzGzoHsz',
      appKey     : 'b26lBsbwmVyxTSnNrsBrnv3U',
      placeholder: "在上方填上邮箱地址可以收到我回复的邮件哦~",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
